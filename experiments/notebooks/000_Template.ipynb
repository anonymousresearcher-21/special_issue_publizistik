{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß™ SLR Abstract Screening Experiment\n",
        "#### Experiment Information\n",
        "- **ID**: \n",
        "- **Date**: \n",
        "#### üéØ Goal\n",
        "- \n",
        "#### ‚öôÔ∏è Configuration\n",
        "- **LLM** : GPT-4o\n",
        "- **Data**: \n",
        "- **Examples** : \n",
        "- **Output**: \n",
        "#### üìù Notes\n",
        "- \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîß Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Essential imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import json\n",
        "from pathlib import Path\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "# Configure pandas display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_theme()  # This is the correct way to set seaborn style\n",
        "plt.rcParams['figure.figsize'] = (12, 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Import \n",
        "\n",
        "# Define the data paths for both datasets\n",
        "DATA_PATH_1 = \"../data/SSOT_manual_LB_20250808_120908.csv\" # ‚¨ÖÔ∏è Change this path if needed\n",
        "DATA_PATH_2 = \"../data/SSOT_manual_BM_20250813_132621.csv\" # ‚¨ÖÔ∏è Change this path if needed\n",
        "\n",
        "# Load the first dataset (df1)\n",
        "try:\n",
        "    df_LB = pd.read_csv(DATA_PATH_1)\n",
        "    print(f\"‚úì First dataset loaded successfully\")\n",
        "    print(f\"‚úì Shape of dataset 1: {df_LB.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: The file LB dataset was not found in the data directory\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading the first dataset: {str(e)}\")\n",
        "\n",
        "# Load the second dataset (df2)\n",
        "try:\n",
        "    df_BM = pd.read_csv(DATA_PATH_2)\n",
        "    print(f\"\\n‚úì Second dataset loaded successfully\")\n",
        "    print(f\"‚úì Shape of dataset 2: {df_BM.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: The file df_BM was not found in the data directory\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading the second dataset: {str(e)}\")\n",
        "\n",
        "# Display basic information about both datasets\n",
        "print(\"\\nFirst few rows of dataset 1:\\n\")\n",
        "display(df_LB.head())\n",
        "\n",
        "print(\"\\nFirst few rows of dataset 2:\\n\")\n",
        "display(df_BM.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß´ Define Experiment Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Experiment Metadata\n",
        "EXPERIMENT_ID = \"001\"  # ‚¨ÖÔ∏è Change this for each new experiment\n",
        "EXPERIMENT_DATE = \"2025-08-13\"  # ‚¨ÖÔ∏è Update the date\n",
        "EXPERIMENT_CATEGORY = \"Testing\"  # ‚¨ÖÔ∏è Category of experiment\n",
        "EXPERIMENT_GOAL = \"Test Set Up\"  # ‚¨ÖÔ∏è What are you testing?\n",
        "\n",
        "# Model Configuration\n",
        "MODEL_NAME = \"gpt-4o\"\n",
        "TEMPERATURE = 0.0\n",
        "MAX_TOKENS = 4000\n",
        "\n",
        "# Print experiment info\n",
        "print(\"üß™ EXPERIMENT SETUP\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"ID: {EXPERIMENT_ID}\")\n",
        "print(f\"Date: {EXPERIMENT_DATE}\")\n",
        "print(f\"Category: {EXPERIMENT_CATEGORY}\")\n",
        "print(f\"üéØGoal: {EXPERIMENT_GOAL}\")\n",
        "print(f\"Model: {MODEL_NAME} (temp={TEMPERATURE})\")\n",
        "print(\"=\" * 50)\n",
        "print(\"‚úÖ Experiment configuration loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì£ Set up Basic API Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from datetime import datetime\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Get the API key from environment variables\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Validate API key\n",
        "if not api_key:\n",
        "    print(\"‚ö†Ô∏è  Error: OPENAI_API_KEY not found.\")\n",
        "    print(\"Please make sure you have a .env file with OPENAI_API_KEY='sk-...'\")\n",
        "else:\n",
        "    print(\"‚úÖ OpenAI API Key loaded successfully.\")\n",
        "    client = OpenAI(api_key=api_key)\n",
        "    print(\"‚úÖ OpenAI client initialized.\")\n",
        "\n",
        "# Enhanced analysis function for abstract screening\n",
        "def screen_abstract_llm(abstract_text, system_prompt, user_prompt_template, \n",
        "                       model=\"gpt-4o\", temperature=0.0):\n",
        "    \"\"\"\n",
        "    Screen an abstract using LLM with system and user prompts.\n",
        "    \n",
        "    Args:\n",
        "        abstract_text (str): The abstract to analyze\n",
        "        system_prompt (str): The system prompt defining the role\n",
        "        user_prompt_template (str): Template with {abstract} placeholder\n",
        "        model (str): The OpenAI model to use\n",
        "        temperature (float): Temperature setting for response randomness\n",
        "    \n",
        "    Returns:\n",
        "        dict: Result with decision, reasoning, and metadata\n",
        "    \"\"\"\n",
        "    if 'client' not in globals():\n",
        "        return {\"error\": \"OpenAI client is not initialized. Please check your API key.\"}\n",
        "\n",
        "    try:\n",
        "        # Insert abstract into user prompt template\n",
        "        user_prompt = user_prompt_template.format(abstract=abstract_text)\n",
        "        \n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "            max_tokens=4000\n",
        "        )\n",
        "        \n",
        "        if response and response.choices:\n",
        "            result = {\n",
        "                \"decision\": \"INCLUDE\" if \"INCLUDE\" in response.choices[0].message.content.upper() else \"EXCLUDE\",\n",
        "                \"reasoning\": response.choices[0].message.content,\n",
        "                \"model\": model,\n",
        "                \"temperature\": temperature,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"error\": None\n",
        "            }\n",
        "            return result\n",
        "        else:\n",
        "            return {\"error\": \"API Error: Empty or invalid response.\"}\n",
        "            \n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"API Error: {e}\"}\n",
        "\n",
        "print(\"‚úÖ Enhanced screening function defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèõÔ∏è Set Up System Prompt "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# System prompt configuration\n",
        "# System prompt configuration\n",
        "SYSTEM_PROMPT_ID = \"SYS_001\"  # ‚¨ÖÔ∏è Change this ID for different system prompts\n",
        "SYSTEM_PROMPT_DESCRIPTION = \"Generic expert literature review screener for systematic reviews\"\n",
        "\n",
        "# Define the system prompt that sets the LLM's role\n",
        "SYSTEM_PROMPT = \"\"\"You are an expert in scientific literature review and systematic review methodology.\n",
        "\n",
        "Your task is to screen research abstracts and decide whether they should be INCLUDED or EXCLUDED from a systematic literature review based on provided criteria.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Carefully read the provided inclusion/exclusion criteria\n",
        "2. Review any example abstracts to understand the decision-making pattern\n",
        "3. Apply the criteria systematically to the given abstract and title\n",
        "4. Provide your decision in the exact format requested\n",
        "5. Base your reasoning strictly on the provided criteria\n",
        "\n",
        "Be consistent, objective, and systematic in your evaluation. Do not make up additional criteria beyond what is provided. Focus only on what is explicitly stated in the instructions.\"\"\"\n",
        "\n",
        "print(f\"‚úÖ System prompt defined\")\n",
        "print(f\"üìã ID: {SYSTEM_PROMPT_ID}\")\n",
        "print(f\"üìè Length: {len(SYSTEM_PROMPT)} characters\")\n",
        "print(f\"üìÑ Description: {SYSTEM_PROMPT_DESCRIPTION}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üë©üèª‚Äç‚öïÔ∏è Create User Prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# User prompt configuration\n",
        "USER_PROMPT_ID = \"USR_001\"  # ‚¨ÖÔ∏è Change this ID for different user prompts\n",
        "USER_PROMPT_DESCRIPTION = \"Basic CTAM screening with criteria and examples from CSV files\"\n",
        "\n",
        "# File paths for modular components\n",
        "CRITERIA_FILE = \"../prompts/Criteria_LB_01.csv\"  # ‚¨ÖÔ∏è Change criteria file here\n",
        "EXAMPLES_FILE = \"../prompts/exmpl_single_LB_01.csv\"  # ‚¨ÖÔ∏è Change examples file here (or set to None)\n",
        "\n",
        "# Output configuration\n",
        "OUTPUT_FORMAT = \"Binary\"  # ‚¨ÖÔ∏è Options: \"Binary\", \"Yes/Maybe/No\", \"Likert\"\n",
        "DECISION_OPTIONS = [\"INCLUDE\", \"EXCLUDE\"] # ‚¨ÖÔ∏è Change according to the output format\n",
        "\n",
        "# Additional metadata for results tracking\n",
        "DOMAIN = \"political_communication\" # ‚¨ÖÔ∏è Change this to the domain of the study\n",
        "TOPIC = \"media_diversity\"  # ‚¨ÖÔ∏è Change this to the topic of the study\n",
        "DATASET_SOURCE = \"LB\"  # ‚¨ÖÔ∏è Which dataset (BM/LB)\n",
        "\n",
        "# Define the user prompt template with placeholders\n",
        "USER_PROMPT_TEMPLATE = \"\"\"## SCREENING TASK:\n",
        "You are screening abstracts for a systematic literature review on {topic} in {domain}.\n",
        "\n",
        "## INCLUSION/EXCLUSION CRITERIA:\n",
        "{criteria_text}\n",
        "\n",
        "{examples_section}\n",
        "\n",
        "## ABSTRACT TO SCREEN:\n",
        "**Title:** {title}\n",
        "**Abstract:** {abstract}\n",
        "\n",
        "## YOUR DECISION:\n",
        "Based strictly on the criteria above, provide your decision as either \"{decision_include}\" or \"{decision_exclude}\" followed by your reasoning:\n",
        "\n",
        "**Decision:** \n",
        "**Reasoning:** \"\"\"\n",
        "\n",
        "print(f\"‚úÖ User prompt configuration and template loaded\")\n",
        "print(f\"üìã ID: {USER_PROMPT_ID}\")\n",
        "print(f\"üìÑ Description: {USER_PROMPT_DESCRIPTION}\")\n",
        "print(f\"üìÅ Criteria: {CRITERIA_FILE}\")\n",
        "print(f\"üìÅ Examples: {EXAMPLES_FILE}\")\n",
        "print(f\"üéØ Output: {OUTPUT_FORMAT}\")\n",
        "print(f\"üî¨ Topic: {TOPIC} | Domain: {DOMAIN} | Source: {DATASET_SOURCE}\")\n",
        "print(f\"üìè Template length: {len(USER_PROMPT_TEMPLATE)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Valdiation Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_experiment_setup(df, dataset_source=\"LB\"):\n",
        "    \"\"\"\n",
        "    Validate that all required variables and data are available for the experiment.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame to be used in experiment\n",
        "        dataset_source: Dataset identifier\n",
        "    \n",
        "    Returns:\n",
        "        bool: True if all validations pass, False otherwise\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"üîç VALIDATION CHECK\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    validation_passed = True\n",
        "    \n",
        "    # Check required global variables\n",
        "    required_vars = {\n",
        "        'EXPERIMENT_ID': globals().get('EXPERIMENT_ID'),\n",
        "        'SYSTEM_PROMPT_ID': globals().get('SYSTEM_PROMPT_ID'), \n",
        "        'USER_PROMPT_ID': globals().get('USER_PROMPT_ID'),\n",
        "        'SYSTEM_PROMPT': globals().get('SYSTEM_PROMPT'),\n",
        "        'USER_PROMPT_TEMPLATE': globals().get('USER_PROMPT_TEMPLATE'),\n",
        "        'CRITERIA_FILE': globals().get('CRITERIA_FILE'),\n",
        "        'DECISION_OPTIONS': globals().get('DECISION_OPTIONS'),\n",
        "        'MODEL_NAME': globals().get('MODEL_NAME'),\n",
        "        'TEMPERATURE': globals().get('TEMPERATURE'),\n",
        "        'TOPIC': globals().get('TOPIC'),\n",
        "        'DOMAIN': globals().get('DOMAIN')\n",
        "    }\n",
        "    \n",
        "    # Optional variables that can be None\n",
        "    optional_vars = {\n",
        "        'EXAMPLES_FILE': globals().get('EXAMPLES_FILE')\n",
        "    }\n",
        "    \n",
        "    print(\"üìã Checking required variables:\")\n",
        "    for var_name, var_value in required_vars.items():\n",
        "        if var_value is None:\n",
        "            print(f\"   ‚ùå {var_name}: NOT DEFINED\")\n",
        "            validation_passed = False\n",
        "        else:\n",
        "            print(f\"   ‚úÖ {var_name}: {str(var_value)[:50]}{'...' if len(str(var_value)) > 50 else ''}\")\n",
        "    \n",
        "    print(\"üìã Checking optional variables:\")\n",
        "    for var_name, var_value in optional_vars.items():\n",
        "        if var_value is None:\n",
        "            print(f\"   ‚úÖ {var_name}: None (optional - will run without examples)\")\n",
        "        else:\n",
        "            print(f\"   ‚úÖ {var_name}: {str(var_value)[:50]}{'...' if len(str(var_value)) > 50 else ''}\")\n",
        "    \n",
        "    # Check DataFrame structure\n",
        "    print(f\"\\nüìä Checking DataFrame structure:\")\n",
        "    required_columns = ['abstract', 'title_full', 'stage_2', 'stage_3']\n",
        "    \n",
        "    if df is None:\n",
        "        print(f\"   ‚ùå DataFrame is None\")\n",
        "        validation_passed = False\n",
        "    else:\n",
        "        print(f\"   ‚úÖ DataFrame shape: {df.shape}\")\n",
        "        \n",
        "        for col in required_columns:\n",
        "            if col in df.columns:\n",
        "                print(f\"   ‚úÖ Column '{col}': Present\")\n",
        "            else:\n",
        "                print(f\"   ‚ùå Column '{col}': MISSING\")\n",
        "                validation_passed = False\n",
        "    \n",
        "    # Check data availability\n",
        "    if df is not None and all(col in df.columns for col in required_columns):\n",
        "        print(f\"\\nüìà Checking data availability:\")\n",
        "        stage2_true = len(df[df['stage_2'] == True])\n",
        "        stage2_false = len(df[df['stage_2'] == False])\n",
        "        stage3_true = len(df[df['stage_3'] == True])\n",
        "        stage3_false = len(df[df['stage_3'] == False])\n",
        "        \n",
        "        print(f\"   üìä Stage 2 True: {stage2_true}\")\n",
        "        print(f\"   üìä Stage 2 False: {stage2_false}\")\n",
        "        print(f\"   üìä Stage 3 True: {stage3_true}\")\n",
        "        print(f\"   üìä Stage 3 False: {stage3_false}\")\n",
        "        \n",
        "        if stage3_true < 10:\n",
        "            print(f\"   ‚ö†Ô∏è  Warning: Only {stage3_true} stage_3=True examples available\")\n",
        "        if stage3_false < 10:\n",
        "            print(f\"   ‚ö†Ô∏è  Warning: Only {stage3_false} stage_3=False examples available\")\n",
        "    \n",
        "    # Check file paths\n",
        "    print(f\"\\nüìÅ Checking file paths:\")\n",
        "    import os\n",
        "    \n",
        "    # CRITERIA_FILE is required\n",
        "    if CRITERIA_FILE and os.path.exists(CRITERIA_FILE):\n",
        "        print(f\"   ‚úÖ Criteria file: {CRITERIA_FILE}\")\n",
        "    elif CRITERIA_FILE:\n",
        "        print(f\"   ‚ùå Criteria file: {CRITERIA_FILE} (NOT FOUND)\")\n",
        "        validation_passed = False\n",
        "    else:\n",
        "        print(f\"   ‚ùå Criteria file: NOT SPECIFIED\")\n",
        "        validation_passed = False\n",
        "    \n",
        "    # EXAMPLES_FILE is optional\n",
        "    if EXAMPLES_FILE is None:\n",
        "        print(f\"   ‚úÖ Examples file: None (will run without examples)\")\n",
        "    elif os.path.exists(EXAMPLES_FILE):\n",
        "        print(f\"   ‚úÖ Examples file: {EXAMPLES_FILE}\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå Examples file: {EXAMPLES_FILE} (NOT FOUND)\")\n",
        "        validation_passed = False\n",
        "    \n",
        "    # Check API function\n",
        "    print(f\"\\nü§ñ Checking API function:\")\n",
        "    if 'screen_abstract_llm' in globals():\n",
        "        print(f\"   ‚úÖ screen_abstract_llm function: Available\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå screen_abstract_llm function: NOT DEFINED\")\n",
        "        validation_passed = False\n",
        "    \n",
        "    # Final result\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    if validation_passed:\n",
        "        print(\"‚úÖ ALL VALIDATIONS PASSED - Ready to run experiment!\")\n",
        "    else:\n",
        "        print(\"‚ùå VALIDATION FAILED - Please fix the issues above before running\")\n",
        "    \n",
        "    return validation_passed\n",
        "\n",
        "# Run validation\n",
        "validation_result = validate_experiment_setup(df_LB, \"LB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî¨ Set Up Function - BINARY "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix \n",
        "from datetime import datetime\n",
        "import os\n",
        "import time\n",
        "\n",
        "def run_classification_experiment(\n",
        "    df, \n",
        "    n_total_examples=25,  # ‚¨ÖÔ∏è Total number of examples to test\n",
        "    n_stage3_true=5,     # ‚¨ÖÔ∏è Number of stage_3=True examples\n",
        "    n_stage3_false=20,    # ‚¨ÖÔ∏è Number of stage_3=False examples\n",
        "    dataset_source=\"LB\",  # ‚¨ÖÔ∏è Dataset identifier (LB/BM)\n",
        "    batch_size=20,        # ‚¨ÖÔ∏è Batch size for processing (max 20 to avoid timeouts)\n",
        "    save_results=True,    # ‚¨ÖÔ∏è Whether to save results to CSV\n",
        "    verbose=True          # ‚¨ÖÔ∏è Print progress updates\n",
        "):\n",
        "    \"\"\"\n",
        "    Run LLM classification experiment on abstracts with batch processing.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with abstracts (must have 'abstract', 'title_full', 'stage_2', 'stage_3')\n",
        "        n_total_examples: Total number of examples to test\n",
        "        n_stage3_true: Number of stage_3=True examples to include\n",
        "        n_stage3_false: Number of stage_3=False examples to include\n",
        "        dataset_source: Dataset identifier for results filename\n",
        "        batch_size: Number of examples to process in each batch (max 20)\n",
        "        save_results: Whether to save results to CSV\n",
        "        verbose: Whether to print progress\n",
        "    \n",
        "    Returns:\n",
        "        dict: Results including metrics and DataFrame\n",
        "    \"\"\"\n",
        "    \n",
        "    # Validate batch size\n",
        "    if batch_size > 20:\n",
        "        print(\"‚ö†Ô∏è  Warning: Batch size > 20 may cause timeouts. Setting to 20.\")\n",
        "        batch_size = 20\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"üß™ Starting Classification Experiment with Batch Processing\")\n",
        "        print(f\"üìä Dataset: {dataset_source}\")\n",
        "        print(f\"üéØ Total examples: {n_total_examples}\")\n",
        "        print(f\"‚úÖ Stage 3 True: {n_stage3_true}\")\n",
        "        print(f\"‚ùå Stage 3 False: {n_stage3_false}\")\n",
        "        print(f\"üì¶ Batch size: {batch_size}\")\n",
        "        print(\"=\" * 50)\n",
        "    \n",
        "    # Sample examples\n",
        "    stage3_true_samples = df[df['stage_3'] == True].sample(n=n_stage3_true, random_state=42)\n",
        "    stage3_false_samples = df[df['stage_3'] == False].sample(n=n_stage3_false, random_state=42)\n",
        "    \n",
        "    # Combine samples\n",
        "    test_samples = pd.concat([stage3_true_samples, stage3_false_samples]).reset_index(drop=True)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"üìù Sampled {len(test_samples)} examples\")\n",
        "    \n",
        "    # Load criteria and examples text\n",
        "    def load_criteria_text(criteria_file):\n",
        "        try:\n",
        "            criteria_df = pd.read_csv(criteria_file)\n",
        "            criteria_text = \"\"\n",
        "            \n",
        "            # Add inclusion criteria\n",
        "            inclusion_criteria = criteria_df[criteria_df['type'] == 'inclusion']\n",
        "            if len(inclusion_criteria) > 0:\n",
        "                criteria_text += \"**INCLUSION CRITERIA:**\\n\"\n",
        "                for _, row in inclusion_criteria.iterrows():\n",
        "                    criteria_text += f\"- **{row['criterion_id']}**: {row['description']}\\n\"\n",
        "                    if pd.notna(row['examples']) and row['examples'].strip():\n",
        "                        criteria_text += f\"  *Examples: {row['examples']}*\\n\"\n",
        "            \n",
        "            # Add exclusion criteria\n",
        "            exclusion_criteria = criteria_df[criteria_df['type'] == 'exclusion']\n",
        "            if len(exclusion_criteria) > 0:\n",
        "                criteria_text += \"\\n**EXCLUSION CRITERIA:**\\n\"\n",
        "                for _, row in exclusion_criteria.iterrows():\n",
        "                    criteria_text += f\"- **{row['criterion_id']}**: {row['description']}\\n\"\n",
        "                    if pd.notna(row['examples']) and row['examples'].strip():\n",
        "                        criteria_text += f\"  *Examples: {row['examples']}*\\n\"\n",
        "            \n",
        "            return criteria_text\n",
        "        except Exception as e:\n",
        "            return f\"Error loading criteria: {e}\"\n",
        "    \n",
        "    def load_examples_text(examples_file):\n",
        "        if not examples_file:\n",
        "            return \"\"\n",
        "        try:\n",
        "            examples_df = pd.read_csv(examples_file)\n",
        "            examples_text = \"\\n## EXAMPLE DECISIONS:\\n\"\n",
        "            \n",
        "            for _, row in examples_df.iterrows():\n",
        "                decision_label = \"INCLUDE\" if row['decision'].upper() == 'INCLUDE' else \"EXCLUDE\"\n",
        "                examples_text += f\"\\n**{decision_label} Example:**\\n\"\n",
        "                examples_text += f\"*Title:* {row['title']}\\n\"\n",
        "                examples_text += f\"*Abstract:* {row['abstract_text'][:200]}{'...' if len(row['abstract_text']) > 200 else ''}\\n\"\n",
        "                examples_text += f\"‚Üí **{decision_label}** ({row['reasoning']})\\n\"\n",
        "            \n",
        "            return examples_text\n",
        "        except Exception as e:\n",
        "            return f\"\\n## EXAMPLES:\\nError loading examples: {e}\\n\"\n",
        "    \n",
        "    # Load prompt components\n",
        "    criteria_text = load_criteria_text(CRITERIA_FILE)\n",
        "    examples_section = load_examples_text(EXAMPLES_FILE) if EXAMPLES_FILE else \"\"\n",
        "    \n",
        "    # Initialize results list\n",
        "    results_list = []\n",
        "    \n",
        "    # Calculate number of batches\n",
        "    total_examples = len(test_samples)\n",
        "    num_batches = (total_examples + batch_size - 1) // batch_size  # Ceiling division\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"üì¶ Processing {total_examples} examples in {num_batches} batch(es)\")\n",
        "        print(f\"‚è±Ô∏è  Estimated time: ~{num_batches * 2} minutes (2 min per batch)\")\n",
        "    \n",
        "    # Process examples in batches\n",
        "    for batch_idx in range(num_batches):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = min(start_idx + batch_size, total_examples)\n",
        "        batch_samples = test_samples.iloc[start_idx:end_idx]\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"\\nüîÑ Processing Batch {batch_idx + 1}/{num_batches} (examples {start_idx + 1}-{end_idx})\")\n",
        "        \n",
        "        batch_start_time = time.time()\n",
        "        \n",
        "        for idx, row in batch_samples.iterrows():\n",
        "            sample_number = start_idx + (idx - batch_samples.index[0]) + 1\n",
        "            \n",
        "            try:\n",
        "                # Create complete prompt\n",
        "                complete_prompt = USER_PROMPT_TEMPLATE.format(\n",
        "                    topic=TOPIC,\n",
        "                    domain=DOMAIN,\n",
        "                    criteria_text=criteria_text,\n",
        "                    examples_section=examples_section,\n",
        "                    title=row['title_full'],\n",
        "                    abstract=row['abstract'],\n",
        "                    decision_include=DECISION_OPTIONS[0],\n",
        "                    decision_exclude=DECISION_OPTIONS[1]\n",
        "                )\n",
        "                \n",
        "                # Call LLM\n",
        "                llm_result = screen_abstract_llm(\n",
        "                    abstract_text=complete_prompt,\n",
        "                    system_prompt=SYSTEM_PROMPT,\n",
        "                    user_prompt_template=\"{abstract}\",  # Just pass through since we formatted above\n",
        "                    model=MODEL_NAME,\n",
        "                    temperature=TEMPERATURE\n",
        "                )\n",
        "                \n",
        "                # Parse LLM decision\n",
        "                llm_decision = llm_result.get('decision', 'UNKNOWN')\n",
        "                llm_reasoning = llm_result.get('reasoning', 'No reasoning provided')\n",
        "                \n",
        "                # Convert to binary for evaluation\n",
        "                llm_binary = 1 if llm_decision == 'INCLUDE' else 0\n",
        "                stage2_binary = 1 if row['stage_2'] else 0\n",
        "                stage3_binary = 1 if row['stage_3'] else 0\n",
        "                \n",
        "                # Store result\n",
        "                result_row = {\n",
        "                    'example_id': sample_number,\n",
        "                    'title': row['title_full'],\n",
        "                    'abstract': row['abstract'],\n",
        "                    'stage_2_true': row['stage_2'],\n",
        "                    'stage_3_true': row['stage_3'],\n",
        "                    'stage_2_binary': stage2_binary,\n",
        "                    'stage_3_binary': stage3_binary,\n",
        "                    'llm_decision': llm_decision,\n",
        "                    'llm_binary': llm_binary,\n",
        "                    'llm_reasoning': llm_reasoning,\n",
        "                    'experiment_id': EXPERIMENT_ID,\n",
        "                    'dataset_source': dataset_source,\n",
        "                    'system_prompt_id': SYSTEM_PROMPT_ID,\n",
        "                    'user_prompt_id': USER_PROMPT_ID,\n",
        "                    'model': MODEL_NAME,\n",
        "                    'temperature': TEMPERATURE,\n",
        "                    'timestamp': datetime.now().isoformat()\n",
        "                }\n",
        "                \n",
        "                results_list.append(result_row)\n",
        "                \n",
        "            except Exception as e:\n",
        "                if verbose:\n",
        "                    print(f\"‚ùå Error processing example {sample_number}: {e}\")\n",
        "                \n",
        "                # Store error result\n",
        "                result_row = {\n",
        "                    'example_id': sample_number,\n",
        "                    'title': row['title_full'],\n",
        "                    'abstract': row['abstract'],\n",
        "                    'stage_2_true': row['stage_2'],\n",
        "                    'stage_3_true': row['stage_3'],\n",
        "                    'stage_2_binary': 1 if row['stage_2'] else 0,\n",
        "                    'stage_3_binary': 1 if row['stage_3'] else 0,\n",
        "                    'llm_decision': 'ERROR',\n",
        "                    'llm_binary': 0,\n",
        "                    'llm_reasoning': f'Processing error: {e}',\n",
        "                    'experiment_id': EXPERIMENT_ID,\n",
        "                    'dataset_source': dataset_source,\n",
        "                    'system_prompt_id': SYSTEM_PROMPT_ID,\n",
        "                    'user_prompt_id': USER_PROMPT_ID,\n",
        "                    'model': MODEL_NAME,\n",
        "                    'temperature': TEMPERATURE,\n",
        "                    'timestamp': datetime.now().isoformat()\n",
        "                }\n",
        "                \n",
        "                results_list.append(result_row)\n",
        "        \n",
        "        # Batch completion info\n",
        "        batch_time = time.time() - batch_start_time\n",
        "        if verbose:\n",
        "            print(f\"‚úÖ Batch {batch_idx + 1} completed in {batch_time:.1f}s\")\n",
        "            if batch_idx < num_batches - 1:  # Not the last batch\n",
        "                print(f\"‚è≥ Brief pause before next batch...\")\n",
        "                time.sleep(2)  # Small delay between batches\n",
        "    \n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame(results_list)\n",
        "    \n",
        "   # Calculate detailed metrics for stage_2\n",
        "    valid_results_stage2 = results_df[results_df['llm_decision'] != 'ERROR']\n",
        "    if len(valid_results_stage2) > 0:\n",
        "        y_true_stage2 = valid_results_stage2['stage_2_binary'].values\n",
        "        y_pred_stage2 = valid_results_stage2['llm_binary'].values\n",
        "        \n",
        "        # Basic metrics\n",
        "        accuracy_stage2 = accuracy_score(y_true_stage2, y_pred_stage2)\n",
        "        precision_stage2 = precision_score(y_true_stage2, y_pred_stage2, zero_division=0)\n",
        "        recall_stage2 = recall_score(y_true_stage2, y_pred_stage2, zero_division=0)\n",
        "        f1_stage2 = f1_score(y_true_stage2, y_pred_stage2, zero_division=0)\n",
        "        \n",
        "        # Confusion matrix metrics\n",
        "        tn2, fp2, fn2, tp2 = confusion_matrix(y_true_stage2, y_pred_stage2).ravel()\n",
        "    else:\n",
        "        accuracy_stage2 = precision_stage2 = recall_stage2 = f1_stage2 = 0.0\n",
        "        tp2 = fp2 = tn2 = fn2 = 0\n",
        "    \n",
        "    # Calculate detailed metrics for stage_3\n",
        "    valid_results_stage3 = results_df[results_df['llm_decision'] != 'ERROR']\n",
        "    if len(valid_results_stage3) > 0:\n",
        "        y_true_stage3 = valid_results_stage3['stage_3_binary'].values\n",
        "        y_pred_stage3 = valid_results_stage3['llm_binary'].values\n",
        "        \n",
        "        # Basic metrics\n",
        "        accuracy_stage3 = accuracy_score(y_true_stage3, y_pred_stage3)\n",
        "        precision_stage3 = precision_score(y_true_stage3, y_pred_stage3, zero_division=0)\n",
        "        recall_stage3 = recall_score(y_true_stage3, y_pred_stage3, zero_division=0)\n",
        "        f1_stage3 = f1_score(y_true_stage3, y_pred_stage3, zero_division=0)\n",
        "        \n",
        "        # Confusion matrix metrics\n",
        "        tn3, fp3, fn3, tp3 = confusion_matrix(y_true_stage3, y_pred_stage3).ravel()\n",
        "    else:\n",
        "        accuracy_stage3 = precision_stage3 = recall_stage3 = f1_stage3 = 0.0\n",
        "        tp3 = fp3 = tn3 = fn3 = 0\n",
        "    \n",
        "    # Updated metrics dictionary\n",
        "    metrics = {\n",
        "        'stage_2_metrics': {\n",
        "            'accuracy': accuracy_stage2,\n",
        "            'precision': precision_stage2,\n",
        "            'recall': recall_stage2,\n",
        "            'f1_score': f1_stage2,\n",
        "            'tp': int(tp2),\n",
        "            'fp': int(fp2),\n",
        "            'tn': int(tn2),\n",
        "            'fn': int(fn2)\n",
        "        },\n",
        "        'stage_3_metrics': {\n",
        "            'accuracy': accuracy_stage3,\n",
        "            'precision': precision_stage3,\n",
        "            'recall': recall_stage3,\n",
        "            'f1_score': f1_stage3,\n",
        "            'tp': int(tp3),\n",
        "            'fp': int(fp3),\n",
        "            'tn': int(tn3),\n",
        "            'fn': int(fn3)\n",
        "        },\n",
        "        'total_examples': len(results_df),\n",
        "        'successful_classifications': len(valid_results_stage2),\n",
        "        'errors': len(results_df) - len(valid_results_stage2)\n",
        "    }\n",
        "    \n",
        "    # Enhanced results printing\n",
        "    if verbose:\n",
        "        print(f\"\\nüìä EXPERIMENT RESULTS\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"üìà Stage 2 Evaluation:\")\n",
        "        print(f\"   Accuracy:  {accuracy_stage2:.3f}\")\n",
        "        print(f\"   Precision: {precision_stage2:.3f}\")\n",
        "        print(f\"   Recall:    {recall_stage2:.3f}\")\n",
        "        print(f\"   F1 Score:  {f1_stage2:.3f}\")\n",
        "        print(f\"   TP: {tp2}, FP: {fp2}, TN: {tn2}, FN: {fn2}\")\n",
        "        print(f\"\\nüìà Stage 3 Evaluation:\")\n",
        "        print(f\"   Accuracy:  {accuracy_stage3:.3f}\")\n",
        "        print(f\"   Precision: {precision_stage3:.3f}\")\n",
        "        print(f\"   Recall:    {recall_stage3:.3f}\")\n",
        "        print(f\"   F1 Score:  {f1_stage3:.3f}\")\n",
        "        print(f\"   TP: {tp3}, FP: {fp3}, TN: {tn3}, FN: {fn3}\")\n",
        "        print(f\"\\nüìã Processing Summary:\")\n",
        "        print(f\"   Total examples: {len(results_df)}\")\n",
        "        print(f\"   Successful: {len(valid_results_stage2)}\")\n",
        "        print(f\"   Errors: {len(results_df) - len(valid_results_stage2)}\")\n",
        "    \n",
        "    # Save results\n",
        "    if save_results:\n",
        "        # Create filename with timestamp\n",
        "        timestamp = datetime.now().strftime(\"%m%d%H%M\")\n",
        "        filename = f\"{EXPERIMENT_ID}_{dataset_source}_{timestamp}.csv\"\n",
        "        results_dir = \"../results\"\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "        output_path = os.path.join(results_dir, filename)\n",
        "        \n",
        "        results_df.to_csv(output_path, index=False)\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"\\nüíæ Results saved to: {output_path}\")\n",
        "    \n",
        "    return {\n",
        "        'results_df': results_df,\n",
        "        'metrics': metrics,\n",
        "        'filename': filename if save_results else None\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Classification experiment function with batch processing defined\")\n",
        "print(\"üöÄ Ready to run: run_classification_experiment(df_LB, batch_size=15)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî¨ Set Up Function - YES/MAYBE/NO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix \n",
        "from datetime import datetime\n",
        "import os\n",
        "import time\n",
        "\n",
        "def run_classification_experiment(\n",
        "    df, \n",
        "    n_total_examples=50,  # ‚¨ÖÔ∏è Total number of examples to test\n",
        "    n_stage3_true=5,     # ‚¨ÖÔ∏è Number of stage_3=True examples\n",
        "    n_stage3_false=45,    # ‚¨ÖÔ∏è Number of stage_3=False examples\n",
        "    dataset_source=\"LB\",  # ‚¨ÖÔ∏è Dataset identifier (LB/BM)\n",
        "    batch_size=20,        # ‚¨ÖÔ∏è Batch size for processing (max 20 to avoid timeouts)\n",
        "    save_results=True,    # ‚¨ÖÔ∏è Whether to save results to CSV\n",
        "    verbose=True          # ‚¨ÖÔ∏è Print progress updates\n",
        "):\n",
        "    \"\"\"\n",
        "    Run LLM classification experiment on abstracts with batch processing.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with abstracts (must have 'abstract', 'title_full', 'stage_2', 'stage_3')\n",
        "        n_total_examples: Total number of examples to test\n",
        "        n_stage3_true: Number of stage_3=True examples to include\n",
        "        n_stage3_false: Number of stage_3=False examples to include\n",
        "        dataset_source: Dataset identifier for results filename\n",
        "        batch_size: Number of examples to process in each batch (max 20)\n",
        "        save_results: Whether to save results to CSV\n",
        "        verbose: Whether to print progress\n",
        "    \n",
        "    Returns:\n",
        "        dict: Results including metrics and DataFrame\n",
        "    \"\"\"\n",
        "    \n",
        "    # Validate batch size\n",
        "    if batch_size > 20:\n",
        "        print(\"‚ö†Ô∏è  Warning: Batch size > 20 may cause timeouts. Setting to 20.\")\n",
        "        batch_size = 20\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"üß™ Starting Classification Experiment with Batch Processing\")\n",
        "        print(f\"üìä Dataset: {dataset_source}\")\n",
        "        print(f\"üéØ Total examples: {n_total_examples}\")\n",
        "        print(f\"‚úÖ Stage 3 True: {n_stage3_true}\")\n",
        "        print(f\"‚ùå Stage 3 False: {n_stage3_false}\")\n",
        "        print(f\"üì¶ Batch size: {batch_size}\")\n",
        "        print(\"=\" * 50)\n",
        "    \n",
        "    # Sample examples\n",
        "    stage3_true_samples = df[df['stage_3'] == True].sample(n=n_stage3_true, random_state=42)\n",
        "    stage3_false_samples = df[df['stage_3'] == False].sample(n=n_stage3_false, random_state=42)\n",
        "    \n",
        "    # Combine samples\n",
        "    test_samples = pd.concat([stage3_true_samples, stage3_false_samples]).reset_index(drop=True)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"üìù Sampled {len(test_samples)} examples\")\n",
        "    \n",
        "    # Load criteria and examples text\n",
        "    def load_criteria_text(criteria_file):\n",
        "        try:\n",
        "            criteria_df = pd.read_csv(criteria_file)\n",
        "            criteria_text = \"\"\n",
        "            \n",
        "            # Add inclusion criteria\n",
        "            inclusion_criteria = criteria_df[criteria_df['type'] == 'inclusion']\n",
        "            if len(inclusion_criteria) > 0:\n",
        "                criteria_text += \"**INCLUSION CRITERIA:**\\n\"\n",
        "                for _, row in inclusion_criteria.iterrows():\n",
        "                    criteria_text += f\"- **{row['criterion_id']}**: {row['description']}\\n\"\n",
        "                    if pd.notna(row['examples']) and row['examples'].strip():\n",
        "                        criteria_text += f\"  *Examples: {row['examples']}*\\n\"\n",
        "            \n",
        "            # Add exclusion criteria\n",
        "            exclusion_criteria = criteria_df[criteria_df['type'] == 'exclusion']\n",
        "            if len(exclusion_criteria) > 0:\n",
        "                criteria_text += \"\\n**EXCLUSION CRITERIA:**\\n\"\n",
        "                for _, row in exclusion_criteria.iterrows():\n",
        "                    criteria_text += f\"- **{row['criterion_id']}**: {row['description']}\\n\"\n",
        "                    if pd.notna(row['examples']) and row['examples'].strip():\n",
        "                        criteria_text += f\"  *Examples: {row['examples']}*\\n\"\n",
        "            \n",
        "            return criteria_text\n",
        "        except Exception as e:\n",
        "            return f\"Error loading criteria: {e}\"\n",
        "    \n",
        "    def load_examples_text(examples_file):\n",
        "        if not examples_file:\n",
        "            return \"\"\n",
        "        try:\n",
        "            examples_df = pd.read_csv(examples_file)\n",
        "            examples_text = \"\\n## EXAMPLE DECISIONS:\\n\"\n",
        "            \n",
        "            for _, row in examples_df.iterrows():\n",
        "                decision_label = \"INCLUDE\" if row['decision'].upper() == 'INCLUDE' else \"EXCLUDE\"\n",
        "                examples_text += f\"\\n**{decision_label} Example:**\\n\"\n",
        "                examples_text += f\"*Title:* {row['title']}\\n\"\n",
        "                examples_text += f\"*Abstract:* {row['abstract_text'][:200]}{'...' if len(row['abstract_text']) > 200 else ''}\\n\"\n",
        "                examples_text += f\"‚Üí **{decision_label}** ({row['reasoning']})\\n\"\n",
        "            \n",
        "            return examples_text\n",
        "        except Exception as e:\n",
        "            return f\"\\n## EXAMPLES:\\nError loading examples: {e}\\n\"\n",
        "    \n",
        "    # Load prompt components\n",
        "    criteria_text = load_criteria_text(CRITERIA_FILE)\n",
        "    examples_section = load_examples_text(EXAMPLES_FILE) if EXAMPLES_FILE else \"\"\n",
        "    \n",
        "    # Initialize results list\n",
        "    results_list = []\n",
        "    \n",
        "    # Calculate number of batches\n",
        "    total_examples = len(test_samples)\n",
        "    num_batches = (total_examples + batch_size - 1) // batch_size  # Ceiling division\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"üì¶ Processing {total_examples} examples in {num_batches} batch(es)\")\n",
        "        print(f\"‚è±Ô∏è  Estimated time: ~{num_batches * 2} minutes (2 min per batch)\")\n",
        "    \n",
        "    # Process examples in batches\n",
        "    for batch_idx in range(num_batches):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = min(start_idx + batch_size, total_examples)\n",
        "        batch_samples = test_samples.iloc[start_idx:end_idx]\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"\\nüîÑ Processing Batch {batch_idx + 1}/{num_batches} (examples {start_idx + 1}-{end_idx})\")\n",
        "        \n",
        "        batch_start_time = time.time()\n",
        "        \n",
        "        for idx, row in batch_samples.iterrows():\n",
        "            sample_number = start_idx + (idx - batch_samples.index[0]) + 1\n",
        "            \n",
        "            try:\n",
        "                # Create complete prompt\n",
        "                complete_prompt = USER_PROMPT_TEMPLATE.format(\n",
        "                    topic=TOPIC,\n",
        "                    domain=DOMAIN,\n",
        "                    criteria_text=criteria_text,\n",
        "                    examples_section=examples_section,\n",
        "                    title=row['title_full'],\n",
        "                    abstract=row['abstract'],\n",
        "                    decision_include=DECISION_OPTIONS[0],\n",
        "                    decision_exclude=DECISION_OPTIONS[1]\n",
        "                )\n",
        "                \n",
        "                # Call LLM\n",
        "                llm_result = screen_abstract_llm(\n",
        "                    abstract_text=complete_prompt,\n",
        "                    system_prompt=SYSTEM_PROMPT,\n",
        "                    user_prompt_template=\"{abstract}\",  # Just pass through since we formatted above\n",
        "                    model=MODEL_NAME,\n",
        "                    temperature=TEMPERATURE\n",
        "                )\n",
        "                \n",
        "                # Parse LLM decision\n",
        "                llm_decision = llm_result.get('decision', 'UNKNOWN')\n",
        "                llm_reasoning = llm_result.get('reasoning', 'No reasoning provided')\n",
        "                \n",
        "                # Convert to binary for evaluation - MAYBE counts as INCLUDE (1) for stage_2 comparison\n",
        "                llm_binary = 1 if llm_decision in ['INCLUDE', 'MAYBE'] else 0\n",
        "                stage2_binary = 1 if row['stage_2'] else 0\n",
        "                stage3_binary = 1 if row['stage_3'] else 0\n",
        "                \n",
        "                # Store result\n",
        "                result_row = {\n",
        "                    'example_id': sample_number,\n",
        "                    'title': row['title_full'],\n",
        "                    'abstract': row['abstract'],\n",
        "                    'stage_2_true': row['stage_2'],\n",
        "                    'stage_3_true': row['stage_3'],\n",
        "                    'stage_2_binary': stage2_binary,\n",
        "                    'stage_3_binary': stage3_binary,\n",
        "                    'llm_decision': llm_decision,\n",
        "                    'llm_binary': llm_binary,\n",
        "                    'llm_reasoning': llm_reasoning,\n",
        "                    'experiment_id': EXPERIMENT_ID,\n",
        "                    'dataset_source': dataset_source,\n",
        "                    'system_prompt_id': SYSTEM_PROMPT_ID,\n",
        "                    'user_prompt_id': USER_PROMPT_ID,\n",
        "                    'model': MODEL_NAME,\n",
        "                    'temperature': TEMPERATURE,\n",
        "                    'timestamp': datetime.now().isoformat()\n",
        "                }\n",
        "                \n",
        "                results_list.append(result_row)\n",
        "                \n",
        "            except Exception as e:\n",
        "                if verbose:\n",
        "                    print(f\"‚ùå Error processing example {sample_number}: {e}\")\n",
        "                \n",
        "                # Store error result\n",
        "                result_row = {\n",
        "                    'example_id': sample_number,\n",
        "                    'title': row['title_full'],\n",
        "                    'abstract': row['abstract'],\n",
        "                    'stage_2_true': row['stage_2'],\n",
        "                    'stage_3_true': row['stage_3'],\n",
        "                    'stage_2_binary': 1 if row['stage_2'] else 0,\n",
        "                    'stage_3_binary': 1 if row['stage_3'] else 0,\n",
        "                    'llm_decision': 'ERROR',\n",
        "                    'llm_binary': 0,\n",
        "                    'llm_reasoning': f'Processing error: {e}',\n",
        "                    'experiment_id': EXPERIMENT_ID,\n",
        "                    'dataset_source': dataset_source,\n",
        "                    'system_prompt_id': SYSTEM_PROMPT_ID,\n",
        "                    'user_prompt_id': USER_PROMPT_ID,\n",
        "                    'model': MODEL_NAME,\n",
        "                    'temperature': TEMPERATURE,\n",
        "                    'timestamp': datetime.now().isoformat()\n",
        "                }\n",
        "                \n",
        "                results_list.append(result_row)\n",
        "        \n",
        "        # Batch completion info\n",
        "        batch_time = time.time() - batch_start_time\n",
        "        if verbose:\n",
        "            print(f\"‚úÖ Batch {batch_idx + 1} completed in {batch_time:.1f}s\")\n",
        "            if batch_idx < num_batches - 1:  # Not the last batch\n",
        "                print(f\"‚è≥ Brief pause before next batch...\")\n",
        "                time.sleep(2)  # Small delay between batches\n",
        "    \n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame(results_list)\n",
        "    \n",
        "    # Calculate detailed metrics for stage_2 (MAYBE counts as positive)\n",
        "    valid_results_stage2 = results_df[results_df['llm_decision'] != 'ERROR']\n",
        "    if len(valid_results_stage2) > 0:\n",
        "        y_true_stage2 = valid_results_stage2['stage_2_binary'].values\n",
        "        y_pred_stage2 = valid_results_stage2['llm_binary'].values\n",
        "        \n",
        "        # Basic metrics\n",
        "        accuracy_stage2 = accuracy_score(y_true_stage2, y_pred_stage2)\n",
        "        precision_stage2 = precision_score(y_true_stage2, y_pred_stage2, zero_division=0)\n",
        "        recall_stage2 = recall_score(y_true_stage2, y_pred_stage2, zero_division=0)\n",
        "        f1_stage2 = f1_score(y_true_stage2, y_pred_stage2, zero_division=0)\n",
        "        \n",
        "        # Confusion matrix metrics\n",
        "        tn2, fp2, fn2, tp2 = confusion_matrix(y_true_stage2, y_pred_stage2).ravel()\n",
        "    else:\n",
        "        accuracy_stage2 = precision_stage2 = recall_stage2 = f1_stage2 = 0.0\n",
        "        tp2 = fp2 = tn2 = fn2 = 0\n",
        "    \n",
        "    # Get value counts for LLM decisions\n",
        "    llm_decision_counts = valid_results_stage2['llm_decision'].value_counts() if len(valid_results_stage2) > 0 else {}\n",
        "    \n",
        "    # Updated metrics dictionary (only stage_2, no stage_3)\n",
        "    metrics = {\n",
        "        'stage_2_metrics': {\n",
        "            'accuracy': accuracy_stage2,\n",
        "            'precision': precision_stage2,\n",
        "            'recall': recall_stage2,\n",
        "            'f1_score': f1_stage2,\n",
        "            'tp': int(tp2),\n",
        "            'fp': int(fp2),\n",
        "            'tn': int(tn2),\n",
        "            'fn': int(fn2)\n",
        "        },\n",
        "        'decision_counts': {\n",
        "            'INCLUDE': int(llm_decision_counts.get('INCLUDE', 0)),\n",
        "            'MAYBE': int(llm_decision_counts.get('MAYBE', 0)),\n",
        "            'EXCLUDE': int(llm_decision_counts.get('EXCLUDE', 0)),\n",
        "            'ERROR': int(llm_decision_counts.get('ERROR', 0))\n",
        "        },\n",
        "        'total_examples': len(results_df),\n",
        "        'successful_classifications': len(valid_results_stage2),\n",
        "        'errors': len(results_df) - len(valid_results_stage2)\n",
        "    }\n",
        "    \n",
        "    # Enhanced results printing\n",
        "    if verbose:\n",
        "        print(f\"\\nüìä EXPERIMENT RESULTS\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"üìà Stage 2 Evaluation (MAYBE counted as INCLUDE):\")\n",
        "        print(f\"   Accuracy:  {accuracy_stage2:.3f}\")\n",
        "        print(f\"   Precision: {precision_stage2:.3f}\")\n",
        "        print(f\"   Recall:    {recall_stage2:.3f}\")\n",
        "        print(f\"   F1 Score:  {f1_stage2:.3f}\")\n",
        "        print(f\"   TP: {tp2}, FP: {fp2}, TN: {tn2}, FN: {fn2}\")\n",
        "        print(f\"\\nüìä LLM Decision Distribution:\")\n",
        "        print(f\"   INCLUDE: {metrics['decision_counts']['INCLUDE']}\")\n",
        "        print(f\"   MAYBE:   {metrics['decision_counts']['MAYBE']}\")\n",
        "        print(f\"   EXCLUDE: {metrics['decision_counts']['EXCLUDE']}\")\n",
        "        print(f\"   ERROR:   {metrics['decision_counts']['ERROR']}\")\n",
        "        print(f\"\\nüìã Processing Summary:\")\n",
        "        print(f\"   Total examples: {len(results_df)}\")\n",
        "        print(f\"   Successful: {len(valid_results_stage2)}\")\n",
        "        print(f\"   Errors: {len(results_df) - len(valid_results_stage2)}\")\n",
        "    \n",
        "    # Save results\n",
        "    if save_results:\n",
        "        # Create filename with timestamp\n",
        "        timestamp = datetime.now().strftime(\"%m%d%H%M\")\n",
        "        filename = f\"{EXPERIMENT_ID}_{dataset_source}_{timestamp}.csv\"\n",
        "        results_dir = \"../results\"\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "        output_path = os.path.join(results_dir, filename)\n",
        "        \n",
        "        results_df.to_csv(output_path, index=False)\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"\\nüíæ Results saved to: {output_path}\")\n",
        "    \n",
        "    return {\n",
        "        'results_df': results_df,\n",
        "        'metrics': metrics,\n",
        "        'filename': filename if save_results else None\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Classification experiment function with MAYBE option support defined\")\n",
        "print(\"üöÄ Ready to run: run_classification_experiment(df_LB, batch_size=15)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî¨ Set up Function - LIKERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix \n",
        "from datetime import datetime\n",
        "import os\n",
        "import time\n",
        "\n",
        "def run_classification_experiment(\n",
        "    df, \n",
        "    n_total_examples=50,  # ‚¨ÖÔ∏è Total number of examples to test\n",
        "    n_stage3_true=5,     # ‚¨ÖÔ∏è Number of stage_3=True examples\n",
        "    n_stage3_false=45,    # ‚¨ÖÔ∏è Number of stage_3=False examples\n",
        "    dataset_source=\"LB\",  # ‚¨ÖÔ∏è Dataset identifier (LB/BM)\n",
        "    batch_size=20,        # ‚¨ÖÔ∏è Batch size for processing (max 20 to avoid timeouts)\n",
        "    save_results=True,    # ‚¨ÖÔ∏è Whether to save results to CSV\n",
        "    verbose=True          # ‚¨ÖÔ∏è Print progress updates\n",
        "):\n",
        "    \"\"\"\n",
        "    Run LLM classification experiment on abstracts with batch processing.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with abstracts (must have 'abstract', 'title_full', 'stage_2', 'stage_3')\n",
        "        n_total_examples: Total number of examples to test\n",
        "        n_stage3_true: Number of stage_3=True examples to include\n",
        "        n_stage3_false: Number of stage_3=False examples to include\n",
        "        dataset_source: Dataset identifier for results filename\n",
        "        batch_size: Number of examples to process in each batch (max 20)\n",
        "        save_results: Whether to save results to CSV\n",
        "        verbose: Whether to print progress\n",
        "    \n",
        "    Returns:\n",
        "        dict: Results including metrics and DataFrame\n",
        "    \"\"\"\n",
        "    \n",
        "    # Validate batch size\n",
        "    if batch_size > 20:\n",
        "        print(\"‚ö†Ô∏è  Warning: Batch size > 20 may cause timeouts. Setting to 20.\")\n",
        "        batch_size = 20\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"üß™ Starting Classification Experiment with Batch Processing\")\n",
        "        print(f\"üìä Dataset: {dataset_source}\")\n",
        "        print(f\"üéØ Total examples: {n_total_examples}\")\n",
        "        print(f\"‚úÖ Stage 3 True: {n_stage3_true}\")\n",
        "        print(f\"‚ùå Stage 3 False: {n_stage3_false}\")\n",
        "        print(f\"üì¶ Batch size: {batch_size}\")\n",
        "        print(\"=\" * 50)\n",
        "    \n",
        "    # Sample examples\n",
        "    stage3_true_samples = df[df['stage_3'] == True].sample(n=n_stage3_true, random_state=42)\n",
        "    stage3_false_samples = df[df['stage_3'] == False].sample(n=n_stage3_false, random_state=42)\n",
        "    \n",
        "    # Combine samples\n",
        "    test_samples = pd.concat([stage3_true_samples, stage3_false_samples]).reset_index(drop=True)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"üìù Sampled {len(test_samples)} examples\")\n",
        "    \n",
        "    # Load criteria and examples text\n",
        "    def load_criteria_text(criteria_file):\n",
        "        try:\n",
        "            criteria_df = pd.read_csv(criteria_file)\n",
        "            criteria_text = \"\"\n",
        "            \n",
        "            # Add inclusion criteria\n",
        "            inclusion_criteria = criteria_df[criteria_df['type'] == 'inclusion']\n",
        "            if len(inclusion_criteria) > 0:\n",
        "                criteria_text += \"**INCLUSION CRITERIA:**\\n\"\n",
        "                for _, row in inclusion_criteria.iterrows():\n",
        "                    criteria_text += f\"- **{row['criterion_id']}**: {row['description']}\\n\"\n",
        "                    if pd.notna(row['examples']) and row['examples'].strip():\n",
        "                        criteria_text += f\"  *Examples: {row['examples']}*\\n\"\n",
        "            \n",
        "            # Add exclusion criteria\n",
        "            exclusion_criteria = criteria_df[criteria_df['type'] == 'exclusion']\n",
        "            if len(exclusion_criteria) > 0:\n",
        "                criteria_text += \"\\n**EXCLUSION CRITERIA:**\\n\"\n",
        "                for _, row in exclusion_criteria.iterrows():\n",
        "                    criteria_text += f\"- **{row['criterion_id']}**: {row['description']}\\n\"\n",
        "                    if pd.notna(row['examples']) and row['examples'].strip():\n",
        "                        criteria_text += f\"  *Examples: {row['examples']}*\\n\"\n",
        "            \n",
        "            return criteria_text\n",
        "        except Exception as e:\n",
        "            return f\"Error loading criteria: {e}\"\n",
        "    \n",
        "    def load_examples_text(examples_file):\n",
        "        if not examples_file:\n",
        "            return \"\"\n",
        "        try:\n",
        "            examples_df = pd.read_csv(examples_file)\n",
        "            examples_text = \"\\n## EXAMPLE DECISIONS:\\n\"\n",
        "            \n",
        "            for _, row in examples_df.iterrows():\n",
        "                decision_label = \"INCLUDE\" if row['decision'].upper() == 'INCLUDE' else \"EXCLUDE\"\n",
        "                examples_text += f\"\\n**{decision_label} Example:**\\n\"\n",
        "                examples_text += f\"*Title:* {row['title']}\\n\"\n",
        "                examples_text += f\"*Abstract:* {row['abstract_text'][:200]}{'...' if len(row['abstract_text']) > 200 else ''}\\n\"\n",
        "                examples_text += f\"‚Üí **{decision_label}** ({row['reasoning']})\\n\"\n",
        "            \n",
        "            return examples_text\n",
        "        except Exception as e:\n",
        "            return f\"\\n## EXAMPLES:\\nError loading examples: {e}\\n\"\n",
        "    \n",
        "    # Load prompt components\n",
        "    criteria_text = load_criteria_text(CRITERIA_FILE)\n",
        "    examples_section = load_examples_text(EXAMPLES_FILE) if EXAMPLES_FILE else \"\"\n",
        "    \n",
        "    # Initialize results list\n",
        "    results_list = []\n",
        "    \n",
        "    # Calculate number of batches\n",
        "    total_examples = len(test_samples)\n",
        "    num_batches = (total_examples + batch_size - 1) // batch_size  # Ceiling division\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"üì¶ Processing {total_examples} examples in {num_batches} batch(es)\")\n",
        "        print(f\"‚è±Ô∏è  Estimated time: ~{num_batches * 2} minutes (2 min per batch)\")\n",
        "    \n",
        "    # Process examples in batches\n",
        "    for batch_idx in range(num_batches):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = min(start_idx + batch_size, total_examples)\n",
        "        batch_samples = test_samples.iloc[start_idx:end_idx]\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"\\nüîÑ Processing Batch {batch_idx + 1}/{num_batches} (examples {start_idx + 1}-{end_idx})\")\n",
        "        \n",
        "        batch_start_time = time.time()\n",
        "        \n",
        "        for idx, row in batch_samples.iterrows():\n",
        "            sample_number = start_idx + (idx - batch_samples.index[0]) + 1\n",
        "            \n",
        "            try:\n",
        "                # Create complete prompt\n",
        "                complete_prompt = USER_PROMPT_TEMPLATE.format(\n",
        "                    topic=TOPIC,\n",
        "                    domain=DOMAIN,\n",
        "                    criteria_text=criteria_text,\n",
        "                    examples_section=examples_section,\n",
        "                    title=row['title_full'],\n",
        "                    abstract=row['abstract']\n",
        "                )\n",
        "                \n",
        "                # Call LLM\n",
        "                llm_result = screen_abstract_llm(\n",
        "                    abstract_text=complete_prompt,\n",
        "                    system_prompt=SYSTEM_PROMPT,\n",
        "                    user_prompt_template=\"{abstract}\",  # Just pass through since we formatted above\n",
        "                    model=MODEL_NAME,\n",
        "                    temperature=TEMPERATURE\n",
        "                )\n",
        "                \n",
        "                # Parse LLM decision - extract Likert score from response\n",
        "                llm_reasoning = llm_result.get('reasoning', 'No reasoning provided')\n",
        "                \n",
        "                # Extract Likert score (1-5) from the response\n",
        "                llm_score = None\n",
        "                for score in ['5', '4', '3', '2', '1']:  # Check in order of preference\n",
        "                    if f\"Relevance Score:** {score}\" in llm_reasoning or f\"**{score}\" in llm_reasoning:\n",
        "                        llm_score = int(score)\n",
        "                        break\n",
        "                \n",
        "                # Fallback: look for any number 1-5 in the response\n",
        "                if llm_score is None:\n",
        "                    import re\n",
        "                    scores = re.findall(r'\\b[1-5]\\b', llm_reasoning)\n",
        "                    if scores:\n",
        "                        llm_score = int(scores[0])\n",
        "                    else:\n",
        "                        llm_score = 3  # Default to middle score if no score found\n",
        "                \n",
        "                # Convert Likert scores to binary for evaluation\n",
        "                # Stage 2: 3,4,5 = positive (worth further review)\n",
        "                stage2_llm_binary = 1 if llm_score >= 3 else 0\n",
        "                # Stage 3: 4,5 = positive (definitely include)  \n",
        "                stage3_llm_binary = 1 if llm_score >= 4 else 0\n",
        "                \n",
        "                stage2_binary = 1 if row['stage_2'] else 0\n",
        "                stage3_binary = 1 if row['stage_3'] else 0\n",
        "                \n",
        "                # Store result\n",
        "                result_row = {\n",
        "                    'example_id': sample_number,\n",
        "                    'title': row['title_full'],\n",
        "                    'abstract': row['abstract'],\n",
        "                    'stage_2_true': row['stage_2'],\n",
        "                    'stage_3_true': row['stage_3'],\n",
        "                    'stage_2_binary': stage2_binary,\n",
        "                    'stage_3_binary': stage3_binary,\n",
        "                    'llm_score': llm_score,  # New: Likert score (1-5)\n",
        "                    'llm_decision': str(llm_score),  # For compatibility\n",
        "                    'stage2_llm_binary': stage2_llm_binary,  # New: Binary for stage 2 (3+ = positive)\n",
        "                    'stage3_llm_binary': stage3_llm_binary,  # New: Binary for stage 3 (4+ = positive)\n",
        "                    'llm_reasoning': llm_reasoning,\n",
        "                    'experiment_id': EXPERIMENT_ID,\n",
        "                    'dataset_source': dataset_source,\n",
        "                    'system_prompt_id': SYSTEM_PROMPT_ID,\n",
        "                    'user_prompt_id': USER_PROMPT_ID,\n",
        "                    'model': MODEL_NAME,\n",
        "                    'temperature': TEMPERATURE,\n",
        "                    'timestamp': datetime.now().isoformat()\n",
        "                }\n",
        "                \n",
        "                results_list.append(result_row)\n",
        "                \n",
        "            except Exception as e:\n",
        "                if verbose:\n",
        "                    print(f\"‚ùå Error processing example {sample_number}: {e}\")\n",
        "                \n",
        "                # Store error result\n",
        "                result_row = {\n",
        "                    'example_id': sample_number,\n",
        "                    'title': row['title_full'],\n",
        "                    'abstract': row['abstract'],\n",
        "                    'stage_2_true': row['stage_2'],\n",
        "                    'stage_3_true': row['stage_3'],\n",
        "                    'stage_2_binary': 1 if row['stage_2'] else 0,\n",
        "                    'stage_3_binary': 1 if row['stage_3'] else 0,\n",
        "                    'llm_score': 0,  # Error case\n",
        "                    'llm_decision': 'ERROR',\n",
        "                    'stage2_llm_binary': 0,\n",
        "                    'stage3_llm_binary': 0,\n",
        "                    'llm_reasoning': f'Processing error: {e}',\n",
        "                    'experiment_id': EXPERIMENT_ID,\n",
        "                    'dataset_source': dataset_source,\n",
        "                    'system_prompt_id': SYSTEM_PROMPT_ID,\n",
        "                    'user_prompt_id': USER_PROMPT_ID,\n",
        "                    'model': MODEL_NAME,\n",
        "                    'temperature': TEMPERATURE,\n",
        "                    'timestamp': datetime.now().isoformat()\n",
        "                }\n",
        "                \n",
        "                results_list.append(result_row)\n",
        "        \n",
        "        # Batch completion info\n",
        "        batch_time = time.time() - batch_start_time\n",
        "        if verbose:\n",
        "            print(f\"‚úÖ Batch {batch_idx + 1} completed in {batch_time:.1f}s\")\n",
        "            if batch_idx < num_batches - 1:  # Not the last batch\n",
        "                print(f\"‚è≥ Brief pause before next batch...\")\n",
        "                time.sleep(2)  # Small delay between batches\n",
        "    \n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame(results_list)\n",
        "    \n",
        "    # Filter out errors for analysis\n",
        "    valid_results = results_df[results_df['llm_decision'] != 'ERROR']\n",
        "    \n",
        "    # LIKERT SCALE ANALYSIS\n",
        "    if len(valid_results) > 0:\n",
        "        # Overall Likert distribution\n",
        "        likert_counts = valid_results['llm_score'].value_counts().sort_index()\n",
        "        \n",
        "        # Likert distribution by Stage 2 ground truth\n",
        "        stage2_true_scores = valid_results[valid_results['stage_2_true'] == True]['llm_score'].value_counts().sort_index()\n",
        "        stage2_false_scores = valid_results[valid_results['stage_2_true'] == False]['llm_score'].value_counts().sort_index()\n",
        "        \n",
        "        # Likert distribution by Stage 3 ground truth\n",
        "        stage3_true_scores = valid_results[valid_results['stage_3_true'] == True]['llm_score'].value_counts().sort_index()\n",
        "        stage3_false_scores = valid_results[valid_results['stage_3_true'] == False]['llm_score'].value_counts().sort_index()\n",
        "        \n",
        "        # BINARY CLASSIFICATION METRICS\n",
        "        # Stage 2 evaluation (3,4,5 vs 1,2)\n",
        "        y_true_stage2 = valid_results['stage_2_binary'].values\n",
        "        y_pred_stage2 = valid_results['stage2_llm_binary'].values\n",
        "        \n",
        "        accuracy_stage2 = accuracy_score(y_true_stage2, y_pred_stage2)\n",
        "        precision_stage2 = precision_score(y_true_stage2, y_pred_stage2, zero_division=0)\n",
        "        recall_stage2 = recall_score(y_true_stage2, y_pred_stage2, zero_division=0)\n",
        "        f1_stage2 = f1_score(y_true_stage2, y_pred_stage2, zero_division=0)\n",
        "        tn2, fp2, fn2, tp2 = confusion_matrix(y_true_stage2, y_pred_stage2).ravel()\n",
        "        \n",
        "        # Stage 3 evaluation (4,5 vs 1,2,3)\n",
        "        y_true_stage3 = valid_results['stage_3_binary'].values\n",
        "        y_pred_stage3 = valid_results['stage3_llm_binary'].values\n",
        "        \n",
        "        accuracy_stage3 = accuracy_score(y_true_stage3, y_pred_stage3)\n",
        "        precision_stage3 = precision_score(y_true_stage3, y_pred_stage3, zero_division=0)\n",
        "        recall_stage3 = recall_score(y_true_stage3, y_pred_stage3, zero_division=0)\n",
        "        f1_stage3 = f1_score(y_true_stage3, y_pred_stage3, zero_division=0)\n",
        "        tn3, fp3, fn3, tp3 = confusion_matrix(y_true_stage3, y_pred_stage3).ravel()\n",
        "        \n",
        "    else:\n",
        "        # Handle case with no valid results\n",
        "        likert_counts = pd.Series(dtype=int)\n",
        "        stage2_true_scores = stage2_false_scores = pd.Series(dtype=int)\n",
        "        stage3_true_scores = stage3_false_scores = pd.Series(dtype=int)\n",
        "        accuracy_stage2 = precision_stage2 = recall_stage2 = f1_stage2 = 0.0\n",
        "        accuracy_stage3 = precision_stage3 = recall_stage3 = f1_stage3 = 0.0\n",
        "        tp2 = fp2 = tn2 = fn2 = tp3 = fp3 = tn3 = fn3 = 0\n",
        "    \n",
        "    # Updated metrics dictionary with Likert analysis\n",
        "    metrics = {\n",
        "        'stage_2_metrics': {\n",
        "            'accuracy': accuracy_stage2,\n",
        "            'precision': precision_stage2,\n",
        "            'recall': recall_stage2,\n",
        "            'f1_score': f1_stage2,\n",
        "            'tp': int(tp2),\n",
        "            'fp': int(fp2),\n",
        "            'tn': int(tn2),\n",
        "            'fn': int(fn2),\n",
        "            'threshold': '3+ (moderate to high relevance)'\n",
        "        },\n",
        "        'stage_3_metrics': {\n",
        "            'accuracy': accuracy_stage3,\n",
        "            'precision': precision_stage3,\n",
        "            'recall': recall_stage3,\n",
        "            'f1_score': f1_stage3,\n",
        "            'tp': int(tp3),\n",
        "            'fp': int(fp3),\n",
        "            'tn': int(tn3),\n",
        "            'fn': int(fn3),\n",
        "            'threshold': '4+ (high relevance)'\n",
        "        },\n",
        "        'likert_analysis': {\n",
        "            'overall_distribution': {f'score_{i}': int(likert_counts.get(i, 0)) for i in range(1, 6)},\n",
        "            'stage2_true_distribution': {f'score_{i}': int(stage2_true_scores.get(i, 0)) for i in range(1, 6)},\n",
        "            'stage2_false_distribution': {f'score_{i}': int(stage2_false_scores.get(i, 0)) for i in range(1, 6)},\n",
        "            'stage3_true_distribution': {f'score_{i}': int(stage3_true_scores.get(i, 0)) for i in range(1, 6)},\n",
        "            'stage3_false_distribution': {f'score_{i}': int(stage3_false_scores.get(i, 0)) for i in range(1, 6)}\n",
        "        },\n",
        "        'total_examples': len(results_df),\n",
        "        'successful_classifications': len(valid_results),\n",
        "        'errors': len(results_df) - len(valid_results)\n",
        "    }\n",
        "    \n",
        "    # Enhanced results printing\n",
        "    if verbose:\n",
        "        print(f\"\\nüìä EXPERIMENT RESULTS\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"üìä Likert Scale Distribution:\")\n",
        "        for i in range(1, 6):\n",
        "            count = likert_counts.get(i, 0)\n",
        "            print(f\"   Score {i}: {count}\")\n",
        "        \n",
        "        print(f\"\\nüìà Stage 2 Evaluation (3+ = positive):\")\n",
        "        print(f\"   Accuracy:  {accuracy_stage2:.3f}\")\n",
        "        print(f\"   Precision: {precision_stage2:.3f}\")\n",
        "        print(f\"   Recall:    {recall_stage2:.3f}\")\n",
        "        print(f\"   F1 Score:  {f1_stage2:.3f}\")\n",
        "        print(f\"   TP: {tp2}, FP: {fp2}, TN: {tn2}, FN: {fn2}\")\n",
        "        \n",
        "        print(f\"\\nüìà Stage 3 Evaluation (4+ = positive):\")\n",
        "        print(f\"   Accuracy:  {accuracy_stage3:.3f}\")\n",
        "        print(f\"   Precision: {precision_stage3:.3f}\")\n",
        "        print(f\"   Recall:    {recall_stage3:.3f}\")\n",
        "        print(f\"   F1 Score:  {f1_stage3:.3f}\")\n",
        "        print(f\"   TP: {tp3}, FP: {fp3}, TN: {tn3}, FN: {fn3}\")\n",
        "        \n",
        "        print(f\"\\nüìä Score Distribution by Ground Truth:\")\n",
        "        print(f\"   Stage 2 True:  [1:{stage2_true_scores.get(1,0)}, 2:{stage2_true_scores.get(2,0)}, 3:{stage2_true_scores.get(3,0)}, 4:{stage2_true_scores.get(4,0)}, 5:{stage2_true_scores.get(5,0)}]\")\n",
        "        print(f\"   Stage 2 False: [1:{stage2_false_scores.get(1,0)}, 2:{stage2_false_scores.get(2,0)}, 3:{stage2_false_scores.get(3,0)}, 4:{stage2_false_scores.get(4,0)}, 5:{stage2_false_scores.get(5,0)}]\")\n",
        "        print(f\"   Stage 3 True:  [1:{stage3_true_scores.get(1,0)}, 2:{stage3_true_scores.get(2,0)}, 3:{stage3_true_scores.get(3,0)}, 4:{stage3_true_scores.get(4,0)}, 5:{stage3_true_scores.get(5,0)}]\")\n",
        "        print(f\"   Stage 3 False: [1:{stage3_false_scores.get(1,0)}, 2:{stage3_false_scores.get(2,0)}, 3:{stage3_false_scores.get(3,0)}, 4:{stage3_false_scores.get(4,0)}, 5:{stage3_false_scores.get(5,0)}]\")\n",
        "        \n",
        "        print(f\"\\nüìã Processing Summary:\")\n",
        "        print(f\"   Total examples: {len(results_df)}\")\n",
        "        print(f\"   Successful: {len(valid_results)}\")\n",
        "        print(f\"   Errors: {len(results_df) - len(valid_results)}\")\n",
        "    \n",
        "    # Save results\n",
        "    if save_results:\n",
        "        # Create filename with timestamp\n",
        "        timestamp = datetime.now().strftime(\"%m%d%H%M\")\n",
        "        filename = f\"{EXPERIMENT_ID}_{dataset_source}_{timestamp}.csv\"\n",
        "        results_dir = \"../results\"\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "        output_path = os.path.join(results_dir, filename)\n",
        "        \n",
        "        results_df.to_csv(output_path, index=False)\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"\\nüíæ Results saved to: {output_path}\")\n",
        "    \n",
        "    return {\n",
        "        'results_df': results_df,\n",
        "        'metrics': metrics,\n",
        "        'filename': filename if save_results else None\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Classification experiment function with Likert scale analysis defined\")\n",
        "print(\"üöÄ Ready to run: run_classification_experiment(df_LB, batch_size=20)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Run experiment! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run experiment with default settings\n",
        "results = run_classification_experiment(df_LB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Results Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load results file - you can specify the exact file path here\n",
        "RESULTS_FILE_PATH = \"../results/0002_LB_08131412.csv\"  # ‚¨ÖÔ∏è Change this to your specific file path\n",
        "\n",
        "# Alternative: Set to None to auto-load the most recent file\n",
        "# RESULTS_FILE_PATH = None\n",
        "\n",
        "if RESULTS_FILE_PATH:\n",
        "    # Load specific file\n",
        "    if os.path.exists(RESULTS_FILE_PATH):\n",
        "        print(f\"üìÅ Loading specified file: {os.path.basename(RESULTS_FILE_PATH)}\")\n",
        "        df_results = pd.read_csv(RESULTS_FILE_PATH)\n",
        "    else:\n",
        "        print(f\"‚ùå Error: File not found: {RESULTS_FILE_PATH}\")\n",
        "        df_results = None\n",
        "else:\n",
        "    # Auto-load most recent file (original behavior)\n",
        "    results_dir = \"../results\"\n",
        "    result_files = [f for f in os.listdir(results_dir) if f.endswith('.csv')]\n",
        "    if result_files:\n",
        "        latest_file = sorted(result_files)[-1]\n",
        "        file_path = os.path.join(results_dir, latest_file)\n",
        "        print(f\"üìÅ Auto-loading most recent file: {latest_file}\")\n",
        "        df_results = pd.read_csv(file_path)\n",
        "    else:\n",
        "        print(\"‚ùå No result files found in ../results directory\")\n",
        "        df_results = None\n",
        "\n",
        "# Continue with analysis if file was loaded successfully\n",
        "if df_results is not None:\n",
        "    print(f\"\\nüìä RESULTS OVERVIEW\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Shape: {df_results.shape}\")\n",
        "    print(f\"Columns: {list(df_results.columns)}\")\n",
        "    \n",
        "    print(f\"\\nüéØ DECISION SUMMARY\")\n",
        "    print(\"=\" * 30)\n",
        "    print(df_results['llm_decision'].value_counts())\n",
        "    \n",
        "    print(f\"\\nüìà PERFORMANCE PREVIEW\")\n",
        "    print(\"=\" * 30)\n",
        "    print(\"Stage 2 vs LLM:\")\n",
        "    print(pd.crosstab(df_results['stage_2_true'], df_results['llm_decision']))\n",
        "    print(\"\\nStage 3 vs LLM:\")\n",
        "    print(pd.crosstab(df_results['stage_3_true'], df_results['llm_decision']))\n",
        "    \n",
        "    print(f\"\\nüìã FIRST FEW RESULTS\")\n",
        "    print(\"=\" * 30)\n",
        "    display(df_results[['example_id', 'stage_2_true', 'stage_3_true', 'llm_decision', 'llm_reasoning']].head())\n",
        "else:\n",
        "    print(\"‚ùå Could not load results file for analysis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display full reasoning for first 5 examples\n",
        "print(\"ü§ñ FULL LLM REASONING EXAMPLES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for idx in range(min(5, len(df_results))):\n",
        "    row = df_results.iloc[idx]\n",
        "    print(f\"\\nüìã EXAMPLE {row['example_id']} - {row['llm_decision']}\")\n",
        "    print(f\"üéØ Ground Truth: Stage 2={row['stage_2_true']}, Stage 3={row['stage_3_true']}\")\n",
        "    print(f\"üìñ Title: {row['title'][:100]}{'...' if len(row['title']) > 100 else ''}\")\n",
        "    print(f\"\\nüí≠ FULL REASONING:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(row['llm_reasoning'])\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ûï Add experiment info to the results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_experiment_to_summary_safe(results_dict, summary_file=\"../results/experiment_summary.csv\"):\n",
        "    \"\"\"Safely add experiment results to summary - never overwrites, only appends\"\"\"\n",
        "    \n",
        "    from datetime import datetime\n",
        "    \n",
        "    # Load existing summary (should exist now)\n",
        "    if not os.path.exists(summary_file):\n",
        "        print(f\"‚ùå Summary file not found: {summary_file}\")\n",
        "        print(\"Please run create_empty_experiment_summary() first!\")\n",
        "        return None\n",
        "    \n",
        "    existing_summary = pd.read_csv(summary_file)\n",
        "    print(f\"üìä Current summary has {len(existing_summary)} experiments\")\n",
        "    \n",
        "    # Create new row with only the data we have\n",
        "    new_row_data = {\n",
        "        # Basic experiment info (always available)\n",
        "        'experiment_id': EXPERIMENT_ID,\n",
        "        'experiment_date': EXPERIMENT_DATE,\n",
        "        'experiment_category': EXPERIMENT_CATEGORY,\n",
        "        'experiment_goal': EXPERIMENT_GOAL,\n",
        "        'system_prompt_id': SYSTEM_PROMPT_ID,\n",
        "        'user_prompt_id': USER_PROMPT_ID,\n",
        "        'model_name': MODEL_NAME,\n",
        "        'temperature': TEMPERATURE,\n",
        "        'max_tokens': MAX_TOKENS,\n",
        "        'criteria_file': CRITERIA_FILE,\n",
        "        'examples_file': EXAMPLES_FILE,\n",
        "        'output_format': OUTPUT_FORMAT,\n",
        "        'domain': DOMAIN,\n",
        "        'topic': TOPIC,\n",
        "        'dataset_source': DATASET_SOURCE,\n",
        "        'n_total_examples': results_dict['metrics']['total_examples'],\n",
        "        'n_successful': results_dict['metrics']['successful_classifications'],\n",
        "        'n_errors': results_dict['metrics']['errors'],\n",
        "        'results_filename': results_dict['filename'],\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    # Add Stage 2 metrics (always available)\n",
        "    stage2_metrics = results_dict['metrics']['stage_2_metrics']\n",
        "    new_row_data.update({\n",
        "        'stage2_accuracy': stage2_metrics['accuracy'],\n",
        "        'stage2_precision': stage2_metrics['precision'],\n",
        "        'stage2_recall': stage2_metrics['recall'],\n",
        "        'stage2_f1': stage2_metrics['f1_score'],\n",
        "        'stage2_tp': stage2_metrics['tp'],\n",
        "        'stage2_fp': stage2_metrics['fp'],\n",
        "        'stage2_tn': stage2_metrics['tn'],\n",
        "        'stage2_fn': stage2_metrics['fn']\n",
        "    })\n",
        "    \n",
        "    # Add Stage 3 metrics if available\n",
        "    if 'stage_3_metrics' in results_dict['metrics']:\n",
        "        stage3_metrics = results_dict['metrics']['stage_3_metrics']\n",
        "        new_row_data.update({\n",
        "            'stage3_accuracy': stage3_metrics['accuracy'],\n",
        "            'stage3_precision': stage3_metrics['precision'],\n",
        "            'stage3_recall': stage3_metrics['recall'],\n",
        "            'stage3_f1': stage3_metrics['f1_score'],\n",
        "            'stage3_tp': stage3_metrics['tp'],\n",
        "            'stage3_fp': stage3_metrics['fp'],\n",
        "            'stage3_tn': stage3_metrics['tn'],\n",
        "            'stage3_fn': stage3_metrics['fn']\n",
        "        })\n",
        "        \n",
        "        # Add thresholds if available\n",
        "        if 'threshold' in stage3_metrics:\n",
        "            new_row_data['stage3_threshold'] = stage3_metrics['threshold']\n",
        "    \n",
        "    # Add Stage 2 threshold if available\n",
        "    if 'threshold' in stage2_metrics:\n",
        "        new_row_data['stage2_threshold'] = stage2_metrics['threshold']\n",
        "    \n",
        "    # Add decision counts if available (for MAYBE experiments)\n",
        "    if 'decision_counts' in results_dict['metrics']:\n",
        "        decision_counts = results_dict['metrics']['decision_counts']\n",
        "        new_row_data.update({\n",
        "            'llm_include_count': decision_counts.get('INCLUDE', np.nan),\n",
        "            'llm_maybe_count': decision_counts.get('MAYBE', np.nan),\n",
        "            'llm_exclude_count': decision_counts.get('EXCLUDE', np.nan),\n",
        "            'llm_error_count': decision_counts.get('ERROR', np.nan)\n",
        "        })\n",
        "    \n",
        "    # Add Likert analysis if available\n",
        "    if 'likert_analysis' in results_dict['metrics']:\n",
        "        likert_analysis = results_dict['metrics']['likert_analysis']\n",
        "        \n",
        "        # Overall distribution\n",
        "        if 'overall_distribution' in likert_analysis:\n",
        "            overall_dist = likert_analysis['overall_distribution']\n",
        "            for i in range(1, 6):\n",
        "                new_row_data[f'likert_score_{i}'] = overall_dist.get(f'score_{i}', np.nan)\n",
        "        \n",
        "        # Stage 2 distributions\n",
        "        if 'stage2_true_distribution' in likert_analysis:\n",
        "            stage2_true_dist = likert_analysis['stage2_true_distribution']\n",
        "            for i in range(1, 6):\n",
        "                new_row_data[f'stage2_true_score_{i}'] = stage2_true_dist.get(f'score_{i}', np.nan)\n",
        "        \n",
        "        if 'stage2_false_distribution' in likert_analysis:\n",
        "            stage2_false_dist = likert_analysis['stage2_false_distribution']\n",
        "            for i in range(1, 6):\n",
        "                new_row_data[f'stage2_false_score_{i}'] = stage2_false_dist.get(f'score_{i}', np.nan)\n",
        "        \n",
        "        # Stage 3 distributions\n",
        "        if 'stage3_true_distribution' in likert_analysis:\n",
        "            stage3_true_dist = likert_analysis['stage3_true_distribution']\n",
        "            for i in range(1, 6):\n",
        "                new_row_data[f'stage3_true_score_{i}'] = stage3_true_dist.get(f'score_{i}', np.nan)\n",
        "        \n",
        "        if 'stage3_false_distribution' in likert_analysis:\n",
        "            stage3_false_dist = likert_analysis['stage3_false_distribution']\n",
        "            for i in range(1, 6):\n",
        "                new_row_data[f'stage3_false_score_{i}'] = stage3_false_dist.get(f'score_{i}', np.nan)\n",
        "    \n",
        "    # Create new row DataFrame with all columns from existing summary\n",
        "    new_row = pd.DataFrame([new_row_data])\n",
        "    \n",
        "    # Reindex to match existing summary columns (fills missing with NaN automatically)\n",
        "    new_row = new_row.reindex(columns=existing_summary.columns)\n",
        "    \n",
        "    # Append to existing summary (never overwrites)\n",
        "    updated_summary = pd.concat([existing_summary, new_row], ignore_index=True)\n",
        "    \n",
        "    # Save updated summary\n",
        "    updated_summary.to_csv(summary_file, index=False)\n",
        "    \n",
        "    print(f\"‚úÖ Added experiment {EXPERIMENT_ID} to existing summary\")\n",
        "    print(f\"üíæ Summary saved with {len(updated_summary)} total experiments\")\n",
        "    \n",
        "    # Show the last 5 rows\n",
        "    print(f\"\\nüìã LAST 5 EXPERIMENTS:\")\n",
        "    print(\"=\" * 100)\n",
        "    display(updated_summary.tail())\n",
        "    \n",
        "    print(f\"\\nüìä SUMMARY STATS:\")\n",
        "    print(f\"   Total experiments: {len(updated_summary)}\")\n",
        "    print(f\"   Unique experiment IDs: {updated_summary['experiment_id'].nunique()}\")\n",
        "    print(f\"   Datasets used: {updated_summary['dataset_source'].unique().tolist()}\")\n",
        "    \n",
        "    return updated_summary\n",
        "\n",
        "# Usage: \n",
        "# summary_df = add_experiment_to_summary_safe(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Conclusions and Next Steps\n",
        "\n",
        "### Key Findings\n",
        "- \n",
        "\n",
        "### Next Steps\n",
        "- [Suggest follow-up experiments]\n",
        "- [List potential improvements]\n",
        "- [Identify areas for further investigation]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "SLRenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
