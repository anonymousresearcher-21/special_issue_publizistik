{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß™ SLR Abstract Screening Experiment\n",
        "#### Experiment Information\n",
        "- **ID**: 005\n",
        "- **Date**: 08/13\n",
        "#### üéØ Goal\n",
        "- Test all configurations on a smaller portion of the dataset to identify problems and patterns\n",
        "#### ‚öôÔ∏è Configuration\n",
        "- **LLM** : GPT-4o\n",
        "- **Data**: LB\n",
        "- **Examples** : 1\n",
        "- **Output**: Yes/Maybe/No\n",
        "#### üìù Notes\n",
        "- \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîß Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Essential imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import json\n",
        "from pathlib import Path\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "# Configure pandas display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_theme()  # This is the correct way to set seaborn style\n",
        "plt.rcParams['figure.figsize'] = (12, 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì First dataset loaded successfully\n",
            "‚úì Shape of dataset 1: (3944, 15)\n",
            "\n",
            "‚úì Second dataset loaded successfully\n",
            "‚úì Shape of dataset 2: (917, 13)\n",
            "\n",
            "First few rows of dataset 1:\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>abstract</th>\n",
              "      <th>acmid</th>\n",
              "      <th>author</th>\n",
              "      <th>doi</th>\n",
              "      <th>outlet</th>\n",
              "      <th>title_full</th>\n",
              "      <th>url</th>\n",
              "      <th>year</th>\n",
              "      <th>qualtrics_id</th>\n",
              "      <th>wos_id</th>\n",
              "      <th>ebsco_id</th>\n",
              "      <th>stage_1</th>\n",
              "      <th>stage_2</th>\n",
              "      <th>stage_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bindu2018503</td>\n",
              "      <td>Online social networks have become immensely p...</td>\n",
              "      <td></td>\n",
              "      <td>Bindu, P V and Mishra, R and Thilagam, P S</td>\n",
              "      <td>10.1007/s10844-017-0494-z</td>\n",
              "      <td>Journal of Intelligent Information Systems</td>\n",
              "      <td>{Discovering spammer communities in TWITTER}</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>2018</td>\n",
              "      <td>12</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Moraga2018470</td>\n",
              "      <td>This article explores the ways Latinos‚Äîas audi...</td>\n",
              "      <td></td>\n",
              "      <td>Moraga, J E</td>\n",
              "      <td>10.1177/0193723518797030</td>\n",
              "      <td>Journal of Sport and Social Issues</td>\n",
              "      <td>{On ESPN Deportes: Latinos, Sport MEDIA, and t...</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>2018</td>\n",
              "      <td>22</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Lanosga20181676</td>\n",
              "      <td>This study of American investigative reporting...</td>\n",
              "      <td></td>\n",
              "      <td>Lanosga, G and Martin, J</td>\n",
              "      <td>10.1177/1464884916683555</td>\n",
              "      <td>JOURNALISm</td>\n",
              "      <td>{JOURNALISts, sources, and policy outcomes: In...</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>2018</td>\n",
              "      <td>47</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Warner2018720</td>\n",
              "      <td>In this study, we test the indirect and condit...</td>\n",
              "      <td></td>\n",
              "      <td>Warner, B R and Jennings, F J and Bramlett, J ...</td>\n",
              "      <td>10.1080/15205436.2018.1472283</td>\n",
              "      <td>Mass Communication and Society</td>\n",
              "      <td>{A MultiMEDIA Analysis of Persuasion in the 20...</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>2018</td>\n",
              "      <td>50</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Burrows20181117</td>\n",
              "      <td>Professional communicators produce a diverse r...</td>\n",
              "      <td></td>\n",
              "      <td>Burrows, E</td>\n",
              "      <td>10.1177/0163443718764807</td>\n",
              "      <td>MEDIA, Culture and Society</td>\n",
              "      <td>{Indigenous MEDIA producers' perspectives on o...</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>2018</td>\n",
              "      <td>56</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                ID                                           abstract acmid                                             author                            doi                                      outlet                                         title_full                                                url  year qualtrics_id wos_id ebsco_id  stage_1  stage_2  stage_3\n",
              "0     Bindu2018503  Online social networks have become immensely p...               Bindu, P V and Mishra, R and Thilagam, P S      10.1007/s10844-017-0494-z  Journal of Intelligent Information Systems       {Discovering spammer communities in TWITTER}  https://www.scopus.com/inward/record.uri?eid=2...  2018           12             NaN     True    False    False\n",
              "1    Moraga2018470  This article explores the ways Latinos‚Äîas audi...                                              Moraga, J E       10.1177/0193723518797030          Journal of Sport and Social Issues  {On ESPN Deportes: Latinos, Sport MEDIA, and t...  https://www.scopus.com/inward/record.uri?eid=2...  2018           22             NaN     True    False    False\n",
              "2  Lanosga20181676  This study of American investigative reporting...                                 Lanosga, G and Martin, J       10.1177/1464884916683555                                  JOURNALISm  {JOURNALISts, sources, and policy outcomes: In...  https://www.scopus.com/inward/record.uri?eid=2...  2018           47             NaN     True    False     True\n",
              "3    Warner2018720  In this study, we test the indirect and condit...        Warner, B R and Jennings, F J and Bramlett, J ...  10.1080/15205436.2018.1472283              Mass Communication and Society  {A MultiMEDIA Analysis of Persuasion in the 20...  https://www.scopus.com/inward/record.uri?eid=2...  2018           50             NaN     True    False    False\n",
              "4  Burrows20181117  Professional communicators produce a diverse r...                                               Burrows, E       10.1177/0163443718764807                  MEDIA, Culture and Society  {Indigenous MEDIA producers' perspectives on o...  https://www.scopus.com/inward/record.uri?eid=2...  2018           56             NaN     True    False    False"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "First few rows of dataset 2:\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>(internal) id</th>\n",
              "      <th>(source) id</th>\n",
              "      <th>abstract</th>\n",
              "      <th>title_full</th>\n",
              "      <th>journal</th>\n",
              "      <th>authors</th>\n",
              "      <th>tags</th>\n",
              "      <th>consensus</th>\n",
              "      <th>labeled_at...9</th>\n",
              "      <th>code</th>\n",
              "      <th>stage_1</th>\n",
              "      <th>stage_2</th>\n",
              "      <th>stage_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>33937314</td>\n",
              "      <td>175</td>\n",
              "      <td>There is a worry that serious forms of politic...</td>\n",
              "      <td>Is Context the Key? The (Non-)Differential Eff...</td>\n",
              "      <td>Polit. Commun.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>o</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>33937315</td>\n",
              "      <td>113</td>\n",
              "      <td>The electoral model of democracy holds the ide...</td>\n",
              "      <td>POLITICAL NEWS IN ONLINE AND PRINT NEWSPAPERS ...</td>\n",
              "      <td>Digit. Journal.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>o</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33937316</td>\n",
              "      <td>122</td>\n",
              "      <td>Machine learning is a field at the intersectio...</td>\n",
              "      <td>Machine Learning for Sociology</td>\n",
              "      <td>Annu. Rev. Sociol.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>o</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33937317</td>\n",
              "      <td>467</td>\n",
              "      <td>Research on digital glocalization has found th...</td>\n",
              "      <td>Improving Health in Low-Income Communities Wit...</td>\n",
              "      <td>J. Commun.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>o</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33937318</td>\n",
              "      <td>10</td>\n",
              "      <td>Political scientists often wish to classify do...</td>\n",
              "      <td>Using Word Order in Political Text Classificat...</td>\n",
              "      <td>Polit. Anal.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>o</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   (internal) id  (source) id                                           abstract                                         title_full             journal authors  tags consensus  labeled_at...9  code  stage_1  stage_2  stage_3\n",
              "0       33937314          175  There is a worry that serious forms of politic...  Is Context the Key? The (Non-)Differential Eff...      Polit. Commun.     NaN   NaN         o             NaN    -1     True    False    False\n",
              "1       33937315          113  The electoral model of democracy holds the ide...  POLITICAL NEWS IN ONLINE AND PRINT NEWSPAPERS ...     Digit. Journal.     NaN   NaN         o             NaN    -1     True    False    False\n",
              "2       33937316          122  Machine learning is a field at the intersectio...                     Machine Learning for Sociology  Annu. Rev. Sociol.     NaN   NaN         o             NaN    -1     True    False    False\n",
              "3       33937317          467  Research on digital glocalization has found th...  Improving Health in Low-Income Communities Wit...          J. Commun.     NaN   NaN         o             NaN    -1     True    False    False\n",
              "4       33937318           10  Political scientists often wish to classify do...  Using Word Order in Political Text Classificat...        Polit. Anal.     NaN   NaN         o             NaN    -1     True    False    False"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Data Import \n",
        "\n",
        "# Define the data paths for both datasets\n",
        "DATA_PATH_1 = \"../data/SSOT_manual_LB_20250808_120908.csv\" # ‚¨ÖÔ∏è Change this path if needed\n",
        "DATA_PATH_2 = \"../data/SSOT_manual_BM_20250813_132621.csv\" # ‚¨ÖÔ∏è Change this path if needed\n",
        "\n",
        "# Load the first dataset (df1)\n",
        "try:\n",
        "    df_LB = pd.read_csv(DATA_PATH_1)\n",
        "    print(f\"‚úì First dataset loaded successfully\")\n",
        "    print(f\"‚úì Shape of dataset 1: {df_LB.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: The file LB dataset was not found in the data directory\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading the first dataset: {str(e)}\")\n",
        "\n",
        "# Load the second dataset (df2)\n",
        "try:\n",
        "    df_BM = pd.read_csv(DATA_PATH_2)\n",
        "    print(f\"\\n‚úì Second dataset loaded successfully\")\n",
        "    print(f\"‚úì Shape of dataset 2: {df_BM.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: The file df_BM was not found in the data directory\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading the second dataset: {str(e)}\")\n",
        "\n",
        "# Display basic information about both datasets\n",
        "print(\"\\nFirst few rows of dataset 1:\\n\")\n",
        "display(df_LB.head())\n",
        "\n",
        "print(\"\\nFirst few rows of dataset 2:\\n\")\n",
        "display(df_BM.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß´ Define Experiment Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ EXPERIMENT SETUP\n",
            "==================================================\n",
            "ID: 005\n",
            "Date: 2025-08-13\n",
            "Category: Testing\n",
            "üéØGoal: Test Set Up\n",
            "Model: gpt-4o (temp=0.0)\n",
            "==================================================\n",
            "‚úÖ Experiment configuration loaded\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Experiment Metadata\n",
        "EXPERIMENT_ID = \"005\"  # ‚¨ÖÔ∏è Change this for each new experiment\n",
        "EXPERIMENT_DATE = \"2025-08-13\"  # ‚¨ÖÔ∏è Update the date\n",
        "EXPERIMENT_CATEGORY = \"Testing\"  # ‚¨ÖÔ∏è Category of experiment\n",
        "EXPERIMENT_GOAL = \"Test Set Up\"  # ‚¨ÖÔ∏è What are you testing?\n",
        "\n",
        "# Model Configuration\n",
        "MODEL_NAME = \"gpt-4o\"\n",
        "TEMPERATURE = 0.0\n",
        "MAX_TOKENS = 4000\n",
        "\n",
        "# Print experiment info\n",
        "print(\"üß™ EXPERIMENT SETUP\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"ID: {EXPERIMENT_ID}\")\n",
        "print(f\"Date: {EXPERIMENT_DATE}\")\n",
        "print(f\"Category: {EXPERIMENT_CATEGORY}\")\n",
        "print(f\"üéØGoal: {EXPERIMENT_GOAL}\")\n",
        "print(f\"Model: {MODEL_NAME} (temp={TEMPERATURE})\")\n",
        "print(\"=\" * 50)\n",
        "print(\"‚úÖ Experiment configuration loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì£ Set up Basic API Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ OpenAI API Key loaded successfully.\n",
            "‚úÖ OpenAI client initialized.\n",
            "‚úÖ Enhanced screening function defined.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from datetime import datetime\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Get the API key from environment variables\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Validate API key\n",
        "if not api_key:\n",
        "    print(\"‚ö†Ô∏è  Error: OPENAI_API_KEY not found.\")\n",
        "    print(\"Please make sure you have a .env file with OPENAI_API_KEY='sk-...'\")\n",
        "else:\n",
        "    print(\"‚úÖ OpenAI API Key loaded successfully.\")\n",
        "    client = OpenAI(api_key=api_key)\n",
        "    print(\"‚úÖ OpenAI client initialized.\")\n",
        "\n",
        "# Enhanced analysis function for abstract screening\n",
        "def screen_abstract_llm(abstract_text, system_prompt, user_prompt_template, \n",
        "                       model=\"gpt-4o\", temperature=0.0):\n",
        "    \"\"\"\n",
        "    Screen an abstract using LLM with system and user prompts.\n",
        "    \n",
        "    Args:\n",
        "        abstract_text (str): The abstract to analyze\n",
        "        system_prompt (str): The system prompt defining the role\n",
        "        user_prompt_template (str): Template with {abstract} placeholder\n",
        "        model (str): The OpenAI model to use\n",
        "        temperature (float): Temperature setting for response randomness\n",
        "    \n",
        "    Returns:\n",
        "        dict: Result with decision, reasoning, and metadata\n",
        "    \"\"\"\n",
        "    if 'client' not in globals():\n",
        "        return {\"error\": \"OpenAI client is not initialized. Please check your API key.\"}\n",
        "\n",
        "    try:\n",
        "        # Insert abstract into user prompt template\n",
        "        user_prompt = user_prompt_template.format(abstract=abstract_text)\n",
        "        \n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "            max_tokens=4000\n",
        "        )\n",
        "        \n",
        "        if response and response.choices:\n",
        "            result = {\n",
        "                \"decision\": \"INCLUDE\" if \"INCLUDE\" in response.choices[0].message.content.upper() else \"EXCLUDE\",\n",
        "                \"reasoning\": response.choices[0].message.content,\n",
        "                \"model\": model,\n",
        "                \"temperature\": temperature,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"error\": None\n",
        "            }\n",
        "            return result\n",
        "        else:\n",
        "            return {\"error\": \"API Error: Empty or invalid response.\"}\n",
        "            \n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"API Error: {e}\"}\n",
        "\n",
        "print(\"‚úÖ Enhanced screening function defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèõÔ∏è Set Up System Prompt "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ System prompt defined\n",
            "üìã ID: SYS_001\n",
            "üìè Length: 759 characters\n",
            "üìÑ Description: Generic expert literature review screener for systematic reviews\n"
          ]
        }
      ],
      "source": [
        "# System prompt configuration\n",
        "# System prompt configuration\n",
        "SYSTEM_PROMPT_ID = \"SYS_001\"  # ‚¨ÖÔ∏è Change this ID for different system prompts\n",
        "SYSTEM_PROMPT_DESCRIPTION = \"Generic expert literature review screener for systematic reviews\"\n",
        "\n",
        "# Define the system prompt that sets the LLM's role\n",
        "SYSTEM_PROMPT = \"\"\"You are an expert in scientific literature review and systematic review methodology.\n",
        "\n",
        "Your task is to screen research abstracts and decide whether they should be INCLUDED or EXCLUDED from a systematic literature review based on provided criteria.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Carefully read the provided inclusion/exclusion criteria\n",
        "2. Review any example abstracts to understand the decision-making pattern\n",
        "3. Apply the criteria systematically to the given abstract and title\n",
        "4. Provide your decision in the exact format requested\n",
        "5. Base your reasoning strictly on the provided criteria\n",
        "\n",
        "Be consistent, objective, and systematic in your evaluation. Do not make up additional criteria beyond what is provided. Focus only on what is explicitly stated in the instructions.\"\"\"\n",
        "\n",
        "print(f\"‚úÖ System prompt defined\")\n",
        "print(f\"üìã ID: {SYSTEM_PROMPT_ID}\")\n",
        "print(f\"üìè Length: {len(SYSTEM_PROMPT)} characters\")\n",
        "print(f\"üìÑ Description: {SYSTEM_PROMPT_DESCRIPTION}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üë©üèª‚Äç‚öïÔ∏è Create User Prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ User prompt configuration and template loaded\n",
            "üìã ID: USR_004\n",
            "üìÑ Description: Basic screening with criteria and example from CSV files and maybe option\n",
            "üìÅ Criteria: ../prompts/Criteria_LB_01.csv\n",
            "üìÅ Examples: ../prompts/exmpl_single_LB_01.csv\n",
            "üéØ Output: Yes/Maybe/No\n",
            "üî¨ Topic: media_diversity | Domain: political_communication | Source: LB\n",
            "üìè Template length: 1679 characters\n"
          ]
        }
      ],
      "source": [
        "# User prompt configuration\n",
        "USER_PROMPT_ID = \"USR_004\"  # ‚¨ÖÔ∏è Change this ID for different user prompts\n",
        "USER_PROMPT_DESCRIPTION = \"Basic screening with criteria and example from CSV files and maybe option\"\n",
        "\n",
        "# File paths for modular components\n",
        "CRITERIA_FILE = \"../prompts/Criteria_LB_01.csv\"  # ‚¨ÖÔ∏è Change criteria file here\n",
        "EXAMPLES_FILE = \"../prompts/exmpl_single_LB_01.csv\"  # ‚¨ÖÔ∏è Change examples file here (or set to None)\n",
        "\n",
        "# Output configuration\n",
        "OUTPUT_FORMAT = \"Yes/Maybe/No\"  # ‚¨ÖÔ∏è Options: \"Binary\", \"Yes/Maybe/No\", \"Likert\"\n",
        "DECISION_OPTIONS = [\"INCLUDE\", \"EXCLUDE\", \"MAYBE\"] # ‚¨ÖÔ∏è Change according to the output format\n",
        "\n",
        "# Additional metadata for results tracking\n",
        "DOMAIN = \"political_communication\" # ‚¨ÖÔ∏è Change this to the domain of the study\n",
        "TOPIC = \"media_diversity\"  # ‚¨ÖÔ∏è Change this to the topic of the study\n",
        "DATASET_SOURCE = \"LB\"  # ‚¨ÖÔ∏è Which dataset (BM/LB)\n",
        "\n",
        "# Define the user prompt template with placeholders\n",
        "USER_PROMPT_TEMPLATE = \"\"\"## SCREENING TASK:\n",
        "You are conducting the first screening stage for a systematic literature review on {topic} in {domain}. Your task is to evaluate whether each abstract should be included, excluded, or requires further review based on the full text.\n",
        "\n",
        "## INCLUSION/EXCLUSION CRITERIA:\n",
        "{criteria_text}\n",
        "\n",
        "{examples_section}\n",
        "\n",
        "## DECISION GUIDELINES:\n",
        "\n",
        "**INCLUDE**: Choose this when the abstract clearly meets the inclusion criteria and does NOT meet any exclusion criteria. The abstract provides sufficient information to confidently determine relevance.\n",
        "\n",
        "**EXCLUDE**: Choose this when the abstract clearly violates one or more exclusion criteria OR clearly fails to meet the inclusion criteria. The abstract provides sufficient information to confidently determine irrelevance.\n",
        "\n",
        "**MAYBE**: Choose this when the abstract is potentially relevant but lacks sufficient detail to make a confident decision. Use this option when:\n",
        "- The abstract mentions relevant concepts but lacks specific details about methodology, context, or scope\n",
        "- Key information needed to apply the criteria is missing or ambiguous\n",
        "- The study appears to be in the right domain but the connection to your research question is unclear\n",
        "- You would need to see the full text to properly evaluate against the criteria\n",
        "\n",
        "## ABSTRACT TO SCREEN:\n",
        "**Title:** {title}\n",
        "**Abstract:** {abstract}\n",
        "\n",
        "## YOUR DECISION:\n",
        "Provide your decision as one of: **INCLUDE**, **MAYBE**, or **EXCLUDE**\n",
        "\n",
        "**Decision:** [Choose exactly one: INCLUDE, MAYBE, or EXCLUDE]\n",
        "\n",
        "**Reasoning:** [Explain your decision. For MAYBE decisions, specifically describe what additional information from the full text would be needed to make a final determination.]\"\"\"\n",
        "\n",
        "print(f\"‚úÖ User prompt configuration and template loaded\")\n",
        "print(f\"üìã ID: {USER_PROMPT_ID}\")\n",
        "print(f\"üìÑ Description: {USER_PROMPT_DESCRIPTION}\")\n",
        "print(f\"üìÅ Criteria: {CRITERIA_FILE}\")\n",
        "print(f\"üìÅ Examples: {EXAMPLES_FILE}\")\n",
        "print(f\"üéØ Output: {OUTPUT_FORMAT}\")\n",
        "print(f\"üî¨ Topic: {TOPIC} | Domain: {DOMAIN} | Source: {DATASET_SOURCE}\")\n",
        "print(f\"üìè Template length: {len(USER_PROMPT_TEMPLATE)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Valdiation Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç VALIDATION CHECK\n",
            "==================================================\n",
            "üìã Checking required variables:\n",
            "   ‚úÖ EXPERIMENT_ID: 004\n",
            "   ‚úÖ SYSTEM_PROMPT_ID: SYS_001\n",
            "   ‚úÖ USER_PROMPT_ID: USR_004\n",
            "   ‚úÖ SYSTEM_PROMPT: You are an expert in scientific literature review ...\n",
            "   ‚úÖ USER_PROMPT_TEMPLATE: ## SCREENING TASK:\n",
            "You are conducting the first sc...\n",
            "   ‚úÖ CRITERIA_FILE: ../prompts/Criteria_LB_01.csv\n",
            "   ‚úÖ DECISION_OPTIONS: ['INCLUDE', 'EXCLUDE', 'MAYBE']\n",
            "   ‚úÖ MODEL_NAME: gpt-4o\n",
            "   ‚úÖ TEMPERATURE: 0.0\n",
            "   ‚úÖ TOPIC: media_diversity\n",
            "   ‚úÖ DOMAIN: political_communication\n",
            "üìã Checking optional variables:\n",
            "   ‚úÖ EXAMPLES_FILE: ../prompts/exmpl_single_LB_01.csv\n",
            "\n",
            "üìä Checking DataFrame structure:\n",
            "   ‚úÖ DataFrame shape: (3944, 15)\n",
            "   ‚úÖ Column 'abstract': Present\n",
            "   ‚úÖ Column 'title_full': Present\n",
            "   ‚úÖ Column 'stage_2': Present\n",
            "   ‚úÖ Column 'stage_3': Present\n",
            "\n",
            "üìà Checking data availability:\n",
            "   üìä Stage 2 True: 277\n",
            "   üìä Stage 2 False: 3667\n",
            "   üìä Stage 3 True: 207\n",
            "   üìä Stage 3 False: 3737\n",
            "\n",
            "üìÅ Checking file paths:\n",
            "   ‚úÖ Criteria file: ../prompts/Criteria_LB_01.csv\n",
            "   ‚úÖ Examples file: ../prompts/exmpl_single_LB_01.csv\n",
            "\n",
            "ü§ñ Checking API function:\n",
            "   ‚úÖ screen_abstract_llm function: Available\n",
            "\n",
            "==================================================\n",
            "‚úÖ ALL VALIDATIONS PASSED - Ready to run experiment!\n"
          ]
        }
      ],
      "source": [
        "def validate_experiment_setup(df, dataset_source=\"LB\"):\n",
        "    \"\"\"\n",
        "    Validate that all required variables and data are available for the experiment.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame to be used in experiment\n",
        "        dataset_source: Dataset identifier\n",
        "    \n",
        "    Returns:\n",
        "        bool: True if all validations pass, False otherwise\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"üîç VALIDATION CHECK\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    validation_passed = True\n",
        "    \n",
        "    # Check required global variables\n",
        "    required_vars = {\n",
        "        'EXPERIMENT_ID': globals().get('EXPERIMENT_ID'),\n",
        "        'SYSTEM_PROMPT_ID': globals().get('SYSTEM_PROMPT_ID'), \n",
        "        'USER_PROMPT_ID': globals().get('USER_PROMPT_ID'),\n",
        "        'SYSTEM_PROMPT': globals().get('SYSTEM_PROMPT'),\n",
        "        'USER_PROMPT_TEMPLATE': globals().get('USER_PROMPT_TEMPLATE'),\n",
        "        'CRITERIA_FILE': globals().get('CRITERIA_FILE'),\n",
        "        'DECISION_OPTIONS': globals().get('DECISION_OPTIONS'),\n",
        "        'MODEL_NAME': globals().get('MODEL_NAME'),\n",
        "        'TEMPERATURE': globals().get('TEMPERATURE'),\n",
        "        'TOPIC': globals().get('TOPIC'),\n",
        "        'DOMAIN': globals().get('DOMAIN')\n",
        "    }\n",
        "    \n",
        "    # Optional variables that can be None\n",
        "    optional_vars = {\n",
        "        'EXAMPLES_FILE': globals().get('EXAMPLES_FILE')\n",
        "    }\n",
        "    \n",
        "    print(\"üìã Checking required variables:\")\n",
        "    for var_name, var_value in required_vars.items():\n",
        "        if var_value is None:\n",
        "            print(f\"   ‚ùå {var_name}: NOT DEFINED\")\n",
        "            validation_passed = False\n",
        "        else:\n",
        "            print(f\"   ‚úÖ {var_name}: {str(var_value)[:50]}{'...' if len(str(var_value)) > 50 else ''}\")\n",
        "    \n",
        "    print(\"üìã Checking optional variables:\")\n",
        "    for var_name, var_value in optional_vars.items():\n",
        "        if var_value is None:\n",
        "            print(f\"   ‚úÖ {var_name}: None (optional - will run without examples)\")\n",
        "        else:\n",
        "            print(f\"   ‚úÖ {var_name}: {str(var_value)[:50]}{'...' if len(str(var_value)) > 50 else ''}\")\n",
        "    \n",
        "    # Check DataFrame structure\n",
        "    print(f\"\\nüìä Checking DataFrame structure:\")\n",
        "    required_columns = ['abstract', 'title_full', 'stage_2', 'stage_3']\n",
        "    \n",
        "    if df is None:\n",
        "        print(f\"   ‚ùå DataFrame is None\")\n",
        "        validation_passed = False\n",
        "    else:\n",
        "        print(f\"   ‚úÖ DataFrame shape: {df.shape}\")\n",
        "        \n",
        "        for col in required_columns:\n",
        "            if col in df.columns:\n",
        "                print(f\"   ‚úÖ Column '{col}': Present\")\n",
        "            else:\n",
        "                print(f\"   ‚ùå Column '{col}': MISSING\")\n",
        "                validation_passed = False\n",
        "    \n",
        "    # Check data availability\n",
        "    if df is not None and all(col in df.columns for col in required_columns):\n",
        "        print(f\"\\nüìà Checking data availability:\")\n",
        "        stage2_true = len(df[df['stage_2'] == True])\n",
        "        stage2_false = len(df[df['stage_2'] == False])\n",
        "        stage3_true = len(df[df['stage_3'] == True])\n",
        "        stage3_false = len(df[df['stage_3'] == False])\n",
        "        \n",
        "        print(f\"   üìä Stage 2 True: {stage2_true}\")\n",
        "        print(f\"   üìä Stage 2 False: {stage2_false}\")\n",
        "        print(f\"   üìä Stage 3 True: {stage3_true}\")\n",
        "        print(f\"   üìä Stage 3 False: {stage3_false}\")\n",
        "        \n",
        "        if stage3_true < 10:\n",
        "            print(f\"   ‚ö†Ô∏è  Warning: Only {stage3_true} stage_3=True examples available\")\n",
        "        if stage3_false < 10:\n",
        "            print(f\"   ‚ö†Ô∏è  Warning: Only {stage3_false} stage_3=False examples available\")\n",
        "    \n",
        "    # Check file paths\n",
        "    print(f\"\\nüìÅ Checking file paths:\")\n",
        "    import os\n",
        "    \n",
        "    # CRITERIA_FILE is required\n",
        "    if CRITERIA_FILE and os.path.exists(CRITERIA_FILE):\n",
        "        print(f\"   ‚úÖ Criteria file: {CRITERIA_FILE}\")\n",
        "    elif CRITERIA_FILE:\n",
        "        print(f\"   ‚ùå Criteria file: {CRITERIA_FILE} (NOT FOUND)\")\n",
        "        validation_passed = False\n",
        "    else:\n",
        "        print(f\"   ‚ùå Criteria file: NOT SPECIFIED\")\n",
        "        validation_passed = False\n",
        "    \n",
        "    # EXAMPLES_FILE is optional\n",
        "    if EXAMPLES_FILE is None:\n",
        "        print(f\"   ‚úÖ Examples file: None (will run without examples)\")\n",
        "    elif os.path.exists(EXAMPLES_FILE):\n",
        "        print(f\"   ‚úÖ Examples file: {EXAMPLES_FILE}\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå Examples file: {EXAMPLES_FILE} (NOT FOUND)\")\n",
        "        validation_passed = False\n",
        "    \n",
        "    # Check API function\n",
        "    print(f\"\\nü§ñ Checking API function:\")\n",
        "    if 'screen_abstract_llm' in globals():\n",
        "        print(f\"   ‚úÖ screen_abstract_llm function: Available\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå screen_abstract_llm function: NOT DEFINED\")\n",
        "        validation_passed = False\n",
        "    \n",
        "    # Final result\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    if validation_passed:\n",
        "        print(\"‚úÖ ALL VALIDATIONS PASSED - Ready to run experiment!\")\n",
        "    else:\n",
        "        print(\"‚ùå VALIDATION FAILED - Please fix the issues above before running\")\n",
        "    \n",
        "    return validation_passed\n",
        "\n",
        "# Run validation\n",
        "validation_result = validate_experiment_setup(df_LB, \"LB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî¨ Set Up Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Classification experiment function with MAYBE option support defined\n",
            "üöÄ Ready to run: run_classification_experiment(df_LB, batch_size=15)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix \n",
        "from datetime import datetime\n",
        "import os\n",
        "import time\n",
        "\n",
        "def run_classification_experiment(\n",
        "    df, \n",
        "    n_total_examples=50,  # ‚¨ÖÔ∏è Total number of examples to test\n",
        "    n_stage3_true=5,     # ‚¨ÖÔ∏è Number of stage_3=True examples\n",
        "    n_stage3_false=45,    # ‚¨ÖÔ∏è Number of stage_3=False examples\n",
        "    dataset_source=\"LB\",  # ‚¨ÖÔ∏è Dataset identifier (LB/BM)\n",
        "    batch_size=20,        # ‚¨ÖÔ∏è Batch size for processing (max 20 to avoid timeouts)\n",
        "    save_results=True,    # ‚¨ÖÔ∏è Whether to save results to CSV\n",
        "    verbose=True          # ‚¨ÖÔ∏è Print progress updates\n",
        "):\n",
        "    \"\"\"\n",
        "    Run LLM classification experiment on abstracts with batch processing.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with abstracts (must have 'abstract', 'title_full', 'stage_2', 'stage_3')\n",
        "        n_total_examples: Total number of examples to test\n",
        "        n_stage3_true: Number of stage_3=True examples to include\n",
        "        n_stage3_false: Number of stage_3=False examples to include\n",
        "        dataset_source: Dataset identifier for results filename\n",
        "        batch_size: Number of examples to process in each batch (max 20)\n",
        "        save_results: Whether to save results to CSV\n",
        "        verbose: Whether to print progress\n",
        "    \n",
        "    Returns:\n",
        "        dict: Results including metrics and DataFrame\n",
        "    \"\"\"\n",
        "    \n",
        "    # Validate batch size\n",
        "    if batch_size > 20:\n",
        "        print(\"‚ö†Ô∏è  Warning: Batch size > 20 may cause timeouts. Setting to 20.\")\n",
        "        batch_size = 20\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"üß™ Starting Classification Experiment with Batch Processing\")\n",
        "        print(f\"üìä Dataset: {dataset_source}\")\n",
        "        print(f\"üéØ Total examples: {n_total_examples}\")\n",
        "        print(f\"‚úÖ Stage 3 True: {n_stage3_true}\")\n",
        "        print(f\"‚ùå Stage 3 False: {n_stage3_false}\")\n",
        "        print(f\"üì¶ Batch size: {batch_size}\")\n",
        "        print(\"=\" * 50)\n",
        "    \n",
        "    # Sample examples\n",
        "    stage3_true_samples = df[df['stage_3'] == True].sample(n=n_stage3_true, random_state=42)\n",
        "    stage3_false_samples = df[df['stage_3'] == False].sample(n=n_stage3_false, random_state=42)\n",
        "    \n",
        "    # Combine samples\n",
        "    test_samples = pd.concat([stage3_true_samples, stage3_false_samples]).reset_index(drop=True)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"üìù Sampled {len(test_samples)} examples\")\n",
        "    \n",
        "    # Load criteria and examples text\n",
        "    def load_criteria_text(criteria_file):\n",
        "        try:\n",
        "            criteria_df = pd.read_csv(criteria_file)\n",
        "            criteria_text = \"\"\n",
        "            \n",
        "            # Add inclusion criteria\n",
        "            inclusion_criteria = criteria_df[criteria_df['type'] == 'inclusion']\n",
        "            if len(inclusion_criteria) > 0:\n",
        "                criteria_text += \"**INCLUSION CRITERIA:**\\n\"\n",
        "                for _, row in inclusion_criteria.iterrows():\n",
        "                    criteria_text += f\"- **{row['criterion_id']}**: {row['description']}\\n\"\n",
        "                    if pd.notna(row['examples']) and row['examples'].strip():\n",
        "                        criteria_text += f\"  *Examples: {row['examples']}*\\n\"\n",
        "            \n",
        "            # Add exclusion criteria\n",
        "            exclusion_criteria = criteria_df[criteria_df['type'] == 'exclusion']\n",
        "            if len(exclusion_criteria) > 0:\n",
        "                criteria_text += \"\\n**EXCLUSION CRITERIA:**\\n\"\n",
        "                for _, row in exclusion_criteria.iterrows():\n",
        "                    criteria_text += f\"- **{row['criterion_id']}**: {row['description']}\\n\"\n",
        "                    if pd.notna(row['examples']) and row['examples'].strip():\n",
        "                        criteria_text += f\"  *Examples: {row['examples']}*\\n\"\n",
        "            \n",
        "            return criteria_text\n",
        "        except Exception as e:\n",
        "            return f\"Error loading criteria: {e}\"\n",
        "    \n",
        "    def load_examples_text(examples_file):\n",
        "        if not examples_file:\n",
        "            return \"\"\n",
        "        try:\n",
        "            examples_df = pd.read_csv(examples_file)\n",
        "            examples_text = \"\\n## EXAMPLE DECISIONS:\\n\"\n",
        "            \n",
        "            for _, row in examples_df.iterrows():\n",
        "                decision_label = \"INCLUDE\" if row['decision'].upper() == 'INCLUDE' else \"EXCLUDE\"\n",
        "                examples_text += f\"\\n**{decision_label} Example:**\\n\"\n",
        "                examples_text += f\"*Title:* {row['title']}\\n\"\n",
        "                examples_text += f\"*Abstract:* {row['abstract_text'][:200]}{'...' if len(row['abstract_text']) > 200 else ''}\\n\"\n",
        "                examples_text += f\"‚Üí **{decision_label}** ({row['reasoning']})\\n\"\n",
        "            \n",
        "            return examples_text\n",
        "        except Exception as e:\n",
        "            return f\"\\n## EXAMPLES:\\nError loading examples: {e}\\n\"\n",
        "    \n",
        "    # Load prompt components\n",
        "    criteria_text = load_criteria_text(CRITERIA_FILE)\n",
        "    examples_section = load_examples_text(EXAMPLES_FILE) if EXAMPLES_FILE else \"\"\n",
        "    \n",
        "    # Initialize results list\n",
        "    results_list = []\n",
        "    \n",
        "    # Calculate number of batches\n",
        "    total_examples = len(test_samples)\n",
        "    num_batches = (total_examples + batch_size - 1) // batch_size  # Ceiling division\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"üì¶ Processing {total_examples} examples in {num_batches} batch(es)\")\n",
        "        print(f\"‚è±Ô∏è  Estimated time: ~{num_batches * 2} minutes (2 min per batch)\")\n",
        "    \n",
        "    # Process examples in batches\n",
        "    for batch_idx in range(num_batches):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = min(start_idx + batch_size, total_examples)\n",
        "        batch_samples = test_samples.iloc[start_idx:end_idx]\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"\\nüîÑ Processing Batch {batch_idx + 1}/{num_batches} (examples {start_idx + 1}-{end_idx})\")\n",
        "        \n",
        "        batch_start_time = time.time()\n",
        "        \n",
        "        for idx, row in batch_samples.iterrows():\n",
        "            sample_number = start_idx + (idx - batch_samples.index[0]) + 1\n",
        "            \n",
        "            try:\n",
        "                # Create complete prompt\n",
        "                complete_prompt = USER_PROMPT_TEMPLATE.format(\n",
        "                    topic=TOPIC,\n",
        "                    domain=DOMAIN,\n",
        "                    criteria_text=criteria_text,\n",
        "                    examples_section=examples_section,\n",
        "                    title=row['title_full'],\n",
        "                    abstract=row['abstract'],\n",
        "                    decision_include=DECISION_OPTIONS[0],\n",
        "                    decision_exclude=DECISION_OPTIONS[1]\n",
        "                )\n",
        "                \n",
        "                # Call LLM\n",
        "                llm_result = screen_abstract_llm(\n",
        "                    abstract_text=complete_prompt,\n",
        "                    system_prompt=SYSTEM_PROMPT,\n",
        "                    user_prompt_template=\"{abstract}\",  # Just pass through since we formatted above\n",
        "                    model=MODEL_NAME,\n",
        "                    temperature=TEMPERATURE\n",
        "                )\n",
        "                \n",
        "                # Parse LLM decision\n",
        "                llm_decision = llm_result.get('decision', 'UNKNOWN')\n",
        "                llm_reasoning = llm_result.get('reasoning', 'No reasoning provided')\n",
        "                \n",
        "                # Convert to binary for evaluation - MAYBE counts as INCLUDE (1) for stage_2 comparison\n",
        "                llm_binary = 1 if llm_decision in ['INCLUDE', 'MAYBE'] else 0\n",
        "                stage2_binary = 1 if row['stage_2'] else 0\n",
        "                stage3_binary = 1 if row['stage_3'] else 0\n",
        "                \n",
        "                # Store result\n",
        "                result_row = {\n",
        "                    'example_id': sample_number,\n",
        "                    'title': row['title_full'],\n",
        "                    'abstract': row['abstract'],\n",
        "                    'stage_2_true': row['stage_2'],\n",
        "                    'stage_3_true': row['stage_3'],\n",
        "                    'stage_2_binary': stage2_binary,\n",
        "                    'stage_3_binary': stage3_binary,\n",
        "                    'llm_decision': llm_decision,\n",
        "                    'llm_binary': llm_binary,\n",
        "                    'llm_reasoning': llm_reasoning,\n",
        "                    'experiment_id': EXPERIMENT_ID,\n",
        "                    'dataset_source': dataset_source,\n",
        "                    'system_prompt_id': SYSTEM_PROMPT_ID,\n",
        "                    'user_prompt_id': USER_PROMPT_ID,\n",
        "                    'model': MODEL_NAME,\n",
        "                    'temperature': TEMPERATURE,\n",
        "                    'timestamp': datetime.now().isoformat()\n",
        "                }\n",
        "                \n",
        "                results_list.append(result_row)\n",
        "                \n",
        "            except Exception as e:\n",
        "                if verbose:\n",
        "                    print(f\"‚ùå Error processing example {sample_number}: {e}\")\n",
        "                \n",
        "                # Store error result\n",
        "                result_row = {\n",
        "                    'example_id': sample_number,\n",
        "                    'title': row['title_full'],\n",
        "                    'abstract': row['abstract'],\n",
        "                    'stage_2_true': row['stage_2'],\n",
        "                    'stage_3_true': row['stage_3'],\n",
        "                    'stage_2_binary': 1 if row['stage_2'] else 0,\n",
        "                    'stage_3_binary': 1 if row['stage_3'] else 0,\n",
        "                    'llm_decision': 'ERROR',\n",
        "                    'llm_binary': 0,\n",
        "                    'llm_reasoning': f'Processing error: {e}',\n",
        "                    'experiment_id': EXPERIMENT_ID,\n",
        "                    'dataset_source': dataset_source,\n",
        "                    'system_prompt_id': SYSTEM_PROMPT_ID,\n",
        "                    'user_prompt_id': USER_PROMPT_ID,\n",
        "                    'model': MODEL_NAME,\n",
        "                    'temperature': TEMPERATURE,\n",
        "                    'timestamp': datetime.now().isoformat()\n",
        "                }\n",
        "                \n",
        "                results_list.append(result_row)\n",
        "        \n",
        "        # Batch completion info\n",
        "        batch_time = time.time() - batch_start_time\n",
        "        if verbose:\n",
        "            print(f\"‚úÖ Batch {batch_idx + 1} completed in {batch_time:.1f}s\")\n",
        "            if batch_idx < num_batches - 1:  # Not the last batch\n",
        "                print(f\"‚è≥ Brief pause before next batch...\")\n",
        "                time.sleep(2)  # Small delay between batches\n",
        "    \n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame(results_list)\n",
        "    \n",
        "    # Calculate detailed metrics for stage_2 (MAYBE counts as positive)\n",
        "    valid_results_stage2 = results_df[results_df['llm_decision'] != 'ERROR']\n",
        "    if len(valid_results_stage2) > 0:\n",
        "        y_true_stage2 = valid_results_stage2['stage_2_binary'].values\n",
        "        y_pred_stage2 = valid_results_stage2['llm_binary'].values\n",
        "        \n",
        "        # Basic metrics\n",
        "        accuracy_stage2 = accuracy_score(y_true_stage2, y_pred_stage2)\n",
        "        precision_stage2 = precision_score(y_true_stage2, y_pred_stage2, zero_division=0)\n",
        "        recall_stage2 = recall_score(y_true_stage2, y_pred_stage2, zero_division=0)\n",
        "        f1_stage2 = f1_score(y_true_stage2, y_pred_stage2, zero_division=0)\n",
        "        \n",
        "        # Confusion matrix metrics\n",
        "        tn2, fp2, fn2, tp2 = confusion_matrix(y_true_stage2, y_pred_stage2).ravel()\n",
        "    else:\n",
        "        accuracy_stage2 = precision_stage2 = recall_stage2 = f1_stage2 = 0.0\n",
        "        tp2 = fp2 = tn2 = fn2 = 0\n",
        "    \n",
        "    # Get value counts for LLM decisions\n",
        "    llm_decision_counts = valid_results_stage2['llm_decision'].value_counts() if len(valid_results_stage2) > 0 else {}\n",
        "    \n",
        "    # Updated metrics dictionary (only stage_2, no stage_3)\n",
        "    metrics = {\n",
        "        'stage_2_metrics': {\n",
        "            'accuracy': accuracy_stage2,\n",
        "            'precision': precision_stage2,\n",
        "            'recall': recall_stage2,\n",
        "            'f1_score': f1_stage2,\n",
        "            'tp': int(tp2),\n",
        "            'fp': int(fp2),\n",
        "            'tn': int(tn2),\n",
        "            'fn': int(fn2)\n",
        "        },\n",
        "        'decision_counts': {\n",
        "            'INCLUDE': int(llm_decision_counts.get('INCLUDE', 0)),\n",
        "            'MAYBE': int(llm_decision_counts.get('MAYBE', 0)),\n",
        "            'EXCLUDE': int(llm_decision_counts.get('EXCLUDE', 0)),\n",
        "            'ERROR': int(llm_decision_counts.get('ERROR', 0))\n",
        "        },\n",
        "        'total_examples': len(results_df),\n",
        "        'successful_classifications': len(valid_results_stage2),\n",
        "        'errors': len(results_df) - len(valid_results_stage2)\n",
        "    }\n",
        "    \n",
        "    # Enhanced results printing\n",
        "    if verbose:\n",
        "        print(f\"\\nüìä EXPERIMENT RESULTS\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"üìà Stage 2 Evaluation (MAYBE counted as INCLUDE):\")\n",
        "        print(f\"   Accuracy:  {accuracy_stage2:.3f}\")\n",
        "        print(f\"   Precision: {precision_stage2:.3f}\")\n",
        "        print(f\"   Recall:    {recall_stage2:.3f}\")\n",
        "        print(f\"   F1 Score:  {f1_stage2:.3f}\")\n",
        "        print(f\"   TP: {tp2}, FP: {fp2}, TN: {tn2}, FN: {fn2}\")\n",
        "        print(f\"\\nüìä LLM Decision Distribution:\")\n",
        "        print(f\"   INCLUDE: {metrics['decision_counts']['INCLUDE']}\")\n",
        "        print(f\"   MAYBE:   {metrics['decision_counts']['MAYBE']}\")\n",
        "        print(f\"   EXCLUDE: {metrics['decision_counts']['EXCLUDE']}\")\n",
        "        print(f\"   ERROR:   {metrics['decision_counts']['ERROR']}\")\n",
        "        print(f\"\\nüìã Processing Summary:\")\n",
        "        print(f\"   Total examples: {len(results_df)}\")\n",
        "        print(f\"   Successful: {len(valid_results_stage2)}\")\n",
        "        print(f\"   Errors: {len(results_df) - len(valid_results_stage2)}\")\n",
        "    \n",
        "    # Save results\n",
        "    if save_results:\n",
        "        # Create filename with timestamp\n",
        "        timestamp = datetime.now().strftime(\"%m%d%H%M\")\n",
        "        filename = f\"{EXPERIMENT_ID}_{dataset_source}_{timestamp}.csv\"\n",
        "        results_dir = \"../results\"\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "        output_path = os.path.join(results_dir, filename)\n",
        "        \n",
        "        results_df.to_csv(output_path, index=False)\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"\\nüíæ Results saved to: {output_path}\")\n",
        "    \n",
        "    return {\n",
        "        'results_df': results_df,\n",
        "        'metrics': metrics,\n",
        "        'filename': filename if save_results else None\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Classification experiment function with MAYBE option support defined\")\n",
        "print(\"üöÄ Ready to run: run_classification_experiment(df_LB, batch_size=15)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Run experiment! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Starting Classification Experiment with Batch Processing\n",
            "üìä Dataset: LB\n",
            "üéØ Total examples: 50\n",
            "‚úÖ Stage 3 True: 5\n",
            "‚ùå Stage 3 False: 45\n",
            "üì¶ Batch size: 20\n",
            "==================================================\n",
            "üìù Sampled 50 examples\n",
            "üì¶ Processing 50 examples in 3 batch(es)\n",
            "‚è±Ô∏è  Estimated time: ~6 minutes (2 min per batch)\n",
            "\n",
            "üîÑ Processing Batch 1/3 (examples 1-20)\n",
            "‚úÖ Batch 1 completed in 94.8s\n",
            "‚è≥ Brief pause before next batch...\n",
            "\n",
            "üîÑ Processing Batch 2/3 (examples 21-40)\n",
            "‚úÖ Batch 2 completed in 103.0s\n",
            "‚è≥ Brief pause before next batch...\n",
            "\n",
            "üîÑ Processing Batch 3/3 (examples 41-50)\n",
            "‚úÖ Batch 3 completed in 46.0s\n",
            "\n",
            "üìä EXPERIMENT RESULTS\n",
            "==================================================\n",
            "üìà Stage 2 Evaluation (MAYBE counted as INCLUDE):\n",
            "   Accuracy:  0.820\n",
            "   Precision: 0.444\n",
            "   Recall:    0.500\n",
            "   F1 Score:  0.471\n",
            "   TP: 4, FP: 5, TN: 37, FN: 4\n",
            "\n",
            "üìä LLM Decision Distribution:\n",
            "   INCLUDE: 9\n",
            "   MAYBE:   0\n",
            "   EXCLUDE: 41\n",
            "   ERROR:   0\n",
            "\n",
            "üìã Processing Summary:\n",
            "   Total examples: 50\n",
            "   Successful: 50\n",
            "   Errors: 0\n",
            "\n",
            "üíæ Results saved to: ../results/004_LB_08131718.csv\n"
          ]
        }
      ],
      "source": [
        "# Run experiment with default settings\n",
        "results = run_classification_experiment(df_LB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Results Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load results file - you can specify the exact file path here\n",
        "RESULTS_FILE_PATH = \"../results/0002_LB_08131412.csv\"  # ‚¨ÖÔ∏è Change this to your specific file path\n",
        "\n",
        "# Alternative: Set to None to auto-load the most recent file\n",
        "# RESULTS_FILE_PATH = None\n",
        "\n",
        "if RESULTS_FILE_PATH:\n",
        "    # Load specific file\n",
        "    if os.path.exists(RESULTS_FILE_PATH):\n",
        "        print(f\"üìÅ Loading specified file: {os.path.basename(RESULTS_FILE_PATH)}\")\n",
        "        df_results = pd.read_csv(RESULTS_FILE_PATH)\n",
        "    else:\n",
        "        print(f\"‚ùå Error: File not found: {RESULTS_FILE_PATH}\")\n",
        "        df_results = None\n",
        "else:\n",
        "    # Auto-load most recent file (original behavior)\n",
        "    results_dir = \"../results\"\n",
        "    result_files = [f for f in os.listdir(results_dir) if f.endswith('.csv')]\n",
        "    if result_files:\n",
        "        latest_file = sorted(result_files)[-1]\n",
        "        file_path = os.path.join(results_dir, latest_file)\n",
        "        print(f\"üìÅ Auto-loading most recent file: {latest_file}\")\n",
        "        df_results = pd.read_csv(file_path)\n",
        "    else:\n",
        "        print(\"‚ùå No result files found in ../results directory\")\n",
        "        df_results = None\n",
        "\n",
        "# Continue with analysis if file was loaded successfully\n",
        "if df_results is not None:\n",
        "    print(f\"\\nüìä RESULTS OVERVIEW\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Shape: {df_results.shape}\")\n",
        "    print(f\"Columns: {list(df_results.columns)}\")\n",
        "    \n",
        "    print(f\"\\nüéØ DECISION SUMMARY\")\n",
        "    print(\"=\" * 30)\n",
        "    print(df_results['llm_decision'].value_counts())\n",
        "    \n",
        "    print(f\"\\nüìà PERFORMANCE PREVIEW\")\n",
        "    print(\"=\" * 30)\n",
        "    print(\"Stage 2 vs LLM:\")\n",
        "    print(pd.crosstab(df_results['stage_2_true'], df_results['llm_decision']))\n",
        "    print(\"\\nStage 3 vs LLM:\")\n",
        "    print(pd.crosstab(df_results['stage_3_true'], df_results['llm_decision']))\n",
        "    \n",
        "    print(f\"\\nüìã FIRST FEW RESULTS\")\n",
        "    print(\"=\" * 30)\n",
        "    display(df_results[['example_id', 'stage_2_true', 'stage_3_true', 'llm_decision', 'llm_reasoning']].head())\n",
        "else:\n",
        "    print(\"‚ùå Could not load results file for analysis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display full reasoning for first 5 examples\n",
        "print(\"ü§ñ FULL LLM REASONING EXAMPLES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for idx in range(min(5, len(df_results))):\n",
        "    row = df_results.iloc[idx]\n",
        "    print(f\"\\nüìã EXAMPLE {row['example_id']} - {row['llm_decision']}\")\n",
        "    print(f\"üéØ Ground Truth: Stage 2={row['stage_2_true']}, Stage 3={row['stage_3_true']}\")\n",
        "    print(f\"üìñ Title: {row['title'][:100]}{'...' if len(row['title']) > 100 else ''}\")\n",
        "    print(f\"\\nüí≠ FULL REASONING:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(row['llm_reasoning'])\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ûï Add experiment info to the results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Added experiment 004 to existing summary\n",
            "üíæ Summary saved to: ../results/experiment_summary.csv\n",
            "\n",
            "üìã LAST 5 EXPERIMENTS IN SUMMARY:\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/xg/jz066c5d5jn87vqnb688_0d00000gn/T/ipykernel_73458/3884107583.py:53: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_summary = pd.concat([existing_summary, new_row], ignore_index=True)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>experiment_id</th>\n",
              "      <th>experiment_date</th>\n",
              "      <th>experiment_category</th>\n",
              "      <th>experiment_goal</th>\n",
              "      <th>system_prompt_id</th>\n",
              "      <th>user_prompt_id</th>\n",
              "      <th>model_name</th>\n",
              "      <th>temperature</th>\n",
              "      <th>max_tokens</th>\n",
              "      <th>criteria_file</th>\n",
              "      <th>examples_file</th>\n",
              "      <th>output_format</th>\n",
              "      <th>domain</th>\n",
              "      <th>topic</th>\n",
              "      <th>dataset_source</th>\n",
              "      <th>n_total_examples</th>\n",
              "      <th>n_successful</th>\n",
              "      <th>n_errors</th>\n",
              "      <th>stage2_accuracy</th>\n",
              "      <th>stage2_precision</th>\n",
              "      <th>stage2_recall</th>\n",
              "      <th>stage2_f1</th>\n",
              "      <th>stage2_tp</th>\n",
              "      <th>stage2_fp</th>\n",
              "      <th>stage2_tn</th>\n",
              "      <th>stage2_fn</th>\n",
              "      <th>stage3_accuracy</th>\n",
              "      <th>stage3_precision</th>\n",
              "      <th>stage3_recall</th>\n",
              "      <th>stage3_f1</th>\n",
              "      <th>stage3_tp</th>\n",
              "      <th>stage3_fp</th>\n",
              "      <th>stage3_tn</th>\n",
              "      <th>stage3_fn</th>\n",
              "      <th>results_filename</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>llm_include_count</th>\n",
              "      <th>llm_maybe_count</th>\n",
              "      <th>llm_exclude_count</th>\n",
              "      <th>llm_error_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2025-08-13</td>\n",
              "      <td>Testing</td>\n",
              "      <td>Test Set Up</td>\n",
              "      <td>SYS_001</td>\n",
              "      <td>USR_001</td>\n",
              "      <td>gpt-4o</td>\n",
              "      <td>0.000</td>\n",
              "      <td>4000</td>\n",
              "      <td>../prompts/Criteria_LB_01.csv</td>\n",
              "      <td>../prompts/exmpl_single_LB_01.csv</td>\n",
              "      <td>Binary</td>\n",
              "      <td>political_communication</td>\n",
              "      <td>media_dviersity</td>\n",
              "      <td>LB</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>0.880</td>\n",
              "      <td>0.800</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.727</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>0.920</td>\n",
              "      <td>0.800</td>\n",
              "      <td>0.800</td>\n",
              "      <td>0.800</td>\n",
              "      <td>4.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>19.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>001_LB_08131314.csv</td>\n",
              "      <td>2025-08-13T13:14:33.374055</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2025-08-13</td>\n",
              "      <td>Testing</td>\n",
              "      <td>Test Set Up</td>\n",
              "      <td>SYS_001</td>\n",
              "      <td>USR_002</td>\n",
              "      <td>gpt-4o</td>\n",
              "      <td>0.000</td>\n",
              "      <td>4000</td>\n",
              "      <td>../prompts/Criteria_LB_01.csv</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Binary</td>\n",
              "      <td>political_communication</td>\n",
              "      <td>media_diversity</td>\n",
              "      <td>LB</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>0</td>\n",
              "      <td>0.840</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.500</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>38</td>\n",
              "      <td>4</td>\n",
              "      <td>0.900</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.800</td>\n",
              "      <td>0.615</td>\n",
              "      <td>4.000</td>\n",
              "      <td>4.000</td>\n",
              "      <td>41.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0002_LB_08131412.csv</td>\n",
              "      <td>2025-08-13T14:18:25.943746</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2025-08-13</td>\n",
              "      <td>Testing</td>\n",
              "      <td>Test Set Up</td>\n",
              "      <td>SYS_001</td>\n",
              "      <td>USR_001</td>\n",
              "      <td>gpt-4o</td>\n",
              "      <td>0.000</td>\n",
              "      <td>4000</td>\n",
              "      <td>../prompts/Criteria_LB_01.csv</td>\n",
              "      <td>../prompts/exmpl_five_LB_01.csv</td>\n",
              "      <td>Binary</td>\n",
              "      <td>political_communication</td>\n",
              "      <td>media_diversity</td>\n",
              "      <td>LB</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>0</td>\n",
              "      <td>0.840</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.556</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>37</td>\n",
              "      <td>3</td>\n",
              "      <td>0.860</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.800</td>\n",
              "      <td>0.533</td>\n",
              "      <td>4.000</td>\n",
              "      <td>6.000</td>\n",
              "      <td>39.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>003_LB_08131430.csv</td>\n",
              "      <td>2025-08-13T14:34:00.733833</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2025-08-13</td>\n",
              "      <td>Testing</td>\n",
              "      <td>Test Set Up</td>\n",
              "      <td>SYS_001</td>\n",
              "      <td>USR_003</td>\n",
              "      <td>gpt-4o</td>\n",
              "      <td>0.000</td>\n",
              "      <td>4000</td>\n",
              "      <td>../prompts/Criteria_LB_01.csv</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Yes/Maybe/No</td>\n",
              "      <td>political_communication</td>\n",
              "      <td>media_diversity</td>\n",
              "      <td>LB</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>0</td>\n",
              "      <td>0.820</td>\n",
              "      <td>0.462</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.571</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>35</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>004_LB_08131504.csv</td>\n",
              "      <td>2025-08-13T15:08:35.001340</td>\n",
              "      <td>13.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>37.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>004</td>\n",
              "      <td>2025-08-13</td>\n",
              "      <td>Testing</td>\n",
              "      <td>Test Set Up</td>\n",
              "      <td>SYS_001</td>\n",
              "      <td>USR_004</td>\n",
              "      <td>gpt-4o</td>\n",
              "      <td>0.000</td>\n",
              "      <td>4000</td>\n",
              "      <td>../prompts/Criteria_LB_01.csv</td>\n",
              "      <td>../prompts/exmpl_single_LB_01.csv</td>\n",
              "      <td>Yes/Maybe/No</td>\n",
              "      <td>political_communication</td>\n",
              "      <td>media_diversity</td>\n",
              "      <td>LB</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>0</td>\n",
              "      <td>0.820</td>\n",
              "      <td>0.444</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.471</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>37</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>004_LB_08131718.csv</td>\n",
              "      <td>2025-08-13T17:20:17.593168</td>\n",
              "      <td>9.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>41.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  experiment_id experiment_date experiment_category experiment_goal system_prompt_id user_prompt_id model_name  temperature  max_tokens                  criteria_file                      examples_file output_format                   domain            topic dataset_source  n_total_examples  n_successful  n_errors  stage2_accuracy  stage2_precision  stage2_recall  stage2_f1  stage2_tp  stage2_fp  stage2_tn  stage2_fn  stage3_accuracy  stage3_precision  stage3_recall  stage3_f1  stage3_tp  stage3_fp  stage3_tn  stage3_fn      results_filename                   timestamp  llm_include_count  llm_maybe_count  llm_exclude_count  llm_error_count\n",
              "0             1      2025-08-13             Testing     Test Set Up          SYS_001        USR_001     gpt-4o        0.000        4000  ../prompts/Criteria_LB_01.csv  ../prompts/exmpl_single_LB_01.csv        Binary  political_communication  media_dviersity             LB                25            25         0            0.880             0.800          0.667      0.727          4          1         18          2            0.920             0.800          0.800      0.800      4.000      1.000     19.000      1.000   001_LB_08131314.csv  2025-08-13T13:14:33.374055                NaN              NaN                NaN              NaN\n",
              "1             2      2025-08-13             Testing     Test Set Up          SYS_001        USR_002     gpt-4o        0.000        4000  ../prompts/Criteria_LB_01.csv                                NaN        Binary  political_communication  media_diversity             LB                50            50         0            0.840             0.500          0.500      0.500          4          4         38          4            0.900             0.500          0.800      0.615      4.000      4.000     41.000      1.000  0002_LB_08131412.csv  2025-08-13T14:18:25.943746                NaN              NaN                NaN              NaN\n",
              "2             3      2025-08-13             Testing     Test Set Up          SYS_001        USR_001     gpt-4o        0.000        4000  ../prompts/Criteria_LB_01.csv    ../prompts/exmpl_five_LB_01.csv        Binary  political_communication  media_diversity             LB                50            50         0            0.840             0.500          0.625      0.556          5          5         37          3            0.860             0.400          0.800      0.533      4.000      6.000     39.000      1.000   003_LB_08131430.csv  2025-08-13T14:34:00.733833                NaN              NaN                NaN              NaN\n",
              "3             4      2025-08-13             Testing     Test Set Up          SYS_001        USR_003     gpt-4o        0.000        4000  ../prompts/Criteria_LB_01.csv                                NaN  Yes/Maybe/No  political_communication  media_diversity             LB                50            50         0            0.820             0.462          0.750      0.571          6          7         35          2              NaN               NaN            NaN        NaN        NaN        NaN        NaN        NaN   004_LB_08131504.csv  2025-08-13T15:08:35.001340             13.000            0.000             37.000            0.000\n",
              "4           004      2025-08-13             Testing     Test Set Up          SYS_001        USR_004     gpt-4o        0.000        4000  ../prompts/Criteria_LB_01.csv  ../prompts/exmpl_single_LB_01.csv  Yes/Maybe/No  political_communication  media_diversity             LB                50            50         0            0.820             0.444          0.500      0.471          4          5         37          4              NaN               NaN            NaN        NaN        NaN        NaN        NaN        NaN   004_LB_08131718.csv  2025-08-13T17:20:17.593168              9.000            0.000             41.000            0.000"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä SUMMARY STATS:\n",
            "   Total experiments: 5\n",
            "   Unique experiment IDs: 5\n",
            "   Datasets used: ['LB']\n"
          ]
        }
      ],
      "source": [
        "def add_experiment_to_summary(results_dict, summary_file=\"../results/experiment_summary.csv\"):\n",
        "    \"\"\"Add new experiment results to the summary DataFrame with confusion matrix metrics and decision counts\"\"\"\n",
        "    \n",
        "    new_row = pd.DataFrame({\n",
        "        'experiment_id': [EXPERIMENT_ID],\n",
        "        'experiment_date': [EXPERIMENT_DATE],\n",
        "        'experiment_category': [EXPERIMENT_CATEGORY],\n",
        "        'experiment_goal': [EXPERIMENT_GOAL],\n",
        "        'system_prompt_id': [SYSTEM_PROMPT_ID],\n",
        "        'user_prompt_id': [USER_PROMPT_ID],\n",
        "        'model_name': [MODEL_NAME],\n",
        "        'temperature': [TEMPERATURE],\n",
        "        'max_tokens': [MAX_TOKENS],\n",
        "        'criteria_file': [CRITERIA_FILE],\n",
        "        'examples_file': [EXAMPLES_FILE],\n",
        "        'output_format': [OUTPUT_FORMAT],\n",
        "        'domain': [DOMAIN],\n",
        "        'topic': [TOPIC],\n",
        "        'dataset_source': [DATASET_SOURCE],\n",
        "        'n_total_examples': [results_dict['metrics']['total_examples']],\n",
        "        'n_successful': [results_dict['metrics']['successful_classifications']],\n",
        "        'n_errors': [results_dict['metrics']['errors']],\n",
        "        # Stage 2 metrics\n",
        "        'stage2_accuracy': [results_dict['metrics']['stage_2_metrics']['accuracy']],\n",
        "        'stage2_precision': [results_dict['metrics']['stage_2_metrics']['precision']],\n",
        "        'stage2_recall': [results_dict['metrics']['stage_2_metrics']['recall']],\n",
        "        'stage2_f1': [results_dict['metrics']['stage_2_metrics']['f1_score']],\n",
        "        'stage2_tp': [results_dict['metrics']['stage_2_metrics']['tp']],\n",
        "        'stage2_fp': [results_dict['metrics']['stage_2_metrics']['fp']],\n",
        "        'stage2_tn': [results_dict['metrics']['stage_2_metrics']['tn']],\n",
        "        'stage2_fn': [results_dict['metrics']['stage_2_metrics']['fn']],\n",
        "        # Decision counts (new columns for MAYBE experiments)\n",
        "        'llm_include_count': [results_dict['metrics']['decision_counts']['INCLUDE']],\n",
        "        'llm_maybe_count': [results_dict['metrics']['decision_counts']['MAYBE']],\n",
        "        'llm_exclude_count': [results_dict['metrics']['decision_counts']['EXCLUDE']],\n",
        "        'llm_error_count': [results_dict['metrics']['decision_counts']['ERROR']],\n",
        "        # Stage 3 metrics (set to None for MAYBE experiments)\n",
        "        'stage3_accuracy': [None],\n",
        "        'stage3_precision': [None],\n",
        "        'stage3_recall': [None],\n",
        "        'stage3_f1': [None],\n",
        "        'stage3_tp': [None],\n",
        "        'stage3_fp': [None],\n",
        "        'stage3_tn': [None],\n",
        "        'stage3_fn': [None],\n",
        "        'results_filename': [results_dict['filename']],\n",
        "        'timestamp': [datetime.now().isoformat()]\n",
        "    })\n",
        "    \n",
        "    # Load existing summary or create new one\n",
        "    if os.path.exists(summary_file):\n",
        "        existing_summary = pd.read_csv(summary_file)\n",
        "        updated_summary = pd.concat([existing_summary, new_row], ignore_index=True)\n",
        "        print(f\"‚úÖ Added experiment {EXPERIMENT_ID} to existing summary\")\n",
        "    else:\n",
        "        updated_summary = new_row\n",
        "        print(f\"‚úÖ Created new summary file with experiment {EXPERIMENT_ID}\")\n",
        "    \n",
        "    # Save updated summary\n",
        "    updated_summary.to_csv(summary_file, index=False)\n",
        "    print(f\"üíæ Summary saved to: {summary_file}\")\n",
        "    \n",
        "    # Display last 5 rows for verification\n",
        "    print(f\"\\nüìã LAST 5 EXPERIMENTS IN SUMMARY:\")\n",
        "    print(\"=\" * 50)\n",
        "    display(updated_summary.tail())\n",
        "    \n",
        "    print(f\"\\nüìä SUMMARY STATS:\")\n",
        "    print(f\"   Total experiments: {len(updated_summary)}\")\n",
        "    print(f\"   Unique experiment IDs: {updated_summary['experiment_id'].nunique()}\")\n",
        "    print(f\"   Datasets used: {updated_summary['dataset_source'].unique().tolist()}\")\n",
        "    \n",
        "    return updated_summary\n",
        "\n",
        "# Usage example (uncomment to run):\n",
        "summary_df = add_experiment_to_summary(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Experiment ID corrected\n",
            "üìä Row at index 3 now has experiment_id: 4\n",
            "\n",
            "üìã LAST 5 EXPERIMENTS IN SUMMARY:\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/xg/jz066c5d5jn87vqnb688_0d00000gn/T/ipykernel_73458/3196894009.py:5: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '005' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  summary_df.loc[4, 'experiment_id'] = '005'\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>experiment_id</th>\n",
              "      <th>experiment_date</th>\n",
              "      <th>experiment_category</th>\n",
              "      <th>experiment_goal</th>\n",
              "      <th>system_prompt_id</th>\n",
              "      <th>user_prompt_id</th>\n",
              "      <th>model_name</th>\n",
              "      <th>temperature</th>\n",
              "      <th>max_tokens</th>\n",
              "      <th>criteria_file</th>\n",
              "      <th>examples_file</th>\n",
              "      <th>output_format</th>\n",
              "      <th>domain</th>\n",
              "      <th>topic</th>\n",
              "      <th>dataset_source</th>\n",
              "      <th>n_total_examples</th>\n",
              "      <th>n_successful</th>\n",
              "      <th>n_errors</th>\n",
              "      <th>stage2_accuracy</th>\n",
              "      <th>stage2_precision</th>\n",
              "      <th>stage2_recall</th>\n",
              "      <th>stage2_f1</th>\n",
              "      <th>stage2_tp</th>\n",
              "      <th>stage2_fp</th>\n",
              "      <th>stage2_tn</th>\n",
              "      <th>stage2_fn</th>\n",
              "      <th>stage3_accuracy</th>\n",
              "      <th>stage3_precision</th>\n",
              "      <th>stage3_recall</th>\n",
              "      <th>stage3_f1</th>\n",
              "      <th>stage3_tp</th>\n",
              "      <th>stage3_fp</th>\n",
              "      <th>stage3_tn</th>\n",
              "      <th>stage3_fn</th>\n",
              "      <th>results_filename</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>llm_include_count</th>\n",
              "      <th>llm_maybe_count</th>\n",
              "      <th>llm_exclude_count</th>\n",
              "      <th>llm_error_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2025-08-13</td>\n",
              "      <td>Testing</td>\n",
              "      <td>Test Set Up</td>\n",
              "      <td>SYS_001</td>\n",
              "      <td>USR_001</td>\n",
              "      <td>gpt-4o</td>\n",
              "      <td>0.000</td>\n",
              "      <td>4000</td>\n",
              "      <td>../prompts/Criteria_LB_01.csv</td>\n",
              "      <td>../prompts/exmpl_single_LB_01.csv</td>\n",
              "      <td>Binary</td>\n",
              "      <td>political_communication</td>\n",
              "      <td>media_dviersity</td>\n",
              "      <td>LB</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>0.880</td>\n",
              "      <td>0.800</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.727</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>0.920</td>\n",
              "      <td>0.800</td>\n",
              "      <td>0.800</td>\n",
              "      <td>0.800</td>\n",
              "      <td>4.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>19.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>001_LB_08131314.csv</td>\n",
              "      <td>2025-08-13T13:14:33.374055</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2025-08-13</td>\n",
              "      <td>Testing</td>\n",
              "      <td>Test Set Up</td>\n",
              "      <td>SYS_001</td>\n",
              "      <td>USR_002</td>\n",
              "      <td>gpt-4o</td>\n",
              "      <td>0.000</td>\n",
              "      <td>4000</td>\n",
              "      <td>../prompts/Criteria_LB_01.csv</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Binary</td>\n",
              "      <td>political_communication</td>\n",
              "      <td>media_diversity</td>\n",
              "      <td>LB</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>0</td>\n",
              "      <td>0.840</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.500</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>38</td>\n",
              "      <td>4</td>\n",
              "      <td>0.900</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.800</td>\n",
              "      <td>0.615</td>\n",
              "      <td>4.000</td>\n",
              "      <td>4.000</td>\n",
              "      <td>41.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0002_LB_08131412.csv</td>\n",
              "      <td>2025-08-13T14:18:25.943746</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2025-08-13</td>\n",
              "      <td>Testing</td>\n",
              "      <td>Test Set Up</td>\n",
              "      <td>SYS_001</td>\n",
              "      <td>USR_001</td>\n",
              "      <td>gpt-4o</td>\n",
              "      <td>0.000</td>\n",
              "      <td>4000</td>\n",
              "      <td>../prompts/Criteria_LB_01.csv</td>\n",
              "      <td>../prompts/exmpl_five_LB_01.csv</td>\n",
              "      <td>Binary</td>\n",
              "      <td>political_communication</td>\n",
              "      <td>media_diversity</td>\n",
              "      <td>LB</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>0</td>\n",
              "      <td>0.840</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.556</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>37</td>\n",
              "      <td>3</td>\n",
              "      <td>0.860</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.800</td>\n",
              "      <td>0.533</td>\n",
              "      <td>4.000</td>\n",
              "      <td>6.000</td>\n",
              "      <td>39.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>003_LB_08131430.csv</td>\n",
              "      <td>2025-08-13T14:34:00.733833</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2025-08-13</td>\n",
              "      <td>Testing</td>\n",
              "      <td>Test Set Up</td>\n",
              "      <td>SYS_001</td>\n",
              "      <td>USR_003</td>\n",
              "      <td>gpt-4o</td>\n",
              "      <td>0.000</td>\n",
              "      <td>4000</td>\n",
              "      <td>../prompts/Criteria_LB_01.csv</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Yes/Maybe/No</td>\n",
              "      <td>political_communication</td>\n",
              "      <td>media_diversity</td>\n",
              "      <td>LB</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>0</td>\n",
              "      <td>0.820</td>\n",
              "      <td>0.462</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.571</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>35</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>004_LB_08131504.csv</td>\n",
              "      <td>2025-08-13T15:08:35.001340</td>\n",
              "      <td>13.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>37.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>005</td>\n",
              "      <td>2025-08-13</td>\n",
              "      <td>Testing</td>\n",
              "      <td>Test Set Up</td>\n",
              "      <td>SYS_001</td>\n",
              "      <td>USR_004</td>\n",
              "      <td>gpt-4o</td>\n",
              "      <td>0.000</td>\n",
              "      <td>4000</td>\n",
              "      <td>../prompts/Criteria_LB_01.csv</td>\n",
              "      <td>../prompts/exmpl_single_LB_01.csv</td>\n",
              "      <td>Yes/Maybe/No</td>\n",
              "      <td>political_communication</td>\n",
              "      <td>media_diversity</td>\n",
              "      <td>LB</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>0</td>\n",
              "      <td>0.820</td>\n",
              "      <td>0.444</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.471</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>37</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>004_LB_08131718.csv</td>\n",
              "      <td>2025-08-13T17:20:17.593168</td>\n",
              "      <td>9.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>41.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  experiment_id experiment_date experiment_category experiment_goal system_prompt_id user_prompt_id model_name  temperature  max_tokens                  criteria_file                      examples_file output_format                   domain            topic dataset_source  n_total_examples  n_successful  n_errors  stage2_accuracy  stage2_precision  stage2_recall  stage2_f1  stage2_tp  stage2_fp  stage2_tn  stage2_fn  stage3_accuracy  stage3_precision  stage3_recall  stage3_f1  stage3_tp  stage3_fp  stage3_tn  stage3_fn      results_filename                   timestamp  llm_include_count  llm_maybe_count  llm_exclude_count  llm_error_count\n",
              "0             1      2025-08-13             Testing     Test Set Up          SYS_001        USR_001     gpt-4o        0.000        4000  ../prompts/Criteria_LB_01.csv  ../prompts/exmpl_single_LB_01.csv        Binary  political_communication  media_dviersity             LB                25            25         0            0.880             0.800          0.667      0.727          4          1         18          2            0.920             0.800          0.800      0.800      4.000      1.000     19.000      1.000   001_LB_08131314.csv  2025-08-13T13:14:33.374055                NaN              NaN                NaN              NaN\n",
              "1             2      2025-08-13             Testing     Test Set Up          SYS_001        USR_002     gpt-4o        0.000        4000  ../prompts/Criteria_LB_01.csv                                NaN        Binary  political_communication  media_diversity             LB                50            50         0            0.840             0.500          0.500      0.500          4          4         38          4            0.900             0.500          0.800      0.615      4.000      4.000     41.000      1.000  0002_LB_08131412.csv  2025-08-13T14:18:25.943746                NaN              NaN                NaN              NaN\n",
              "2             3      2025-08-13             Testing     Test Set Up          SYS_001        USR_001     gpt-4o        0.000        4000  ../prompts/Criteria_LB_01.csv    ../prompts/exmpl_five_LB_01.csv        Binary  political_communication  media_diversity             LB                50            50         0            0.840             0.500          0.625      0.556          5          5         37          3            0.860             0.400          0.800      0.533      4.000      6.000     39.000      1.000   003_LB_08131430.csv  2025-08-13T14:34:00.733833                NaN              NaN                NaN              NaN\n",
              "3             4      2025-08-13             Testing     Test Set Up          SYS_001        USR_003     gpt-4o        0.000        4000  ../prompts/Criteria_LB_01.csv                                NaN  Yes/Maybe/No  political_communication  media_diversity             LB                50            50         0            0.820             0.462          0.750      0.571          6          7         35          2              NaN               NaN            NaN        NaN        NaN        NaN        NaN        NaN   004_LB_08131504.csv  2025-08-13T15:08:35.001340             13.000            0.000             37.000            0.000\n",
              "4           005      2025-08-13             Testing     Test Set Up          SYS_001        USR_004     gpt-4o        0.000        4000  ../prompts/Criteria_LB_01.csv  ../prompts/exmpl_single_LB_01.csv  Yes/Maybe/No  political_communication  media_diversity             LB                50            50         0            0.820             0.444          0.500      0.471          4          5         37          4              NaN               NaN            NaN        NaN        NaN        NaN        NaN        NaN   004_LB_08131718.csv  2025-08-13T17:20:17.593168              9.000            0.000             41.000            0.000"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load the experiment summary\n",
        "summary_df = pd.read_csv(\"../results/experiment_summary.csv\")\n",
        "\n",
        "# Change experiment_id from 4 to 5 for the row at index 3 (4th row, 0-indexed)\n",
        "summary_df.loc[4, 'experiment_id'] = '005'\n",
        "\n",
        "# Save the corrected summary back to file\n",
        "summary_df.to_csv(\"../results/experiment_summary.csv\", index=False)\n",
        "\n",
        "# Verify the change\n",
        "print(\"‚úÖ Experiment ID corrected\")\n",
        "print(f\"üìä Row at index 3 now has experiment_id: {summary_df.loc[3, 'experiment_id']}\")\n",
        "\n",
        "# Display the last 5 rows to confirm\n",
        "print(f\"\\nüìã LAST 5 EXPERIMENTS IN SUMMARY:\")\n",
        "print(\"=\" * 50)\n",
        "display(summary_df.tail())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Conclusions and Next Steps\n",
        "\n",
        "### Key Findings\n",
        "- \n",
        "\n",
        "### Next Steps\n",
        "- [Suggest follow-up experiments]\n",
        "- [List potential improvements]\n",
        "- [Identify areas for further investigation]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "SLRenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
