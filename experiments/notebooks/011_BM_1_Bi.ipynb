{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß™ SLR Abstract Screening Experiment\n",
        "#### Experiment Information\n",
        "- **ID**: 011\n",
        "- **Date**: 08/14\n",
        "#### üéØ Goal\n",
        "- Test Set Up on small dataset\n",
        "#### ‚öôÔ∏è Configuration\n",
        "- **LLM** : GPT-4o\n",
        "- **Data**: BM\n",
        "- **Examples** : Single\n",
        "- **Output**: Binary\n",
        "#### üìù Notes\n",
        "- \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîß Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Essential imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import json\n",
        "from pathlib import Path\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "# Configure pandas display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_theme()  # This is the correct way to set seaborn style\n",
        "plt.rcParams['figure.figsize'] = (12, 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì First dataset loaded successfully\n",
            "‚úì Shape of dataset 1: (3944, 15)\n",
            "\n",
            "‚úì Second dataset loaded successfully\n",
            "‚úì Shape of dataset 2: (917, 13)\n",
            "\n",
            "First few rows of dataset 1:\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>abstract</th>\n",
              "      <th>acmid</th>\n",
              "      <th>author</th>\n",
              "      <th>doi</th>\n",
              "      <th>outlet</th>\n",
              "      <th>title_full</th>\n",
              "      <th>url</th>\n",
              "      <th>year</th>\n",
              "      <th>qualtrics_id</th>\n",
              "      <th>wos_id</th>\n",
              "      <th>ebsco_id</th>\n",
              "      <th>stage_1</th>\n",
              "      <th>stage_2</th>\n",
              "      <th>stage_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bindu2018503</td>\n",
              "      <td>Online social networks have become immensely p...</td>\n",
              "      <td></td>\n",
              "      <td>Bindu, P V and Mishra, R and Thilagam, P S</td>\n",
              "      <td>10.1007/s10844-017-0494-z</td>\n",
              "      <td>Journal of Intelligent Information Systems</td>\n",
              "      <td>{Discovering spammer communities in TWITTER}</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>2018</td>\n",
              "      <td>12</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Moraga2018470</td>\n",
              "      <td>This article explores the ways Latinos‚Äîas audi...</td>\n",
              "      <td></td>\n",
              "      <td>Moraga, J E</td>\n",
              "      <td>10.1177/0193723518797030</td>\n",
              "      <td>Journal of Sport and Social Issues</td>\n",
              "      <td>{On ESPN Deportes: Latinos, Sport MEDIA, and t...</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>2018</td>\n",
              "      <td>22</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Lanosga20181676</td>\n",
              "      <td>This study of American investigative reporting...</td>\n",
              "      <td></td>\n",
              "      <td>Lanosga, G and Martin, J</td>\n",
              "      <td>10.1177/1464884916683555</td>\n",
              "      <td>JOURNALISm</td>\n",
              "      <td>{JOURNALISts, sources, and policy outcomes: In...</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>2018</td>\n",
              "      <td>47</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Warner2018720</td>\n",
              "      <td>In this study, we test the indirect and condit...</td>\n",
              "      <td></td>\n",
              "      <td>Warner, B R and Jennings, F J and Bramlett, J ...</td>\n",
              "      <td>10.1080/15205436.2018.1472283</td>\n",
              "      <td>Mass Communication and Society</td>\n",
              "      <td>{A MultiMEDIA Analysis of Persuasion in the 20...</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>2018</td>\n",
              "      <td>50</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Burrows20181117</td>\n",
              "      <td>Professional communicators produce a diverse r...</td>\n",
              "      <td></td>\n",
              "      <td>Burrows, E</td>\n",
              "      <td>10.1177/0163443718764807</td>\n",
              "      <td>MEDIA, Culture and Society</td>\n",
              "      <td>{Indigenous MEDIA producers' perspectives on o...</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>2018</td>\n",
              "      <td>56</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                ID                                           abstract acmid                                             author                            doi                                      outlet                                         title_full                                                url  year qualtrics_id wos_id ebsco_id  stage_1  stage_2  stage_3\n",
              "0     Bindu2018503  Online social networks have become immensely p...               Bindu, P V and Mishra, R and Thilagam, P S      10.1007/s10844-017-0494-z  Journal of Intelligent Information Systems       {Discovering spammer communities in TWITTER}  https://www.scopus.com/inward/record.uri?eid=2...  2018           12             NaN     True    False    False\n",
              "1    Moraga2018470  This article explores the ways Latinos‚Äîas audi...                                              Moraga, J E       10.1177/0193723518797030          Journal of Sport and Social Issues  {On ESPN Deportes: Latinos, Sport MEDIA, and t...  https://www.scopus.com/inward/record.uri?eid=2...  2018           22             NaN     True    False    False\n",
              "2  Lanosga20181676  This study of American investigative reporting...                                 Lanosga, G and Martin, J       10.1177/1464884916683555                                  JOURNALISm  {JOURNALISts, sources, and policy outcomes: In...  https://www.scopus.com/inward/record.uri?eid=2...  2018           47             NaN     True    False     True\n",
              "3    Warner2018720  In this study, we test the indirect and condit...        Warner, B R and Jennings, F J and Bramlett, J ...  10.1080/15205436.2018.1472283              Mass Communication and Society  {A MultiMEDIA Analysis of Persuasion in the 20...  https://www.scopus.com/inward/record.uri?eid=2...  2018           50             NaN     True    False    False\n",
              "4  Burrows20181117  Professional communicators produce a diverse r...                                               Burrows, E       10.1177/0163443718764807                  MEDIA, Culture and Society  {Indigenous MEDIA producers' perspectives on o...  https://www.scopus.com/inward/record.uri?eid=2...  2018           56             NaN     True    False    False"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "First few rows of dataset 2:\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>(internal) id</th>\n",
              "      <th>(source) id</th>\n",
              "      <th>abstract</th>\n",
              "      <th>title_full</th>\n",
              "      <th>journal</th>\n",
              "      <th>authors</th>\n",
              "      <th>tags</th>\n",
              "      <th>consensus</th>\n",
              "      <th>labeled_at...9</th>\n",
              "      <th>code</th>\n",
              "      <th>stage_1</th>\n",
              "      <th>stage_2</th>\n",
              "      <th>stage_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>33937314</td>\n",
              "      <td>175</td>\n",
              "      <td>There is a worry that serious forms of politic...</td>\n",
              "      <td>Is Context the Key? The (Non-)Differential Eff...</td>\n",
              "      <td>Polit. Commun.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>o</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>33937315</td>\n",
              "      <td>113</td>\n",
              "      <td>The electoral model of democracy holds the ide...</td>\n",
              "      <td>POLITICAL NEWS IN ONLINE AND PRINT NEWSPAPERS ...</td>\n",
              "      <td>Digit. Journal.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>o</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33937316</td>\n",
              "      <td>122</td>\n",
              "      <td>Machine learning is a field at the intersectio...</td>\n",
              "      <td>Machine Learning for Sociology</td>\n",
              "      <td>Annu. Rev. Sociol.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>o</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33937317</td>\n",
              "      <td>467</td>\n",
              "      <td>Research on digital glocalization has found th...</td>\n",
              "      <td>Improving Health in Low-Income Communities Wit...</td>\n",
              "      <td>J. Commun.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>o</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33937318</td>\n",
              "      <td>10</td>\n",
              "      <td>Political scientists often wish to classify do...</td>\n",
              "      <td>Using Word Order in Political Text Classificat...</td>\n",
              "      <td>Polit. Anal.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>o</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   (internal) id  (source) id                                           abstract                                         title_full             journal authors  tags consensus  labeled_at...9  code  stage_1  stage_2  stage_3\n",
              "0       33937314          175  There is a worry that serious forms of politic...  Is Context the Key? The (Non-)Differential Eff...      Polit. Commun.     NaN   NaN         o             NaN    -1     True    False    False\n",
              "1       33937315          113  The electoral model of democracy holds the ide...  POLITICAL NEWS IN ONLINE AND PRINT NEWSPAPERS ...     Digit. Journal.     NaN   NaN         o             NaN    -1     True    False    False\n",
              "2       33937316          122  Machine learning is a field at the intersectio...                     Machine Learning for Sociology  Annu. Rev. Sociol.     NaN   NaN         o             NaN    -1     True    False    False\n",
              "3       33937317          467  Research on digital glocalization has found th...  Improving Health in Low-Income Communities Wit...          J. Commun.     NaN   NaN         o             NaN    -1     True    False    False\n",
              "4       33937318           10  Political scientists often wish to classify do...  Using Word Order in Political Text Classificat...        Polit. Anal.     NaN   NaN         o             NaN    -1     True    False    False"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Data Import \n",
        "\n",
        "# Define the data paths for both datasets\n",
        "DATA_PATH_1 = \"../data/SSOT_manual_LB_20250808_120908.csv\" # ‚¨ÖÔ∏è Change this path if needed\n",
        "DATA_PATH_2 =  \"../data/SSOT_manual_BM_20250813_132621.csv\" # ‚¨ÖÔ∏è Change this path if needed\n",
        "\n",
        "# Load the first dataset (df1)\n",
        "try:\n",
        "    df_LB = pd.read_csv(DATA_PATH_1)\n",
        "    print(f\"‚úì First dataset loaded successfully\")\n",
        "    print(f\"‚úì Shape of dataset 1: {df_LB.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: The file LB dataset was not found in the data directory\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading the first dataset: {str(e)}\")\n",
        "\n",
        "# Load the second dataset (df2)\n",
        "try:\n",
        "    df_BM = pd.read_csv(DATA_PATH_2)\n",
        "    print(f\"\\n‚úì Second dataset loaded successfully\")\n",
        "    print(f\"‚úì Shape of dataset 2: {df_BM.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: The file df_BM was not found in the data directory\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading the second dataset: {str(e)}\")\n",
        "\n",
        "# Display basic information about both datasets\n",
        "print(\"\\nFirst few rows of dataset 1:\\n\")\n",
        "display(df_LB.head())\n",
        "\n",
        "print(\"\\nFirst few rows of dataset 2:\\n\")\n",
        "display(df_BM.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß´ Define Experiment Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ EXPERIMENT SETUP\n",
            "==================================================\n",
            "ID: 011\n",
            "Date: 2025-08-14\n",
            "Category: Testing\n",
            "üéØGoal: Test Set Up\n",
            "Model: gpt-4o (temp=0.0)\n",
            "==================================================\n",
            "‚úÖ Experiment configuration loaded\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Experiment Metadata\n",
        "EXPERIMENT_ID = \"011\"  # ‚¨ÖÔ∏è Change this for each new experiment\n",
        "EXPERIMENT_DATE = \"2025-08-14\"  # ‚¨ÖÔ∏è Update the date\n",
        "EXPERIMENT_CATEGORY = \"Testing\"  # ‚¨ÖÔ∏è Category of experiment\n",
        "EXPERIMENT_GOAL = \"Test Set Up\"  # ‚¨ÖÔ∏è What are you testing?\n",
        "\n",
        "# Model Configuration\n",
        "MODEL_NAME = \"gpt-4o\"\n",
        "TEMPERATURE = 0.0\n",
        "MAX_TOKENS = 4000\n",
        "\n",
        "# Print experiment info\n",
        "print(\"üß™ EXPERIMENT SETUP\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"ID: {EXPERIMENT_ID}\")\n",
        "print(f\"Date: {EXPERIMENT_DATE}\")\n",
        "print(f\"Category: {EXPERIMENT_CATEGORY}\")\n",
        "print(f\"üéØGoal: {EXPERIMENT_GOAL}\")\n",
        "print(f\"Model: {MODEL_NAME} (temp={TEMPERATURE})\")\n",
        "print(\"=\" * 50)\n",
        "print(\"‚úÖ Experiment configuration loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì£ Set up Basic API Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ OpenAI API Key loaded successfully.\n",
            "‚úÖ OpenAI client initialized.\n",
            "‚úÖ Enhanced screening function defined.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from datetime import datetime\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Get the API key from environment variables\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Validate API key\n",
        "if not api_key:\n",
        "    print(\"‚ö†Ô∏è  Error: OPENAI_API_KEY not found.\")\n",
        "    print(\"Please make sure you have a .env file with OPENAI_API_KEY='sk-...'\")\n",
        "else:\n",
        "    print(\"‚úÖ OpenAI API Key loaded successfully.\")\n",
        "    client = OpenAI(api_key=api_key)\n",
        "    print(\"‚úÖ OpenAI client initialized.\")\n",
        "\n",
        "# Enhanced analysis function for abstract screening\n",
        "def screen_abstract_llm(abstract_text, system_prompt, user_prompt_template, \n",
        "                       model=\"gpt-4o\", temperature=0.0):\n",
        "    \"\"\"\n",
        "    Screen an abstract using LLM with system and user prompts.\n",
        "    \n",
        "    Args:\n",
        "        abstract_text (str): The abstract to analyze\n",
        "        system_prompt (str): The system prompt defining the role\n",
        "        user_prompt_template (str): Template with {abstract} placeholder\n",
        "        model (str): The OpenAI model to use\n",
        "        temperature (float): Temperature setting for response randomness\n",
        "    \n",
        "    Returns:\n",
        "        dict: Result with decision, reasoning, and metadata\n",
        "    \"\"\"\n",
        "    if 'client' not in globals():\n",
        "        return {\"error\": \"OpenAI client is not initialized. Please check your API key.\"}\n",
        "\n",
        "    try:\n",
        "        # Insert abstract into user prompt template\n",
        "        user_prompt = user_prompt_template.format(abstract=abstract_text)\n",
        "        \n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "            max_tokens=4000\n",
        "        )\n",
        "        \n",
        "        if response and response.choices:\n",
        "            result = {\n",
        "                \"decision\": \"INCLUDE\" if \"INCLUDE\" in response.choices[0].message.content.upper() else \"EXCLUDE\",\n",
        "                \"reasoning\": response.choices[0].message.content,\n",
        "                \"model\": model,\n",
        "                \"temperature\": temperature,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"error\": None\n",
        "            }\n",
        "            return result\n",
        "        else:\n",
        "            return {\"error\": \"API Error: Empty or invalid response.\"}\n",
        "            \n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"API Error: {e}\"}\n",
        "\n",
        "print(\"‚úÖ Enhanced screening function defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèõÔ∏è Set Up System Prompt "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ System prompt defined\n",
            "üìã ID: SYS_001\n",
            "üìè Length: 759 characters\n",
            "üìÑ Description: Generic expert literature review screener for systematic reviews\n"
          ]
        }
      ],
      "source": [
        "# System prompt configuration\n",
        "# System prompt configuration\n",
        "SYSTEM_PROMPT_ID = \"SYS_001\"  # ‚¨ÖÔ∏è Change this ID for different system prompts\n",
        "SYSTEM_PROMPT_DESCRIPTION = \"Generic expert literature review screener for systematic reviews\"\n",
        "\n",
        "# Define the system prompt that sets the LLM's role\n",
        "SYSTEM_PROMPT = \"\"\"You are an expert in scientific literature review and systematic review methodology.\n",
        "\n",
        "Your task is to screen research abstracts and decide whether they should be INCLUDED or EXCLUDED from a systematic literature review based on provided criteria.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Carefully read the provided inclusion/exclusion criteria\n",
        "2. Review any example abstracts to understand the decision-making pattern\n",
        "3. Apply the criteria systematically to the given abstract and title\n",
        "4. Provide your decision in the exact format requested\n",
        "5. Base your reasoning strictly on the provided criteria\n",
        "\n",
        "Be consistent, objective, and systematic in your evaluation. Do not make up additional criteria beyond what is provided. Focus only on what is explicitly stated in the instructions.\"\"\"\n",
        "\n",
        "print(f\"‚úÖ System prompt defined\")\n",
        "print(f\"üìã ID: {SYSTEM_PROMPT_ID}\")\n",
        "print(f\"üìè Length: {len(SYSTEM_PROMPT)} characters\")\n",
        "print(f\"üìÑ Description: {SYSTEM_PROMPT_DESCRIPTION}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üë©üèª‚Äç‚öïÔ∏è Create User Prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ User prompt configuration and template loaded\n",
            "üìã ID: USR_001\n",
            "üìÑ Description: Basic CTAM screening with criteria and examples from CSV files\n",
            "üìÅ Criteria: ../prompts/Criteria_BM_01.csv\n",
            "üìÅ Examples: ../prompts/exmpl_single_BM_01.csv\n",
            "üéØ Output: Binary\n",
            "üî¨ Topic: Computational Text Analysis Methods | Domain: political_communication | Source: BM\n",
            "üìè Template length: 437 characters\n"
          ]
        }
      ],
      "source": [
        "# User prompt configuration\n",
        "USER_PROMPT_ID = \"USR_001\"  # ‚¨ÖÔ∏è Change this ID for different user prompts\n",
        "USER_PROMPT_DESCRIPTION = \"Basic CTAM screening with criteria and examples from CSV files\"\n",
        "\n",
        "# File paths for modular components\n",
        "CRITERIA_FILE = \"../prompts/Criteria_BM_01.csv\"  # ‚¨ÖÔ∏è Change criteria file here\n",
        "EXAMPLES_FILE = \"../prompts/exmpl_single_BM_01.csv\"  # ‚¨ÖÔ∏è Change examples file here (or set to None)\n",
        "\n",
        "# Output configuration\n",
        "OUTPUT_FORMAT = \"Binary\"  # ‚¨ÖÔ∏è Options: \"Binary\", \"Yes/Maybe/No\", \"Likert\n",
        "DECISION_OPTIONS = [\"INCLUDE\", \"EXCLUDE\"] # ‚¨ÖÔ∏è Change according to the output format\n",
        "\n",
        "# Additional metadata for results tracking\n",
        "DOMAIN = \"political_communication\" # ‚¨ÖÔ∏è Change this to the domain of the study\n",
        "TOPIC = \"Computational Text Analysis Methods\"  # ‚¨ÖÔ∏è Change this to the topic of the study\n",
        "DATASET_SOURCE = \"BM\"  # ‚¨ÖÔ∏è Which dataset (BM/LB)\n",
        "\n",
        "# Define the user prompt template with placeholders\n",
        "USER_PROMPT_TEMPLATE = \"\"\"## SCREENING TASK:\n",
        "You are screening abstracts for a systematic literature review on {topic} in {domain}.\n",
        "\n",
        "## INCLUSION/EXCLUSION CRITERIA:\n",
        "{criteria_text}\n",
        "\n",
        "{examples_section}\n",
        "\n",
        "## ABSTRACT TO SCREEN:\n",
        "**Title:** {title}\n",
        "**Abstract:** {abstract}\n",
        "\n",
        "## YOUR DECISION:\n",
        "Based strictly on the criteria above, provide your decision as either \"{decision_include}\" or \"{decision_exclude}\" followed by your reasoning:\n",
        "\n",
        "**Decision:** \n",
        "**Reasoning:** \"\"\"\n",
        "\n",
        "print(f\"‚úÖ User prompt configuration and template loaded\")\n",
        "print(f\"üìã ID: {USER_PROMPT_ID}\")\n",
        "print(f\"üìÑ Description: {USER_PROMPT_DESCRIPTION}\")\n",
        "print(f\"üìÅ Criteria: {CRITERIA_FILE}\")\n",
        "print(f\"üìÅ Examples: {EXAMPLES_FILE}\")\n",
        "print(f\"üéØ Output: {OUTPUT_FORMAT}\")\n",
        "print(f\"üî¨ Topic: {TOPIC} | Domain: {DOMAIN} | Source: {DATASET_SOURCE}\")\n",
        "print(f\"üìè Template length: {len(USER_PROMPT_TEMPLATE)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Valdiation Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç VALIDATION CHECK\n",
            "==================================================\n",
            "üìã Checking required variables:\n",
            "   ‚úÖ EXPERIMENT_ID: 011\n",
            "   ‚úÖ SYSTEM_PROMPT_ID: SYS_001\n",
            "   ‚úÖ USER_PROMPT_ID: USR_001\n",
            "   ‚úÖ SYSTEM_PROMPT: You are an expert in scientific literature review ...\n",
            "   ‚úÖ USER_PROMPT_TEMPLATE: ## SCREENING TASK:\n",
            "You are screening abstracts for...\n",
            "   ‚úÖ CRITERIA_FILE: ../prompts/Criteria_BM_01.csv\n",
            "   ‚úÖ EXAMPLES_FILE: ../prompts/exmpl_single_BM_01.csv\n",
            "   ‚úÖ DECISION_OPTIONS: ['INCLUDE', 'EXCLUDE']\n",
            "   ‚úÖ MODEL_NAME: gpt-4o\n",
            "   ‚úÖ TEMPERATURE: 0.0\n",
            "   ‚úÖ TOPIC: Computational Text Analysis Methods\n",
            "   ‚úÖ DOMAIN: political_communication\n",
            "\n",
            "üìä Checking DataFrame structure:\n",
            "   ‚úÖ DataFrame shape: (917, 13)\n",
            "   ‚úÖ Column 'abstract': Present\n",
            "   ‚úÖ Column 'title_full': Present\n",
            "   ‚úÖ Column 'stage_2': Present\n",
            "   ‚úÖ Column 'stage_3': Present\n",
            "\n",
            "üìà Checking data availability:\n",
            "   üìä Stage 2 True: 166\n",
            "   üìä Stage 2 False: 751\n",
            "   üìä Stage 3 True: 96\n",
            "   üìä Stage 3 False: 821\n",
            "\n",
            "üìÅ Checking file paths:\n",
            "   ‚úÖ Criteria file: ../prompts/Criteria_BM_01.csv\n",
            "   ‚úÖ Examples file: ../prompts/exmpl_single_BM_01.csv\n",
            "\n",
            "ü§ñ Checking API function:\n",
            "   ‚úÖ screen_abstract_llm function: Available\n",
            "\n",
            "==================================================\n",
            "‚úÖ ALL VALIDATIONS PASSED - Ready to run experiment!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ‚úÖ EXPERIMENT VALIDATION CHECK\n",
        "# =============================================================================\n",
        "\n",
        "def validate_experiment_setup(df, dataset_source=\"BM\"):\n",
        "    \"\"\"\n",
        "    Validate that all required variables and data are available for the experiment.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame to be used in experiment\n",
        "        dataset_source: Dataset identifier\n",
        "    \n",
        "    Returns:\n",
        "        bool: True if all validations pass, False otherwise\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"üîç VALIDATION CHECK\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    validation_passed = True\n",
        "    \n",
        "    # Check required global variables\n",
        "    required_vars = {\n",
        "        'EXPERIMENT_ID': globals().get('EXPERIMENT_ID'),\n",
        "        'SYSTEM_PROMPT_ID': globals().get('SYSTEM_PROMPT_ID'), \n",
        "        'USER_PROMPT_ID': globals().get('USER_PROMPT_ID'),\n",
        "        'SYSTEM_PROMPT': globals().get('SYSTEM_PROMPT'),\n",
        "        'USER_PROMPT_TEMPLATE': globals().get('USER_PROMPT_TEMPLATE'),\n",
        "        'CRITERIA_FILE': globals().get('CRITERIA_FILE'),\n",
        "        'EXAMPLES_FILE': globals().get('EXAMPLES_FILE'),\n",
        "        'DECISION_OPTIONS': globals().get('DECISION_OPTIONS'),\n",
        "        'MODEL_NAME': globals().get('MODEL_NAME'),\n",
        "        'TEMPERATURE': globals().get('TEMPERATURE'),\n",
        "        'TOPIC': globals().get('TOPIC'),  # Changed from METHOD\n",
        "        'DOMAIN': globals().get('DOMAIN')\n",
        "    }\n",
        "    \n",
        "    print(\"üìã Checking required variables:\")\n",
        "    for var_name, var_value in required_vars.items():\n",
        "        if var_value is None:\n",
        "            print(f\"   ‚ùå {var_name}: NOT DEFINED\")\n",
        "            validation_passed = False\n",
        "        else:\n",
        "            print(f\"   ‚úÖ {var_name}: {str(var_value)[:50]}{'...' if len(str(var_value)) > 50 else ''}\")\n",
        "    \n",
        "    # Check DataFrame structure\n",
        "    print(f\"\\nüìä Checking DataFrame structure:\")\n",
        "    required_columns = ['abstract', 'title_full', 'stage_2', 'stage_3']\n",
        "    \n",
        "    if df is None:\n",
        "        print(f\"   ‚ùå DataFrame is None\")\n",
        "        validation_passed = False\n",
        "    else:\n",
        "        print(f\"   ‚úÖ DataFrame shape: {df.shape}\")\n",
        "        \n",
        "        for col in required_columns:\n",
        "            if col in df.columns:\n",
        "                print(f\"   ‚úÖ Column '{col}': Present\")\n",
        "            else:\n",
        "                print(f\"   ‚ùå Column '{col}': MISSING\")\n",
        "                validation_passed = False\n",
        "    \n",
        "    # Check data availability\n",
        "    if df is not None and all(col in df.columns for col in required_columns):\n",
        "        print(f\"\\nüìà Checking data availability:\")\n",
        "        stage2_true = len(df[df['stage_2'] == True])\n",
        "        stage2_false = len(df[df['stage_2'] == False])\n",
        "        stage3_true = len(df[df['stage_3'] == True])\n",
        "        stage3_false = len(df[df['stage_3'] == False])\n",
        "        \n",
        "        print(f\"   üìä Stage 2 True: {stage2_true}\")\n",
        "        print(f\"   üìä Stage 2 False: {stage2_false}\")\n",
        "        print(f\"   üìä Stage 3 True: {stage3_true}\")\n",
        "        print(f\"   üìä Stage 3 False: {stage3_false}\")\n",
        "        \n",
        "        if stage3_true < 10:\n",
        "            print(f\"   ‚ö†Ô∏è  Warning: Only {stage3_true} stage_3=True examples available\")\n",
        "        if stage3_false < 10:\n",
        "            print(f\"   ‚ö†Ô∏è  Warning: Only {stage3_false} stage_3=False examples available\")\n",
        "    \n",
        "    # Check file paths\n",
        "    print(f\"\\nüìÅ Checking file paths:\")\n",
        "    import os\n",
        "    \n",
        "    if CRITERIA_FILE and os.path.exists(CRITERIA_FILE):\n",
        "        print(f\"   ‚úÖ Criteria file: {CRITERIA_FILE}\")\n",
        "    elif CRITERIA_FILE:\n",
        "        print(f\"   ‚ùå Criteria file: {CRITERIA_FILE} (NOT FOUND)\")\n",
        "        validation_passed = False\n",
        "    else:\n",
        "        print(f\"   ‚ùå Criteria file: NOT SPECIFIED\")\n",
        "        validation_passed = False\n",
        "    \n",
        "    if EXAMPLES_FILE:\n",
        "        if os.path.exists(EXAMPLES_FILE):\n",
        "            print(f\"   ‚úÖ Examples file: {EXAMPLES_FILE}\")\n",
        "        else:\n",
        "            print(f\"   ‚ùå Examples file: {EXAMPLES_FILE} (NOT FOUND)\")\n",
        "            validation_passed = False\n",
        "    else:\n",
        "        print(f\"   ‚ÑπÔ∏è  Examples file: None (will run without examples)\")\n",
        "    \n",
        "    # Check API function\n",
        "    print(f\"\\nü§ñ Checking API function:\")\n",
        "    if 'screen_abstract_llm' in globals():\n",
        "        print(f\"   ‚úÖ screen_abstract_llm function: Available\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå screen_abstract_llm function: NOT DEFINED\")\n",
        "        validation_passed = False\n",
        "    \n",
        "    # Final result\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    if validation_passed:\n",
        "        print(\"‚úÖ ALL VALIDATIONS PASSED - Ready to run experiment!\")\n",
        "    else:\n",
        "        print(\"‚ùå VALIDATION FAILED - Please fix the issues above before running\")\n",
        "    \n",
        "    return validation_passed\n",
        "\n",
        "# Run validation\n",
        "validation_result = validate_experiment_setup(df_BM, \"BM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî¨ Set Up Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Classification experiment function defined\n",
            "üöÄ Ready to run: run_classification_experiment(df_BM)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix \n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "def run_classification_experiment(\n",
        "    df, \n",
        "    n_total_examples=50,  # ‚¨ÖÔ∏è Total number of examples to test\n",
        "    n_stage3_true=5,     # ‚¨ÖÔ∏è Number of stage_3=True examples\n",
        "    n_stage3_false=45,    # ‚¨ÖÔ∏è Number of stage_3=False examples\n",
        "    dataset_source=\"BM\",  # ‚¨ÖÔ∏è Dataset identifier (LB/BM)\n",
        "    save_results=True,    # ‚¨ÖÔ∏è Whether to save results to CSV\n",
        "    verbose=True          # ‚¨ÖÔ∏è Print progress updates\n",
        "):\n",
        "    \"\"\"\n",
        "    Run LLM classification experiment on abstracts.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with abstracts (must have 'abstract', 'title_full', 'stage_2', 'stage_3')\n",
        "        n_total_examples: Total number of examples to test\n",
        "        n_stage3_true: Number of stage_3=True examples to include\n",
        "        n_stage3_false: Number of stage_3=False examples to include\n",
        "        dataset_source: Dataset identifier for results filename\n",
        "        save_results: Whether to save results to CSV\n",
        "        verbose: Whether to print progress\n",
        "    \n",
        "    Returns:\n",
        "        dict: Results including metrics and DataFrame\n",
        "    \"\"\"\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"üß™ Starting Classification Experiment\")\n",
        "        print(f\"üìä Dataset: {dataset_source}\")\n",
        "        print(f\"üéØ Total examples: {n_total_examples}\")\n",
        "        print(f\"‚úÖ Stage 3 True: {n_stage3_true}\")\n",
        "        print(f\"‚ùå Stage 3 False: {n_stage3_false}\")\n",
        "        print(\"=\" * 50)\n",
        "    \n",
        "    # Sample examples\n",
        "    stage3_true_samples = df[df['stage_3'] == True].sample(n=n_stage3_true, random_state=42)\n",
        "    stage3_false_samples = df[df['stage_3'] == False].sample(n=n_stage3_false, random_state=42)\n",
        "    \n",
        "    # Combine samples\n",
        "    test_samples = pd.concat([stage3_true_samples, stage3_false_samples]).reset_index(drop=True)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"üìù Sampled {len(test_samples)} examples\")\n",
        "    \n",
        "    # Load criteria and examples text\n",
        "    def load_criteria_text(criteria_file):\n",
        "        try:\n",
        "            criteria_df = pd.read_csv(criteria_file)\n",
        "            criteria_text = \"\"\n",
        "            \n",
        "            # Add inclusion criteria\n",
        "            inclusion_criteria = criteria_df[criteria_df['type'] == 'inclusion']\n",
        "            if len(inclusion_criteria) > 0:\n",
        "                criteria_text += \"**INCLUSION CRITERIA:**\\n\"\n",
        "                for _, row in inclusion_criteria.iterrows():\n",
        "                    criteria_text += f\"- **{row['criterion_id']}**: {row['description']}\\n\"\n",
        "                    if pd.notna(row['examples']) and row['examples'].strip():\n",
        "                        criteria_text += f\"  *Examples: {row['examples']}*\\n\"\n",
        "            \n",
        "            # Add exclusion criteria\n",
        "            exclusion_criteria = criteria_df[criteria_df['type'] == 'exclusion']\n",
        "            if len(exclusion_criteria) > 0:\n",
        "                criteria_text += \"\\n**EXCLUSION CRITERIA:**\\n\"\n",
        "                for _, row in exclusion_criteria.iterrows():\n",
        "                    criteria_text += f\"- **{row['criterion_id']}**: {row['description']}\\n\"\n",
        "                    if pd.notna(row['examples']) and row['examples'].strip():\n",
        "                        criteria_text += f\"  *Examples: {row['examples']}*\\n\"\n",
        "            \n",
        "            return criteria_text\n",
        "        except Exception as e:\n",
        "            return f\"Error loading criteria: {e}\"\n",
        "    \n",
        "    def load_examples_text(examples_file):\n",
        "        if not examples_file:\n",
        "            return \"\"\n",
        "        try:\n",
        "            examples_df = pd.read_csv(examples_file)\n",
        "            examples_text = \"\\n## EXAMPLE DECISIONS:\\n\"\n",
        "            \n",
        "            for _, row in examples_df.iterrows():\n",
        "                decision_label = \"INCLUDE\" if row['decision'].upper() == 'INCLUDE' else \"EXCLUDE\"\n",
        "                examples_text += f\"\\n**{decision_label} Example:**\\n\"\n",
        "                examples_text += f\"*Title:* {row['title']}\\n\"\n",
        "                examples_text += f\"*Abstract:* {row['abstract_text'][:200]}{'...' if len(row['abstract_text']) > 200 else ''}\\n\"\n",
        "                examples_text += f\"‚Üí **{decision_label}** ({row['reasoning']})\\n\"\n",
        "            \n",
        "            return examples_text\n",
        "        except Exception as e:\n",
        "            return f\"\\n## EXAMPLES:\\nError loading examples: {e}\\n\"\n",
        "    \n",
        "    # Load prompt components\n",
        "    criteria_text = load_criteria_text(CRITERIA_FILE)\n",
        "    examples_section = load_examples_text(EXAMPLES_FILE) if EXAMPLES_FILE else \"\"\n",
        "    \n",
        "    # Initialize results list\n",
        "    results_list = []\n",
        "    \n",
        "    # Process each example\n",
        "    for idx, row in test_samples.iterrows():\n",
        "        if verbose and (idx + 1) % 10 == 0:\n",
        "            print(f\"üîÑ Processing example {idx + 1}/{len(test_samples)}\")\n",
        "        \n",
        "        try:\n",
        "            # Create complete prompt\n",
        "            complete_prompt = USER_PROMPT_TEMPLATE.format(\n",
        "                topic=TOPIC,\n",
        "                domain=DOMAIN,\n",
        "                criteria_text=criteria_text,\n",
        "                examples_section=examples_section,\n",
        "                title=row['title_full'],\n",
        "                abstract=row['abstract'],\n",
        "                decision_include=DECISION_OPTIONS[0],\n",
        "                decision_exclude=DECISION_OPTIONS[1]\n",
        "            )\n",
        "            \n",
        "            # Call LLM\n",
        "            llm_result = screen_abstract_llm(\n",
        "                abstract_text=complete_prompt,\n",
        "                system_prompt=SYSTEM_PROMPT,\n",
        "                user_prompt_template=\"{abstract}\",  # Just pass through since we formatted above\n",
        "                model=MODEL_NAME,\n",
        "                temperature=TEMPERATURE\n",
        "            )\n",
        "            \n",
        "            # Parse LLM decision\n",
        "            llm_decision = llm_result.get('decision', 'UNKNOWN')\n",
        "            llm_reasoning = llm_result.get('reasoning', 'No reasoning provided')\n",
        "            \n",
        "            # Convert to binary for evaluation\n",
        "            llm_binary = 1 if llm_decision == 'INCLUDE' else 0\n",
        "            stage2_binary = 1 if row['stage_2'] else 0\n",
        "            stage3_binary = 1 if row['stage_3'] else 0\n",
        "            \n",
        "            # Store result\n",
        "            result_row = {\n",
        "                'example_id': idx + 1,\n",
        "                'title': row['title_full'],\n",
        "                'abstract': row['abstract'],\n",
        "                'stage_2_true': row['stage_2'],\n",
        "                'stage_3_true': row['stage_3'],\n",
        "                'stage_2_binary': stage2_binary,\n",
        "                'stage_3_binary': stage3_binary,\n",
        "                'llm_decision': llm_decision,\n",
        "                'llm_binary': llm_binary,\n",
        "                'llm_reasoning': llm_reasoning,\n",
        "                'experiment_id': EXPERIMENT_ID,\n",
        "                'dataset_source': dataset_source,\n",
        "                'system_prompt_id': SYSTEM_PROMPT_ID,\n",
        "                'user_prompt_id': USER_PROMPT_ID,\n",
        "                'model': MODEL_NAME,\n",
        "                'temperature': TEMPERATURE,\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "            \n",
        "            results_list.append(result_row)\n",
        "            \n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"‚ùå Error processing example {idx + 1}: {e}\")\n",
        "            \n",
        "            # Store error result\n",
        "            result_row = {\n",
        "                'example_id': idx + 1,\n",
        "                'title': row['title_full'],\n",
        "                'abstract': row['abstract'],\n",
        "                'stage_2_true': row['stage_2'],\n",
        "                'stage_3_true': row['stage_3'],\n",
        "                'stage_2_binary': 1 if row['stage_2'] else 0,\n",
        "                'stage_3_binary': 1 if row['stage_3'] else 0,\n",
        "                'llm_decision': 'ERROR',\n",
        "                'llm_binary': 0,\n",
        "                'llm_reasoning': f'Processing error: {e}',\n",
        "                'experiment_id': EXPERIMENT_ID,\n",
        "                'dataset_source': dataset_source,\n",
        "                'system_prompt_id': SYSTEM_PROMPT_ID,\n",
        "                'user_prompt_id': USER_PROMPT_ID,\n",
        "                'model': MODEL_NAME,\n",
        "                'temperature': TEMPERATURE,\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "            \n",
        "            results_list.append(result_row)\n",
        "    \n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame(results_list)\n",
        "    \n",
        "   # Calculate detailed metrics for stage_2\n",
        "    valid_results_stage2 = results_df[results_df['llm_decision'] != 'ERROR']\n",
        "    if len(valid_results_stage2) > 0:\n",
        "        y_true_stage2 = valid_results_stage2['stage_2_binary'].values\n",
        "        y_pred_stage2 = valid_results_stage2['llm_binary'].values\n",
        "        \n",
        "        # Basic metrics\n",
        "        accuracy_stage2 = accuracy_score(y_true_stage2, y_pred_stage2)\n",
        "        precision_stage2 = precision_score(y_true_stage2, y_pred_stage2, zero_division=0)\n",
        "        recall_stage2 = recall_score(y_true_stage2, y_pred_stage2, zero_division=0)\n",
        "        f1_stage2 = f1_score(y_true_stage2, y_pred_stage2, zero_division=0)\n",
        "        \n",
        "        # Confusion matrix metrics\n",
        "        tn2, fp2, fn2, tp2 = confusion_matrix(y_true_stage2, y_pred_stage2).ravel()\n",
        "    else:\n",
        "        accuracy_stage2 = precision_stage2 = recall_stage2 = f1_stage2 = 0.0\n",
        "        tp2 = fp2 = tn2 = fn2 = 0\n",
        "    \n",
        "    # Calculate detailed metrics for stage_3\n",
        "    valid_results_stage3 = results_df[results_df['llm_decision'] != 'ERROR']\n",
        "    if len(valid_results_stage3) > 0:\n",
        "        y_true_stage3 = valid_results_stage3['stage_3_binary'].values\n",
        "        y_pred_stage3 = valid_results_stage3['llm_binary'].values\n",
        "        \n",
        "        # Basic metrics\n",
        "        accuracy_stage3 = accuracy_score(y_true_stage3, y_pred_stage3)\n",
        "        precision_stage3 = precision_score(y_true_stage3, y_pred_stage3, zero_division=0)\n",
        "        recall_stage3 = recall_score(y_true_stage3, y_pred_stage3, zero_division=0)\n",
        "        f1_stage3 = f1_score(y_true_stage3, y_pred_stage3, zero_division=0)\n",
        "        \n",
        "        # Confusion matrix metrics\n",
        "        tn3, fp3, fn3, tp3 = confusion_matrix(y_true_stage3, y_pred_stage3).ravel()\n",
        "    else:\n",
        "        accuracy_stage3 = precision_stage3 = recall_stage3 = f1_stage3 = 0.0\n",
        "        tp3 = fp3 = tn3 = fn3 = 0\n",
        "    \n",
        "    # Updated metrics dictionary\n",
        "    metrics = {\n",
        "        'stage_2_metrics': {\n",
        "            'accuracy': accuracy_stage2,\n",
        "            'precision': precision_stage2,\n",
        "            'recall': recall_stage2,\n",
        "            'f1_score': f1_stage2,\n",
        "            'tp': int(tp2),\n",
        "            'fp': int(fp2),\n",
        "            'tn': int(tn2),\n",
        "            'fn': int(fn2)\n",
        "        },\n",
        "        'stage_3_metrics': {\n",
        "            'accuracy': accuracy_stage3,\n",
        "            'precision': precision_stage3,\n",
        "            'recall': recall_stage3,\n",
        "            'f1_score': f1_stage3,\n",
        "            'tp': int(tp3),\n",
        "            'fp': int(fp3),\n",
        "            'tn': int(tn3),\n",
        "            'fn': int(fn3)\n",
        "        },\n",
        "        'total_examples': len(results_df),\n",
        "        'successful_classifications': len(valid_results_stage2),\n",
        "        'errors': len(results_df) - len(valid_results_stage2)\n",
        "    }\n",
        "    \n",
        "    # Enhanced results printing\n",
        "    if verbose:\n",
        "        print(\"\\nüìä EXPERIMENT RESULTS\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"üìà Stage 2 Evaluation:\")\n",
        "        print(f\"   Accuracy:  {accuracy_stage2:.3f}\")\n",
        "        print(f\"   Precision: {precision_stage2:.3f}\")\n",
        "        print(f\"   Recall:    {recall_stage2:.3f}\")\n",
        "        print(f\"   F1 Score:  {f1_stage2:.3f}\")\n",
        "        print(f\"   TP: {tp2}, FP: {fp2}, TN: {tn2}, FN: {fn2}\")\n",
        "        print(f\"\\nüìà Stage 3 Evaluation:\")\n",
        "        print(f\"   Accuracy:  {accuracy_stage3:.3f}\")\n",
        "        print(f\"   Precision: {precision_stage3:.3f}\")\n",
        "        print(f\"   Recall:    {recall_stage3:.3f}\")\n",
        "        print(f\"   F1 Score:  {f1_stage3:.3f}\")\n",
        "        print(f\"   TP: {tp3}, FP: {fp3}, TN: {tn3}, FN: {fn3}\")\n",
        "        print(f\"\\nüìã Processing Summary:\")\n",
        "        print(f\"   Total examples: {len(results_df)}\")\n",
        "        print(f\"   Successful: {len(valid_results_stage2)}\")\n",
        "        print(f\"   Errors: {len(results_df) - len(valid_results_stage2)}\")\n",
        "    \n",
        "    # Save results\n",
        "    if save_results:\n",
        "        # Create filename with timestamp\n",
        "        timestamp = datetime.now().strftime(\"%m%d%H%M\")\n",
        "        filename = f\"{EXPERIMENT_ID}_{dataset_source}_{timestamp}.csv\"\n",
        "        results_dir = \"../results\"\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "        output_path = os.path.join(results_dir, filename)\n",
        "        \n",
        "        results_df.to_csv(output_path, index=False)\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"\\nüíæ Results saved to: {output_path}\")\n",
        "    \n",
        "    return {\n",
        "        'results_df': results_df,\n",
        "        'metrics': metrics,\n",
        "        'filename': filename if save_results else None\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Classification experiment function defined\")\n",
        "print(\"üöÄ Ready to run: run_classification_experiment(df_BM)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Run experiment! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Starting Classification Experiment\n",
            "üìä Dataset: BM\n",
            "üéØ Total examples: 50\n",
            "‚úÖ Stage 3 True: 5\n",
            "‚ùå Stage 3 False: 45\n",
            "==================================================\n",
            "üìù Sampled 50 examples\n",
            "üîÑ Processing example 10/50\n",
            "üîÑ Processing example 20/50\n",
            "üîÑ Processing example 30/50\n",
            "üîÑ Processing example 40/50\n",
            "üîÑ Processing example 50/50\n",
            "\n",
            "üìä EXPERIMENT RESULTS\n",
            "==================================================\n",
            "üìà Stage 2 Evaluation:\n",
            "   Accuracy:  0.840\n",
            "   Precision: 0.750\n",
            "   Recall:    0.300\n",
            "   F1 Score:  0.429\n",
            "   TP: 3, FP: 1, TN: 39, FN: 7\n",
            "\n",
            "üìà Stage 3 Evaluation:\n",
            "   Accuracy:  0.940\n",
            "   Precision: 0.750\n",
            "   Recall:    0.600\n",
            "   F1 Score:  0.667\n",
            "   TP: 3, FP: 1, TN: 44, FN: 2\n",
            "\n",
            "üìã Processing Summary:\n",
            "   Total examples: 50\n",
            "   Successful: 50\n",
            "   Errors: 0\n",
            "\n",
            "üíæ Results saved to: ../results/011_BM_08141215.csv\n"
          ]
        }
      ],
      "source": [
        "# Run experiment with default settings\n",
        "results = run_classification_experiment(df_BM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Results Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ûï Add experiment info to the results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_experiment_to_summary(results_dict, summary_file=\"../results/experiment_summary.csv\"):\n",
        "    \"\"\"Add new experiment results to the summary DataFrame with confusion matrix metrics\"\"\"\n",
        "    \n",
        "    new_row = pd.DataFrame({\n",
        "        'experiment_id': [EXPERIMENT_ID],\n",
        "        'experiment_date': [EXPERIMENT_DATE],\n",
        "        'experiment_category': [EXPERIMENT_CATEGORY],\n",
        "        'experiment_goal': [EXPERIMENT_GOAL],\n",
        "        'system_prompt_id': [SYSTEM_PROMPT_ID],\n",
        "        'user_prompt_id': [USER_PROMPT_ID],\n",
        "        'model_name': [MODEL_NAME],\n",
        "        'temperature': [TEMPERATURE],\n",
        "        'max_tokens': [MAX_TOKENS],\n",
        "        'criteria_file': [CRITERIA_FILE],\n",
        "        'examples_file': [EXAMPLES_FILE],\n",
        "        'output_format': [OUTPUT_FORMAT],\n",
        "        'domain': [DOMAIN],\n",
        "        'topic': [TOPIC],\n",
        "        'dataset_source': [DATASET_SOURCE],\n",
        "        'n_total_examples': [results_dict['metrics']['total_examples']],\n",
        "        'n_successful': [results_dict['metrics']['successful_classifications']],\n",
        "        'n_errors': [results_dict['metrics']['errors']],\n",
        "        # Stage 2 metrics\n",
        "        'stage2_accuracy': [results_dict['metrics']['stage_2_metrics']['accuracy']],\n",
        "        'stage2_precision': [results_dict['metrics']['stage_2_metrics']['precision']],\n",
        "        'stage2_recall': [results_dict['metrics']['stage_2_metrics']['recall']],\n",
        "        'stage2_f1': [results_dict['metrics']['stage_2_metrics']['f1_score']],\n",
        "        'stage2_tp': [results_dict['metrics']['stage_2_metrics']['tp']],\n",
        "        'stage2_fp': [results_dict['metrics']['stage_2_metrics']['fp']],\n",
        "        'stage2_tn': [results_dict['metrics']['stage_2_metrics']['tn']],\n",
        "        'stage2_fn': [results_dict['metrics']['stage_2_metrics']['fn']],\n",
        "        # Stage 3 metrics\n",
        "        'stage3_accuracy': [results_dict['metrics']['stage_3_metrics']['accuracy']],\n",
        "        'stage3_precision': [results_dict['metrics']['stage_3_metrics']['precision']],\n",
        "        'stage3_recall': [results_dict['metrics']['stage_3_metrics']['recall']],\n",
        "        'stage3_f1': [results_dict['metrics']['stage_3_metrics']['f1_score']],\n",
        "        'stage3_tp': [results_dict['metrics']['stage_3_metrics']['tp']],\n",
        "        'stage3_fp': [results_dict['metrics']['stage_3_metrics']['fp']],\n",
        "        'stage3_tn': [results_dict['metrics']['stage_3_metrics']['tn']],\n",
        "        'stage3_fn': [results_dict['metrics']['stage_3_metrics']['fn']],\n",
        "        'results_filename': [results_dict['filename']],\n",
        "        'timestamp': [datetime.now().isoformat()]\n",
        "    })\n",
        "    \n",
        "    # Load existing summary or create new one\n",
        "    if os.path.exists(summary_file):\n",
        "        existing_summary = pd.read_csv(summary_file)\n",
        "        updated_summary = pd.concat([existing_summary, new_row], ignore_index=True)\n",
        "    else:\n",
        "        updated_summary = new_row\n",
        "    \n",
        "    # Save updated summary\n",
        "    updated_summary.to_csv(summary_file, index=False)\n",
        "    print(f\"‚úÖ Experiment {EXPERIMENT_ID} added to summary: {summary_file}\")\n",
        "\n",
        "\n",
        "    \n",
        "    return updated_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Conclusions and Next Steps\n",
        "\n",
        "### Key Findings\n",
        "- \n",
        "\n",
        "### Next Steps\n",
        "- [Suggest follow-up experiments]\n",
        "- [List potential improvements]\n",
        "- [Identify areas for further investigation]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "SLRenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
