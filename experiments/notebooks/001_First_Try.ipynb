{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß™ SLR Abstract Screening Experiment\n",
        "#### Experiment Information\n",
        "- **ID**: 001\n",
        "- **Date**: 08/13\n",
        "#### üéØ Goal\n",
        "- This is the first try to test the set up! \n",
        "#### ‚öôÔ∏è Configuration\n",
        "- **LLM** : GPT-4o\n",
        "- **Data**: LB\n",
        "- **Examples** : Single\n",
        "- **Output**: Binary\n",
        "#### üìù Notes\n",
        "- \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîß Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Essential imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import json\n",
        "from pathlib import Path\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "# Configure pandas display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_theme()  # This is the correct way to set seaborn style\n",
        "plt.rcParams['figure.figsize'] = (12, 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì First dataset loaded successfully\n",
            "‚úì Shape of dataset 1: (3944, 15)\n",
            "\n",
            "‚úì Second dataset loaded successfully\n",
            "‚úì Shape of dataset 2: (917, 13)\n",
            "\n",
            "First few rows of dataset 1:\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>abstract</th>\n",
              "      <th>acmid</th>\n",
              "      <th>author</th>\n",
              "      <th>doi</th>\n",
              "      <th>outlet</th>\n",
              "      <th>title_full</th>\n",
              "      <th>url</th>\n",
              "      <th>year</th>\n",
              "      <th>qualtrics_id</th>\n",
              "      <th>wos_id</th>\n",
              "      <th>ebsco_id</th>\n",
              "      <th>stage_1</th>\n",
              "      <th>stage_2</th>\n",
              "      <th>stage_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bindu2018503</td>\n",
              "      <td>Online social networks have become immensely p...</td>\n",
              "      <td></td>\n",
              "      <td>Bindu, P V and Mishra, R and Thilagam, P S</td>\n",
              "      <td>10.1007/s10844-017-0494-z</td>\n",
              "      <td>Journal of Intelligent Information Systems</td>\n",
              "      <td>{Discovering spammer communities in TWITTER}</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>2018</td>\n",
              "      <td>12</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Moraga2018470</td>\n",
              "      <td>This article explores the ways Latinos‚Äîas audi...</td>\n",
              "      <td></td>\n",
              "      <td>Moraga, J E</td>\n",
              "      <td>10.1177/0193723518797030</td>\n",
              "      <td>Journal of Sport and Social Issues</td>\n",
              "      <td>{On ESPN Deportes: Latinos, Sport MEDIA, and t...</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>2018</td>\n",
              "      <td>22</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Lanosga20181676</td>\n",
              "      <td>This study of American investigative reporting...</td>\n",
              "      <td></td>\n",
              "      <td>Lanosga, G and Martin, J</td>\n",
              "      <td>10.1177/1464884916683555</td>\n",
              "      <td>JOURNALISm</td>\n",
              "      <td>{JOURNALISts, sources, and policy outcomes: In...</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>2018</td>\n",
              "      <td>47</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Warner2018720</td>\n",
              "      <td>In this study, we test the indirect and condit...</td>\n",
              "      <td></td>\n",
              "      <td>Warner, B R and Jennings, F J and Bramlett, J ...</td>\n",
              "      <td>10.1080/15205436.2018.1472283</td>\n",
              "      <td>Mass Communication and Society</td>\n",
              "      <td>{A MultiMEDIA Analysis of Persuasion in the 20...</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>2018</td>\n",
              "      <td>50</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Burrows20181117</td>\n",
              "      <td>Professional communicators produce a diverse r...</td>\n",
              "      <td></td>\n",
              "      <td>Burrows, E</td>\n",
              "      <td>10.1177/0163443718764807</td>\n",
              "      <td>MEDIA, Culture and Society</td>\n",
              "      <td>{Indigenous MEDIA producers' perspectives on o...</td>\n",
              "      <td>https://www.scopus.com/inward/record.uri?eid=2...</td>\n",
              "      <td>2018</td>\n",
              "      <td>56</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                ID                                           abstract acmid                                             author                            doi                                      outlet                                         title_full                                                url  year qualtrics_id wos_id ebsco_id  stage_1  stage_2  stage_3\n",
              "0     Bindu2018503  Online social networks have become immensely p...               Bindu, P V and Mishra, R and Thilagam, P S      10.1007/s10844-017-0494-z  Journal of Intelligent Information Systems       {Discovering spammer communities in TWITTER}  https://www.scopus.com/inward/record.uri?eid=2...  2018           12             NaN     True    False    False\n",
              "1    Moraga2018470  This article explores the ways Latinos‚Äîas audi...                                              Moraga, J E       10.1177/0193723518797030          Journal of Sport and Social Issues  {On ESPN Deportes: Latinos, Sport MEDIA, and t...  https://www.scopus.com/inward/record.uri?eid=2...  2018           22             NaN     True    False    False\n",
              "2  Lanosga20181676  This study of American investigative reporting...                                 Lanosga, G and Martin, J       10.1177/1464884916683555                                  JOURNALISm  {JOURNALISts, sources, and policy outcomes: In...  https://www.scopus.com/inward/record.uri?eid=2...  2018           47             NaN     True    False     True\n",
              "3    Warner2018720  In this study, we test the indirect and condit...        Warner, B R and Jennings, F J and Bramlett, J ...  10.1080/15205436.2018.1472283              Mass Communication and Society  {A MultiMEDIA Analysis of Persuasion in the 20...  https://www.scopus.com/inward/record.uri?eid=2...  2018           50             NaN     True    False    False\n",
              "4  Burrows20181117  Professional communicators produce a diverse r...                                               Burrows, E       10.1177/0163443718764807                  MEDIA, Culture and Society  {Indigenous MEDIA producers' perspectives on o...  https://www.scopus.com/inward/record.uri?eid=2...  2018           56             NaN     True    False    False"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "First few rows of dataset 2:\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>(internal) id</th>\n",
              "      <th>(source) id</th>\n",
              "      <th>abstract</th>\n",
              "      <th>title_full</th>\n",
              "      <th>journal</th>\n",
              "      <th>authors</th>\n",
              "      <th>tags</th>\n",
              "      <th>consensus</th>\n",
              "      <th>labeled_at...9</th>\n",
              "      <th>code</th>\n",
              "      <th>stage_1</th>\n",
              "      <th>stage_2</th>\n",
              "      <th>stage_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>33937314</td>\n",
              "      <td>175</td>\n",
              "      <td>There is a worry that serious forms of politic...</td>\n",
              "      <td>Is Context the Key? The (Non-)Differential Eff...</td>\n",
              "      <td>Polit. Commun.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>o</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>33937315</td>\n",
              "      <td>113</td>\n",
              "      <td>The electoral model of democracy holds the ide...</td>\n",
              "      <td>POLITICAL NEWS IN ONLINE AND PRINT NEWSPAPERS ...</td>\n",
              "      <td>Digit. Journal.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>o</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33937316</td>\n",
              "      <td>122</td>\n",
              "      <td>Machine learning is a field at the intersectio...</td>\n",
              "      <td>Machine Learning for Sociology</td>\n",
              "      <td>Annu. Rev. Sociol.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>o</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33937317</td>\n",
              "      <td>467</td>\n",
              "      <td>Research on digital glocalization has found th...</td>\n",
              "      <td>Improving Health in Low-Income Communities Wit...</td>\n",
              "      <td>J. Commun.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>o</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33937318</td>\n",
              "      <td>10</td>\n",
              "      <td>Political scientists often wish to classify do...</td>\n",
              "      <td>Using Word Order in Political Text Classificat...</td>\n",
              "      <td>Polit. Anal.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>o</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   (internal) id  (source) id                                           abstract                                         title_full             journal authors  tags consensus  labeled_at...9  code  stage_1  stage_2  stage_3\n",
              "0       33937314          175  There is a worry that serious forms of politic...  Is Context the Key? The (Non-)Differential Eff...      Polit. Commun.     NaN   NaN         o             NaN    -1     True    False    False\n",
              "1       33937315          113  The electoral model of democracy holds the ide...  POLITICAL NEWS IN ONLINE AND PRINT NEWSPAPERS ...     Digit. Journal.     NaN   NaN         o             NaN    -1     True    False    False\n",
              "2       33937316          122  Machine learning is a field at the intersectio...                     Machine Learning for Sociology  Annu. Rev. Sociol.     NaN   NaN         o             NaN    -1     True    False    False\n",
              "3       33937317          467  Research on digital glocalization has found th...  Improving Health in Low-Income Communities Wit...          J. Commun.     NaN   NaN         o             NaN    -1     True    False    False\n",
              "4       33937318           10  Political scientists often wish to classify do...  Using Word Order in Political Text Classificat...        Polit. Anal.     NaN   NaN         o             NaN    -1     True    False    False"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Data Import \n",
        "\n",
        "# Define the data paths for both datasets\n",
        "DATA_PATH_1 = \"../data/SSOT_manual_LB_20250808_120908.csv\" # ‚¨ÖÔ∏è Change this path if needed\n",
        "DATA_PATH_2 =  \"../data/SSOT_manual_BM_20250813_132621.csv\" # ‚¨ÖÔ∏è Change this path if needed\n",
        "\n",
        "# Load the first dataset (df1)\n",
        "try:\n",
        "    df_LB = pd.read_csv(DATA_PATH_1)\n",
        "    print(f\"‚úì First dataset loaded successfully\")\n",
        "    print(f\"‚úì Shape of dataset 1: {df_LB.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: The file LB dataset was not found in the data directory\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading the first dataset: {str(e)}\")\n",
        "\n",
        "# Load the second dataset (df2)\n",
        "try:\n",
        "    df_BM = pd.read_csv(DATA_PATH_2)\n",
        "    print(f\"\\n‚úì Second dataset loaded successfully\")\n",
        "    print(f\"‚úì Shape of dataset 2: {df_BM.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: The file df_BM was not found in the data directory\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading the second dataset: {str(e)}\")\n",
        "\n",
        "# Display basic information about both datasets\n",
        "print(\"\\nFirst few rows of dataset 1:\\n\")\n",
        "display(df_LB.head())\n",
        "\n",
        "print(\"\\nFirst few rows of dataset 2:\\n\")\n",
        "display(df_BM.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß´ Define Experiment Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ EXPERIMENT SETUP\n",
            "==================================================\n",
            "ID: 001\n",
            "Date: 2025-08-13\n",
            "Category: Testing\n",
            "üéØGoal: Test Set Up\n",
            "Model: gpt-4o (temp=0.0)\n",
            "==================================================\n",
            "‚úÖ Experiment configuration loaded\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Experiment Metadata\n",
        "EXPERIMENT_ID = \"001\"  # ‚¨ÖÔ∏è Change this for each new experiment\n",
        "EXPERIMENT_DATE = \"2025-08-13\"  # ‚¨ÖÔ∏è Update the date\n",
        "EXPERIMENT_CATEGORY = \"Testing\"  # ‚¨ÖÔ∏è Category of experiment\n",
        "EXPERIMENT_GOAL = \"Test Set Up\"  # ‚¨ÖÔ∏è What are you testing?\n",
        "\n",
        "# Model Configuration\n",
        "MODEL_NAME = \"gpt-4o\"\n",
        "TEMPERATURE = 0.0\n",
        "MAX_TOKENS = 4000\n",
        "\n",
        "# Print experiment info\n",
        "print(\"üß™ EXPERIMENT SETUP\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"ID: {EXPERIMENT_ID}\")\n",
        "print(f\"Date: {EXPERIMENT_DATE}\")\n",
        "print(f\"Category: {EXPERIMENT_CATEGORY}\")\n",
        "print(f\"üéØGoal: {EXPERIMENT_GOAL}\")\n",
        "print(f\"Model: {MODEL_NAME} (temp={TEMPERATURE})\")\n",
        "print(\"=\" * 50)\n",
        "print(\"‚úÖ Experiment configuration loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì£ Set up Basic API Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ OpenAI API Key loaded successfully.\n",
            "‚úÖ OpenAI client initialized.\n",
            "‚úÖ Enhanced screening function defined.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from datetime import datetime\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Get the API key from environment variables\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Validate API key\n",
        "if not api_key:\n",
        "    print(\"‚ö†Ô∏è  Error: OPENAI_API_KEY not found.\")\n",
        "    print(\"Please make sure you have a .env file with OPENAI_API_KEY='sk-...'\")\n",
        "else:\n",
        "    print(\"‚úÖ OpenAI API Key loaded successfully.\")\n",
        "    client = OpenAI(api_key=api_key)\n",
        "    print(\"‚úÖ OpenAI client initialized.\")\n",
        "\n",
        "# Enhanced analysis function for abstract screening\n",
        "def screen_abstract_llm(abstract_text, system_prompt, user_prompt_template, \n",
        "                       model=\"gpt-4o\", temperature=0.0):\n",
        "    \"\"\"\n",
        "    Screen an abstract using LLM with system and user prompts.\n",
        "    \n",
        "    Args:\n",
        "        abstract_text (str): The abstract to analyze\n",
        "        system_prompt (str): The system prompt defining the role\n",
        "        user_prompt_template (str): Template with {abstract} placeholder\n",
        "        model (str): The OpenAI model to use\n",
        "        temperature (float): Temperature setting for response randomness\n",
        "    \n",
        "    Returns:\n",
        "        dict: Result with decision, reasoning, and metadata\n",
        "    \"\"\"\n",
        "    if 'client' not in globals():\n",
        "        return {\"error\": \"OpenAI client is not initialized. Please check your API key.\"}\n",
        "\n",
        "    try:\n",
        "        # Insert abstract into user prompt template\n",
        "        user_prompt = user_prompt_template.format(abstract=abstract_text)\n",
        "        \n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "            max_tokens=4000\n",
        "        )\n",
        "        \n",
        "        if response and response.choices:\n",
        "            result = {\n",
        "                \"decision\": \"INCLUDE\" if \"INCLUDE\" in response.choices[0].message.content.upper() else \"EXCLUDE\",\n",
        "                \"reasoning\": response.choices[0].message.content,\n",
        "                \"model\": model,\n",
        "                \"temperature\": temperature,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"error\": None\n",
        "            }\n",
        "            return result\n",
        "        else:\n",
        "            return {\"error\": \"API Error: Empty or invalid response.\"}\n",
        "            \n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"API Error: {e}\"}\n",
        "\n",
        "print(\"‚úÖ Enhanced screening function defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèõÔ∏è Set Up System Prompt "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ System prompt defined\n",
            "üìã ID: SYS_001\n",
            "üìè Length: 759 characters\n",
            "üìÑ Description: Generic expert literature review screener for systematic reviews\n"
          ]
        }
      ],
      "source": [
        "# System prompt configuration\n",
        "# System prompt configuration\n",
        "SYSTEM_PROMPT_ID = \"SYS_001\"  # ‚¨ÖÔ∏è Change this ID for different system prompts\n",
        "SYSTEM_PROMPT_DESCRIPTION = \"Generic expert literature review screener for systematic reviews\"\n",
        "\n",
        "# Define the system prompt that sets the LLM's role\n",
        "SYSTEM_PROMPT = \"\"\"You are an expert in scientific literature review and systematic review methodology.\n",
        "\n",
        "Your task is to screen research abstracts and decide whether they should be INCLUDED or EXCLUDED from a systematic literature review based on provided criteria.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Carefully read the provided inclusion/exclusion criteria\n",
        "2. Review any example abstracts to understand the decision-making pattern\n",
        "3. Apply the criteria systematically to the given abstract and title\n",
        "4. Provide your decision in the exact format requested\n",
        "5. Base your reasoning strictly on the provided criteria\n",
        "\n",
        "Be consistent, objective, and systematic in your evaluation. Do not make up additional criteria beyond what is provided. Focus only on what is explicitly stated in the instructions.\"\"\"\n",
        "\n",
        "print(f\"‚úÖ System prompt defined\")\n",
        "print(f\"üìã ID: {SYSTEM_PROMPT_ID}\")\n",
        "print(f\"üìè Length: {len(SYSTEM_PROMPT)} characters\")\n",
        "print(f\"üìÑ Description: {SYSTEM_PROMPT_DESCRIPTION}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üë©üèª‚Äç‚öïÔ∏è Create User Prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ User prompt configuration and template loaded\n",
            "üìã ID: USR_001\n",
            "üìÑ Description: Basic CTAM screening with criteria and examples from CSV files\n",
            "üìÅ Criteria: ../prompts/Criteria_LB_01.csv\n",
            "üìÅ Examples: ../prompts/exmpl_single_LB_01.csv\n",
            "üéØ Output: Binary\n",
            "üî¨ Topic: media_dviersity | Domain: political_communication | Source: LB\n",
            "üìè Template length: 437 characters\n"
          ]
        }
      ],
      "source": [
        "# User prompt configuration\n",
        "USER_PROMPT_ID = \"USR_001\"  # ‚¨ÖÔ∏è Change this ID for different user prompts\n",
        "USER_PROMPT_DESCRIPTION = \"Basic CTAM screening with criteria and examples from CSV files\"\n",
        "\n",
        "# File paths for modular components\n",
        "CRITERIA_FILE = \"../prompts/Criteria_LB_01.csv\"  # ‚¨ÖÔ∏è Change criteria file here\n",
        "EXAMPLES_FILE = \"../prompts/exmpl_single_LB_01.csv\"  # ‚¨ÖÔ∏è Change examples file here (or set to None)\n",
        "\n",
        "# Output configuration\n",
        "OUTPUT_FORMAT = \"Binary\"  # ‚¨ÖÔ∏è Options: \"Binary\", \"Yes/Maybe/No\", \"Likert\n",
        "DECISION_OPTIONS = [\"INCLUDE\", \"EXCLUDE\"] # ‚¨ÖÔ∏è Change according to the output format\n",
        "\n",
        "# Additional metadata for results tracking\n",
        "DOMAIN = \"political_communication\"\n",
        "TOPIC = \"media_dviersity\"\n",
        "DATASET_SOURCE = \"LB\"  # ‚¨ÖÔ∏è Which dataset (BM/LB)\n",
        "\n",
        "# Define the user prompt template with placeholders\n",
        "USER_PROMPT_TEMPLATE = \"\"\"## SCREENING TASK:\n",
        "You are screening abstracts for a systematic literature review on {topic} in {domain}.\n",
        "\n",
        "## INCLUSION/EXCLUSION CRITERIA:\n",
        "{criteria_text}\n",
        "\n",
        "{examples_section}\n",
        "\n",
        "## ABSTRACT TO SCREEN:\n",
        "**Title:** {title}\n",
        "**Abstract:** {abstract}\n",
        "\n",
        "## YOUR DECISION:\n",
        "Based strictly on the criteria above, provide your decision as either \"{decision_include}\" or \"{decision_exclude}\" followed by your reasoning:\n",
        "\n",
        "**Decision:** \n",
        "**Reasoning:** \"\"\"\n",
        "\n",
        "print(f\"‚úÖ User prompt configuration and template loaded\")\n",
        "print(f\"üìã ID: {USER_PROMPT_ID}\")\n",
        "print(f\"üìÑ Description: {USER_PROMPT_DESCRIPTION}\")\n",
        "print(f\"üìÅ Criteria: {CRITERIA_FILE}\")\n",
        "print(f\"üìÅ Examples: {EXAMPLES_FILE}\")\n",
        "print(f\"üéØ Output: {OUTPUT_FORMAT}\")\n",
        "print(f\"üî¨ Topic: {TOPIC} | Domain: {DOMAIN} | Source: {DATASET_SOURCE}\")\n",
        "print(f\"üìè Template length: {len(USER_PROMPT_TEMPLATE)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Valdiation Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç VALIDATION CHECK\n",
            "==================================================\n",
            "üìã Checking required variables:\n",
            "   ‚úÖ EXPERIMENT_ID: 001\n",
            "   ‚úÖ SYSTEM_PROMPT_ID: SYS_001\n",
            "   ‚úÖ USER_PROMPT_ID: USR_001\n",
            "   ‚úÖ SYSTEM_PROMPT: You are an expert in scientific literature review ...\n",
            "   ‚úÖ USER_PROMPT_TEMPLATE: ## SCREENING TASK:\n",
            "You are screening abstracts for...\n",
            "   ‚úÖ CRITERIA_FILE: ../prompts/Criteria_LB_01.csv\n",
            "   ‚úÖ EXAMPLES_FILE: ../prompts/exmpl_single_LB_01.csv\n",
            "   ‚úÖ DECISION_OPTIONS: ['INCLUDE', 'EXCLUDE']\n",
            "   ‚úÖ MODEL_NAME: gpt-4o\n",
            "   ‚úÖ TEMPERATURE: 0.0\n",
            "   ‚úÖ TOPIC: media_dviersity\n",
            "   ‚úÖ DOMAIN: political_communication\n",
            "\n",
            "üìä Checking DataFrame structure:\n",
            "   ‚úÖ DataFrame shape: (3944, 15)\n",
            "   ‚úÖ Column 'abstract': Present\n",
            "   ‚úÖ Column 'title_full': Present\n",
            "   ‚úÖ Column 'stage_2': Present\n",
            "   ‚úÖ Column 'stage_3': Present\n",
            "\n",
            "üìà Checking data availability:\n",
            "   üìä Stage 2 True: 277\n",
            "   üìä Stage 2 False: 3667\n",
            "   üìä Stage 3 True: 207\n",
            "   üìä Stage 3 False: 3737\n",
            "\n",
            "üìÅ Checking file paths:\n",
            "   ‚úÖ Criteria file: ../prompts/Criteria_LB_01.csv\n",
            "   ‚úÖ Examples file: ../prompts/exmpl_single_LB_01.csv\n",
            "\n",
            "ü§ñ Checking API function:\n",
            "   ‚úÖ screen_abstract_llm function: Available\n",
            "\n",
            "==================================================\n",
            "‚úÖ ALL VALIDATIONS PASSED - Ready to run experiment!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ‚úÖ EXPERIMENT VALIDATION CHECK\n",
        "# =============================================================================\n",
        "\n",
        "def validate_experiment_setup(df, dataset_source=\"LB\"):\n",
        "    \"\"\"\n",
        "    Validate that all required variables and data are available for the experiment.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame to be used in experiment\n",
        "        dataset_source: Dataset identifier\n",
        "    \n",
        "    Returns:\n",
        "        bool: True if all validations pass, False otherwise\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"üîç VALIDATION CHECK\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    validation_passed = True\n",
        "    \n",
        "    # Check required global variables\n",
        "    required_vars = {\n",
        "        'EXPERIMENT_ID': globals().get('EXPERIMENT_ID'),\n",
        "        'SYSTEM_PROMPT_ID': globals().get('SYSTEM_PROMPT_ID'), \n",
        "        'USER_PROMPT_ID': globals().get('USER_PROMPT_ID'),\n",
        "        'SYSTEM_PROMPT': globals().get('SYSTEM_PROMPT'),\n",
        "        'USER_PROMPT_TEMPLATE': globals().get('USER_PROMPT_TEMPLATE'),\n",
        "        'CRITERIA_FILE': globals().get('CRITERIA_FILE'),\n",
        "        'EXAMPLES_FILE': globals().get('EXAMPLES_FILE'),\n",
        "        'DECISION_OPTIONS': globals().get('DECISION_OPTIONS'),\n",
        "        'MODEL_NAME': globals().get('MODEL_NAME'),\n",
        "        'TEMPERATURE': globals().get('TEMPERATURE'),\n",
        "        'TOPIC': globals().get('TOPIC'),  # Changed from METHOD\n",
        "        'DOMAIN': globals().get('DOMAIN')\n",
        "    }\n",
        "    \n",
        "    print(\"üìã Checking required variables:\")\n",
        "    for var_name, var_value in required_vars.items():\n",
        "        if var_value is None:\n",
        "            print(f\"   ‚ùå {var_name}: NOT DEFINED\")\n",
        "            validation_passed = False\n",
        "        else:\n",
        "            print(f\"   ‚úÖ {var_name}: {str(var_value)[:50]}{'...' if len(str(var_value)) > 50 else ''}\")\n",
        "    \n",
        "    # Check DataFrame structure\n",
        "    print(f\"\\nüìä Checking DataFrame structure:\")\n",
        "    required_columns = ['abstract', 'title_full', 'stage_2', 'stage_3']\n",
        "    \n",
        "    if df is None:\n",
        "        print(f\"   ‚ùå DataFrame is None\")\n",
        "        validation_passed = False\n",
        "    else:\n",
        "        print(f\"   ‚úÖ DataFrame shape: {df.shape}\")\n",
        "        \n",
        "        for col in required_columns:\n",
        "            if col in df.columns:\n",
        "                print(f\"   ‚úÖ Column '{col}': Present\")\n",
        "            else:\n",
        "                print(f\"   ‚ùå Column '{col}': MISSING\")\n",
        "                validation_passed = False\n",
        "    \n",
        "    # Check data availability\n",
        "    if df is not None and all(col in df.columns for col in required_columns):\n",
        "        print(f\"\\nüìà Checking data availability:\")\n",
        "        stage2_true = len(df[df['stage_2'] == True])\n",
        "        stage2_false = len(df[df['stage_2'] == False])\n",
        "        stage3_true = len(df[df['stage_3'] == True])\n",
        "        stage3_false = len(df[df['stage_3'] == False])\n",
        "        \n",
        "        print(f\"   üìä Stage 2 True: {stage2_true}\")\n",
        "        print(f\"   üìä Stage 2 False: {stage2_false}\")\n",
        "        print(f\"   üìä Stage 3 True: {stage3_true}\")\n",
        "        print(f\"   üìä Stage 3 False: {stage3_false}\")\n",
        "        \n",
        "        if stage3_true < 10:\n",
        "            print(f\"   ‚ö†Ô∏è  Warning: Only {stage3_true} stage_3=True examples available\")\n",
        "        if stage3_false < 10:\n",
        "            print(f\"   ‚ö†Ô∏è  Warning: Only {stage3_false} stage_3=False examples available\")\n",
        "    \n",
        "    # Check file paths\n",
        "    print(f\"\\nüìÅ Checking file paths:\")\n",
        "    import os\n",
        "    \n",
        "    if CRITERIA_FILE and os.path.exists(CRITERIA_FILE):\n",
        "        print(f\"   ‚úÖ Criteria file: {CRITERIA_FILE}\")\n",
        "    elif CRITERIA_FILE:\n",
        "        print(f\"   ‚ùå Criteria file: {CRITERIA_FILE} (NOT FOUND)\")\n",
        "        validation_passed = False\n",
        "    else:\n",
        "        print(f\"   ‚ùå Criteria file: NOT SPECIFIED\")\n",
        "        validation_passed = False\n",
        "    \n",
        "    if EXAMPLES_FILE:\n",
        "        if os.path.exists(EXAMPLES_FILE):\n",
        "            print(f\"   ‚úÖ Examples file: {EXAMPLES_FILE}\")\n",
        "        else:\n",
        "            print(f\"   ‚ùå Examples file: {EXAMPLES_FILE} (NOT FOUND)\")\n",
        "            validation_passed = False\n",
        "    else:\n",
        "        print(f\"   ‚ÑπÔ∏è  Examples file: None (will run without examples)\")\n",
        "    \n",
        "    # Check API function\n",
        "    print(f\"\\nü§ñ Checking API function:\")\n",
        "    if 'screen_abstract_llm' in globals():\n",
        "        print(f\"   ‚úÖ screen_abstract_llm function: Available\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå screen_abstract_llm function: NOT DEFINED\")\n",
        "        validation_passed = False\n",
        "    \n",
        "    # Final result\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    if validation_passed:\n",
        "        print(\"‚úÖ ALL VALIDATIONS PASSED - Ready to run experiment!\")\n",
        "    else:\n",
        "        print(\"‚ùå VALIDATION FAILED - Please fix the issues above before running\")\n",
        "    \n",
        "    return validation_passed\n",
        "\n",
        "# Run validation\n",
        "validation_result = validate_experiment_setup(df_LB, \"LB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî¨ Set Up Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Classification experiment function defined\n",
            "üöÄ Ready to run: run_classification_experiment(df_LB)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix \n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "def run_classification_experiment(\n",
        "    df, \n",
        "    n_total_examples=50,  # ‚¨ÖÔ∏è Total number of examples to test\n",
        "    n_stage3_true=5,     # ‚¨ÖÔ∏è Number of stage_3=True examples\n",
        "    n_stage3_false=45,    # ‚¨ÖÔ∏è Number of stage_3=False examples\n",
        "    dataset_source=\"LB\",  # ‚¨ÖÔ∏è Dataset identifier (LB/BM)\n",
        "    save_results=True,    # ‚¨ÖÔ∏è Whether to save results to CSV\n",
        "    verbose=True          # ‚¨ÖÔ∏è Print progress updates\n",
        "):\n",
        "    \"\"\"\n",
        "    Run LLM classification experiment on abstracts.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with abstracts (must have 'abstract', 'title_full', 'stage_2', 'stage_3')\n",
        "        n_total_examples: Total number of examples to test\n",
        "        n_stage3_true: Number of stage_3=True examples to include\n",
        "        n_stage3_false: Number of stage_3=False examples to include\n",
        "        dataset_source: Dataset identifier for results filename\n",
        "        save_results: Whether to save results to CSV\n",
        "        verbose: Whether to print progress\n",
        "    \n",
        "    Returns:\n",
        "        dict: Results including metrics and DataFrame\n",
        "    \"\"\"\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"üß™ Starting Classification Experiment\")\n",
        "        print(f\"üìä Dataset: {dataset_source}\")\n",
        "        print(f\"üéØ Total examples: {n_total_examples}\")\n",
        "        print(f\"‚úÖ Stage 3 True: {n_stage3_true}\")\n",
        "        print(f\"‚ùå Stage 3 False: {n_stage3_false}\")\n",
        "        print(\"=\" * 50)\n",
        "    \n",
        "    # Sample examples\n",
        "    stage3_true_samples = df[df['stage_3'] == True].sample(n=n_stage3_true, random_state=42)\n",
        "    stage3_false_samples = df[df['stage_3'] == False].sample(n=n_stage3_false, random_state=42)\n",
        "    \n",
        "    # Combine samples\n",
        "    test_samples = pd.concat([stage3_true_samples, stage3_false_samples]).reset_index(drop=True)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"üìù Sampled {len(test_samples)} examples\")\n",
        "    \n",
        "    # Load criteria and examples text\n",
        "    def load_criteria_text(criteria_file):\n",
        "        try:\n",
        "            criteria_df = pd.read_csv(criteria_file)\n",
        "            criteria_text = \"\"\n",
        "            \n",
        "            # Add inclusion criteria\n",
        "            inclusion_criteria = criteria_df[criteria_df['type'] == 'inclusion']\n",
        "            if len(inclusion_criteria) > 0:\n",
        "                criteria_text += \"**INCLUSION CRITERIA:**\\n\"\n",
        "                for _, row in inclusion_criteria.iterrows():\n",
        "                    criteria_text += f\"- **{row['criterion_id']}**: {row['description']}\\n\"\n",
        "                    if pd.notna(row['examples']) and row['examples'].strip():\n",
        "                        criteria_text += f\"  *Examples: {row['examples']}*\\n\"\n",
        "            \n",
        "            # Add exclusion criteria\n",
        "            exclusion_criteria = criteria_df[criteria_df['type'] == 'exclusion']\n",
        "            if len(exclusion_criteria) > 0:\n",
        "                criteria_text += \"\\n**EXCLUSION CRITERIA:**\\n\"\n",
        "                for _, row in exclusion_criteria.iterrows():\n",
        "                    criteria_text += f\"- **{row['criterion_id']}**: {row['description']}\\n\"\n",
        "                    if pd.notna(row['examples']) and row['examples'].strip():\n",
        "                        criteria_text += f\"  *Examples: {row['examples']}*\\n\"\n",
        "            \n",
        "            return criteria_text\n",
        "        except Exception as e:\n",
        "            return f\"Error loading criteria: {e}\"\n",
        "    \n",
        "    def load_examples_text(examples_file):\n",
        "        if not examples_file:\n",
        "            return \"\"\n",
        "        try:\n",
        "            examples_df = pd.read_csv(examples_file)\n",
        "            examples_text = \"\\n## EXAMPLE DECISIONS:\\n\"\n",
        "            \n",
        "            for _, row in examples_df.iterrows():\n",
        "                decision_label = \"INCLUDE\" if row['decision'].upper() == 'INCLUDE' else \"EXCLUDE\"\n",
        "                examples_text += f\"\\n**{decision_label} Example:**\\n\"\n",
        "                examples_text += f\"*Title:* {row['title']}\\n\"\n",
        "                examples_text += f\"*Abstract:* {row['abstract_text'][:200]}{'...' if len(row['abstract_text']) > 200 else ''}\\n\"\n",
        "                examples_text += f\"‚Üí **{decision_label}** ({row['reasoning']})\\n\"\n",
        "            \n",
        "            return examples_text\n",
        "        except Exception as e:\n",
        "            return f\"\\n## EXAMPLES:\\nError loading examples: {e}\\n\"\n",
        "    \n",
        "    # Load prompt components\n",
        "    criteria_text = load_criteria_text(CRITERIA_FILE)\n",
        "    examples_section = load_examples_text(EXAMPLES_FILE) if EXAMPLES_FILE else \"\"\n",
        "    \n",
        "    # Initialize results list\n",
        "    results_list = []\n",
        "    \n",
        "    # Process each example\n",
        "    for idx, row in test_samples.iterrows():\n",
        "        if verbose and (idx + 1) % 10 == 0:\n",
        "            print(f\"üîÑ Processing example {idx + 1}/{len(test_samples)}\")\n",
        "        \n",
        "        try:\n",
        "            # Create complete prompt\n",
        "            complete_prompt = USER_PROMPT_TEMPLATE.format(\n",
        "                topic=TOPIC,\n",
        "                domain=DOMAIN,\n",
        "                criteria_text=criteria_text,\n",
        "                examples_section=examples_section,\n",
        "                title=row['title_full'],\n",
        "                abstract=row['abstract'],\n",
        "                decision_include=DECISION_OPTIONS[0],\n",
        "                decision_exclude=DECISION_OPTIONS[1]\n",
        "            )\n",
        "            \n",
        "            # Call LLM\n",
        "            llm_result = screen_abstract_llm(\n",
        "                abstract_text=complete_prompt,\n",
        "                system_prompt=SYSTEM_PROMPT,\n",
        "                user_prompt_template=\"{abstract}\",  # Just pass through since we formatted above\n",
        "                model=MODEL_NAME,\n",
        "                temperature=TEMPERATURE\n",
        "            )\n",
        "            \n",
        "            # Parse LLM decision\n",
        "            llm_decision = llm_result.get('decision', 'UNKNOWN')\n",
        "            llm_reasoning = llm_result.get('reasoning', 'No reasoning provided')\n",
        "            \n",
        "            # Convert to binary for evaluation\n",
        "            llm_binary = 1 if llm_decision == 'INCLUDE' else 0\n",
        "            stage2_binary = 1 if row['stage_2'] else 0\n",
        "            stage3_binary = 1 if row['stage_3'] else 0\n",
        "            \n",
        "            # Store result\n",
        "            result_row = {\n",
        "                'example_id': idx + 1,\n",
        "                'title': row['title_full'],\n",
        "                'abstract': row['abstract'],\n",
        "                'stage_2_true': row['stage_2'],\n",
        "                'stage_3_true': row['stage_3'],\n",
        "                'stage_2_binary': stage2_binary,\n",
        "                'stage_3_binary': stage3_binary,\n",
        "                'llm_decision': llm_decision,\n",
        "                'llm_binary': llm_binary,\n",
        "                'llm_reasoning': llm_reasoning,\n",
        "                'experiment_id': EXPERIMENT_ID,\n",
        "                'dataset_source': dataset_source,\n",
        "                'system_prompt_id': SYSTEM_PROMPT_ID,\n",
        "                'user_prompt_id': USER_PROMPT_ID,\n",
        "                'model': MODEL_NAME,\n",
        "                'temperature': TEMPERATURE,\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "            \n",
        "            results_list.append(result_row)\n",
        "            \n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"‚ùå Error processing example {idx + 1}: {e}\")\n",
        "            \n",
        "            # Store error result\n",
        "            result_row = {\n",
        "                'example_id': idx + 1,\n",
        "                'title': row['title_full'],\n",
        "                'abstract': row['abstract'],\n",
        "                'stage_2_true': row['stage_2'],\n",
        "                'stage_3_true': row['stage_3'],\n",
        "                'stage_2_binary': 1 if row['stage_2'] else 0,\n",
        "                'stage_3_binary': 1 if row['stage_3'] else 0,\n",
        "                'llm_decision': 'ERROR',\n",
        "                'llm_binary': 0,\n",
        "                'llm_reasoning': f'Processing error: {e}',\n",
        "                'experiment_id': EXPERIMENT_ID,\n",
        "                'dataset_source': dataset_source,\n",
        "                'system_prompt_id': SYSTEM_PROMPT_ID,\n",
        "                'user_prompt_id': USER_PROMPT_ID,\n",
        "                'model': MODEL_NAME,\n",
        "                'temperature': TEMPERATURE,\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "            \n",
        "            results_list.append(result_row)\n",
        "    \n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame(results_list)\n",
        "    \n",
        "   # Calculate detailed metrics for stage_2\n",
        "    valid_results_stage2 = results_df[results_df['llm_decision'] != 'ERROR']\n",
        "    if len(valid_results_stage2) > 0:\n",
        "        y_true_stage2 = valid_results_stage2['stage_2_binary'].values\n",
        "        y_pred_stage2 = valid_results_stage2['llm_binary'].values\n",
        "        \n",
        "        # Basic metrics\n",
        "        accuracy_stage2 = accuracy_score(y_true_stage2, y_pred_stage2)\n",
        "        precision_stage2 = precision_score(y_true_stage2, y_pred_stage2, zero_division=0)\n",
        "        recall_stage2 = recall_score(y_true_stage2, y_pred_stage2, zero_division=0)\n",
        "        f1_stage2 = f1_score(y_true_stage2, y_pred_stage2, zero_division=0)\n",
        "        \n",
        "        # Confusion matrix metrics\n",
        "        tn2, fp2, fn2, tp2 = confusion_matrix(y_true_stage2, y_pred_stage2).ravel()\n",
        "    else:\n",
        "        accuracy_stage2 = precision_stage2 = recall_stage2 = f1_stage2 = 0.0\n",
        "        tp2 = fp2 = tn2 = fn2 = 0\n",
        "    \n",
        "    # Calculate detailed metrics for stage_3\n",
        "    valid_results_stage3 = results_df[results_df['llm_decision'] != 'ERROR']\n",
        "    if len(valid_results_stage3) > 0:\n",
        "        y_true_stage3 = valid_results_stage3['stage_3_binary'].values\n",
        "        y_pred_stage3 = valid_results_stage3['llm_binary'].values\n",
        "        \n",
        "        # Basic metrics\n",
        "        accuracy_stage3 = accuracy_score(y_true_stage3, y_pred_stage3)\n",
        "        precision_stage3 = precision_score(y_true_stage3, y_pred_stage3, zero_division=0)\n",
        "        recall_stage3 = recall_score(y_true_stage3, y_pred_stage3, zero_division=0)\n",
        "        f1_stage3 = f1_score(y_true_stage3, y_pred_stage3, zero_division=0)\n",
        "        \n",
        "        # Confusion matrix metrics\n",
        "        tn3, fp3, fn3, tp3 = confusion_matrix(y_true_stage3, y_pred_stage3).ravel()\n",
        "    else:\n",
        "        accuracy_stage3 = precision_stage3 = recall_stage3 = f1_stage3 = 0.0\n",
        "        tp3 = fp3 = tn3 = fn3 = 0\n",
        "    \n",
        "    # Updated metrics dictionary\n",
        "    metrics = {\n",
        "        'stage_2_metrics': {\n",
        "            'accuracy': accuracy_stage2,\n",
        "            'precision': precision_stage2,\n",
        "            'recall': recall_stage2,\n",
        "            'f1_score': f1_stage2,\n",
        "            'tp': int(tp2),\n",
        "            'fp': int(fp2),\n",
        "            'tn': int(tn2),\n",
        "            'fn': int(fn2)\n",
        "        },\n",
        "        'stage_3_metrics': {\n",
        "            'accuracy': accuracy_stage3,\n",
        "            'precision': precision_stage3,\n",
        "            'recall': recall_stage3,\n",
        "            'f1_score': f1_stage3,\n",
        "            'tp': int(tp3),\n",
        "            'fp': int(fp3),\n",
        "            'tn': int(tn3),\n",
        "            'fn': int(fn3)\n",
        "        },\n",
        "        'total_examples': len(results_df),\n",
        "        'successful_classifications': len(valid_results_stage2),\n",
        "        'errors': len(results_df) - len(valid_results_stage2)\n",
        "    }\n",
        "    \n",
        "    # Enhanced results printing\n",
        "    if verbose:\n",
        "        print(\"\\nüìä EXPERIMENT RESULTS\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"üìà Stage 2 Evaluation:\")\n",
        "        print(f\"   Accuracy:  {accuracy_stage2:.3f}\")\n",
        "        print(f\"   Precision: {precision_stage2:.3f}\")\n",
        "        print(f\"   Recall:    {recall_stage2:.3f}\")\n",
        "        print(f\"   F1 Score:  {f1_stage2:.3f}\")\n",
        "        print(f\"   TP: {tp2}, FP: {fp2}, TN: {tn2}, FN: {fn2}\")\n",
        "        print(f\"\\nüìà Stage 3 Evaluation:\")\n",
        "        print(f\"   Accuracy:  {accuracy_stage3:.3f}\")\n",
        "        print(f\"   Precision: {precision_stage3:.3f}\")\n",
        "        print(f\"   Recall:    {recall_stage3:.3f}\")\n",
        "        print(f\"   F1 Score:  {f1_stage3:.3f}\")\n",
        "        print(f\"   TP: {tp3}, FP: {fp3}, TN: {tn3}, FN: {fn3}\")\n",
        "        print(f\"\\nüìã Processing Summary:\")\n",
        "        print(f\"   Total examples: {len(results_df)}\")\n",
        "        print(f\"   Successful: {len(valid_results_stage2)}\")\n",
        "        print(f\"   Errors: {len(results_df) - len(valid_results_stage2)}\")\n",
        "    \n",
        "    # Save results\n",
        "    if save_results:\n",
        "        # Create filename with timestamp\n",
        "        timestamp = datetime.now().strftime(\"%m%d%H%M\")\n",
        "        filename = f\"{EXPERIMENT_ID}_{dataset_source}_{timestamp}.csv\"\n",
        "        results_dir = \"../results\"\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "        output_path = os.path.join(results_dir, filename)\n",
        "        \n",
        "        results_df.to_csv(output_path, index=False)\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"\\nüíæ Results saved to: {output_path}\")\n",
        "    \n",
        "    return {\n",
        "        'results_df': results_df,\n",
        "        'metrics': metrics,\n",
        "        'filename': filename if save_results else None\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Classification experiment function defined\")\n",
        "print(\"üöÄ Ready to run: run_classification_experiment(df_LB)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Run experiment! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Starting Classification Experiment\n",
            "üìä Dataset: LB\n",
            "üéØ Total examples: 50\n",
            "‚úÖ Stage 3 True: 5\n",
            "‚ùå Stage 3 False: 45\n",
            "==================================================\n",
            "üìù Sampled 50 examples\n",
            "üîÑ Processing example 10/50\n",
            "üîÑ Processing example 20/50\n",
            "üîÑ Processing example 30/50\n",
            "üîÑ Processing example 40/50\n",
            "üîÑ Processing example 50/50\n",
            "\n",
            "üìä EXPERIMENT RESULTS\n",
            "==================================================\n",
            "üìà Stage 2 Evaluation:\n",
            "   Accuracy:  0.840\n",
            "   Precision: 0.500\n",
            "   Recall:    0.625\n",
            "   F1 Score:  0.556\n",
            "   TP: 5, FP: 5, TN: 37, FN: 3\n",
            "\n",
            "üìà Stage 3 Evaluation:\n",
            "   Accuracy:  0.860\n",
            "   Precision: 0.400\n",
            "   Recall:    0.800\n",
            "   F1 Score:  0.533\n",
            "   TP: 4, FP: 6, TN: 39, FN: 1\n",
            "\n",
            "üìã Processing Summary:\n",
            "   Total examples: 50\n",
            "   Successful: 50\n",
            "   Errors: 0\n",
            "\n",
            "üíæ Results saved to: ../results/001_LB_08141120.csv\n"
          ]
        }
      ],
      "source": [
        "# Run experiment with default settings\n",
        "results = run_classification_experiment(df_LB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Results Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Load the most recent results file\n",
        "results_dir = \"../results\"\n",
        "result_files = [f for f in os.listdir(results_dir) if f.endswith('.csv')]\n",
        "if result_files:\n",
        "    # Get the most recent file (assuming timestamp format)\n",
        "    latest_file = sorted(result_files)[-1]\n",
        "    file_path = os.path.join(results_dir, latest_file)\n",
        "    \n",
        "    print(f\"üìÅ Loading: {latest_file}\")\n",
        "    df_results = pd.read_csv(file_path)\n",
        "    \n",
        "    print(f\"\\nüìä RESULTS OVERVIEW\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Shape: {df_results.shape}\")\n",
        "    print(f\"Columns: {list(df_results.columns)}\")\n",
        "    \n",
        "    print(f\"\\nüéØ DECISION SUMMARY\")\n",
        "    print(\"=\" * 30)\n",
        "    print(df_results['llm_decision'].value_counts())\n",
        "    \n",
        "    print(f\"\\nüìà PERFORMANCE PREVIEW\")\n",
        "    print(\"=\" * 30)\n",
        "    print(\"Stage 2 vs LLM:\")\n",
        "    print(pd.crosstab(df_results['stage_2_true'], df_results['llm_decision']))\n",
        "    print(\"\\nStage 3 vs LLM:\")\n",
        "    print(pd.crosstab(df_results['stage_3_true'], df_results['llm_decision']))\n",
        "    \n",
        "    print(f\"\\nüìã FIRST FEW RESULTS\")\n",
        "    print(\"=\" * 30)\n",
        "    display(df_results[['example_id', 'stage_2_true', 'stage_3_true', 'llm_decision', 'llm_reasoning']].head())\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No result files found in ../results directory\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display full reasoning for first 5 examples\n",
        "print(\"ü§ñ FULL LLM REASONING EXAMPLES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for idx in range(min(5, len(df_results))):\n",
        "    row = df_results.iloc[idx]\n",
        "    print(f\"\\nüìã EXAMPLE {row['example_id']} - {row['llm_decision']}\")\n",
        "    print(f\"üéØ Ground Truth: Stage 2={row['stage_2_true']}, Stage 3={row['stage_3_true']}\")\n",
        "    print(f\"üìñ Title: {row['title'][:100]}{'...' if len(row['title']) > 100 else ''}\")\n",
        "    print(f\"\\nüí≠ FULL REASONING:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(row['llm_reasoning'])\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß© Special - Create results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä EXPERIMENT RESULTS SUMMARY (with Confusion Matrix)\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>experiment_id</th>\n",
              "      <td>001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>experiment_date</th>\n",
              "      <td>2025-08-13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>experiment_category</th>\n",
              "      <td>Testing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>experiment_goal</th>\n",
              "      <td>Test Set Up</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>system_prompt_id</th>\n",
              "      <td>SYS_001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>user_prompt_id</th>\n",
              "      <td>USR_001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model_name</th>\n",
              "      <td>gpt-4o</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>temperature</th>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max_tokens</th>\n",
              "      <td>4000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>criteria_file</th>\n",
              "      <td>../prompts/Criteria_LB_01.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>examples_file</th>\n",
              "      <td>../prompts/exmpl_single_LB_01.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>output_format</th>\n",
              "      <td>Binary</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>domain</th>\n",
              "      <td>political_communication</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>topic</th>\n",
              "      <td>media_dviersity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dataset_source</th>\n",
              "      <td>LB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>n_total_examples</th>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>n_successful</th>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>n_errors</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stage2_accuracy</th>\n",
              "      <td>0.880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stage2_precision</th>\n",
              "      <td>0.800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stage2_recall</th>\n",
              "      <td>0.667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stage2_f1</th>\n",
              "      <td>0.727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stage2_tp</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stage2_fp</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stage2_tn</th>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stage2_fn</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stage3_accuracy</th>\n",
              "      <td>0.920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stage3_precision</th>\n",
              "      <td>0.800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stage3_recall</th>\n",
              "      <td>0.800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stage3_f1</th>\n",
              "      <td>0.800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stage3_tp</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stage3_fp</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stage3_tn</th>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stage3_fn</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>results_filename</th>\n",
              "      <td>001_LB_08131314.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <td>2025-08-13T13:14:33.374055</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     0\n",
              "experiment_id                                      001\n",
              "experiment_date                             2025-08-13\n",
              "experiment_category                            Testing\n",
              "experiment_goal                            Test Set Up\n",
              "system_prompt_id                               SYS_001\n",
              "user_prompt_id                                 USR_001\n",
              "model_name                                      gpt-4o\n",
              "temperature                                      0.000\n",
              "max_tokens                                        4000\n",
              "criteria_file            ../prompts/Criteria_LB_01.csv\n",
              "examples_file        ../prompts/exmpl_single_LB_01.csv\n",
              "output_format                                   Binary\n",
              "domain                         political_communication\n",
              "topic                                  media_dviersity\n",
              "dataset_source                                      LB\n",
              "n_total_examples                                    25\n",
              "n_successful                                        25\n",
              "n_errors                                             0\n",
              "stage2_accuracy                                  0.880\n",
              "stage2_precision                                 0.800\n",
              "stage2_recall                                    0.667\n",
              "stage2_f1                                        0.727\n",
              "stage2_tp                                            4\n",
              "stage2_fp                                            1\n",
              "stage2_tn                                           18\n",
              "stage2_fn                                            2\n",
              "stage3_accuracy                                  0.920\n",
              "stage3_precision                                 0.800\n",
              "stage3_recall                                    0.800\n",
              "stage3_f1                                        0.800\n",
              "stage3_tp                                            4\n",
              "stage3_fp                                            1\n",
              "stage3_tn                                           19\n",
              "stage3_fn                                            1\n",
              "results_filename                   001_LB_08131314.csv\n",
              "timestamp                   2025-08-13T13:14:33.374055"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Experiment summary saved to: ../results/experiment_summary.csv\n"
          ]
        }
      ],
      "source": [
        "# Updated experiment results summary DataFrame with confusion matrix metrics\n",
        "experiment_results = pd.DataFrame({\n",
        "    'experiment_id': [EXPERIMENT_ID],\n",
        "    'experiment_date': [EXPERIMENT_DATE],\n",
        "    'experiment_category': [EXPERIMENT_CATEGORY],\n",
        "    'experiment_goal': [EXPERIMENT_GOAL],\n",
        "    'system_prompt_id': [SYSTEM_PROMPT_ID],\n",
        "    'user_prompt_id': [USER_PROMPT_ID],\n",
        "    'model_name': [MODEL_NAME],\n",
        "    'temperature': [TEMPERATURE],\n",
        "    'max_tokens': [MAX_TOKENS],\n",
        "    'criteria_file': [CRITERIA_FILE],\n",
        "    'examples_file': [EXAMPLES_FILE],\n",
        "    'output_format': [OUTPUT_FORMAT],\n",
        "    'domain': [DOMAIN],\n",
        "    'topic': [TOPIC],\n",
        "    'dataset_source': [DATASET_SOURCE],\n",
        "    'n_total_examples': [results['metrics']['total_examples']],\n",
        "    'n_successful': [results['metrics']['successful_classifications']],\n",
        "    'n_errors': [results['metrics']['errors']],\n",
        "    # Stage 2 metrics\n",
        "    'stage2_accuracy': [results['metrics']['stage_2_metrics']['accuracy']],\n",
        "    'stage2_precision': [results['metrics']['stage_2_metrics']['precision']],\n",
        "    'stage2_recall': [results['metrics']['stage_2_metrics']['recall']],\n",
        "    'stage2_f1': [results['metrics']['stage_2_metrics']['f1_score']],\n",
        "    'stage2_tp': [results['metrics']['stage_2_metrics']['tp']],\n",
        "    'stage2_fp': [results['metrics']['stage_2_metrics']['fp']],\n",
        "    'stage2_tn': [results['metrics']['stage_2_metrics']['tn']],\n",
        "    'stage2_fn': [results['metrics']['stage_2_metrics']['fn']],\n",
        "    # Stage 3 metrics\n",
        "    'stage3_accuracy': [results['metrics']['stage_3_metrics']['accuracy']],\n",
        "    'stage3_precision': [results['metrics']['stage_3_metrics']['precision']],\n",
        "    'stage3_recall': [results['metrics']['stage_3_metrics']['recall']],\n",
        "    'stage3_f1': [results['metrics']['stage_3_metrics']['f1_score']],\n",
        "    'stage3_tp': [results['metrics']['stage_3_metrics']['tp']],\n",
        "    'stage3_fp': [results['metrics']['stage_3_metrics']['fp']],\n",
        "    'stage3_tn': [results['metrics']['stage_3_metrics']['tn']],\n",
        "    'stage3_fn': [results['metrics']['stage_3_metrics']['fn']],\n",
        "    'results_filename': [results['filename']],\n",
        "    'timestamp': [datetime.now().isoformat()]\n",
        "})\n",
        "\n",
        "print(\"üìä EXPERIMENT RESULTS SUMMARY (with Confusion Matrix)\")\n",
        "print(\"=\" * 60)\n",
        "display(experiment_results.T)\n",
        "\n",
        "# Save to CSV\n",
        "summary_path = \"../results/experiment_summary.csv\"\n",
        "experiment_results.to_csv(summary_path, index=False)\n",
        "print(f\"\\nüíæ Experiment summary saved to: {summary_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ûï Add experiment info to the results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Current summary has 0 experiments\n",
            "‚úÖ Added experiment 001 to existing summary\n",
            "üíæ Summary saved with 1 total experiments\n",
            "\n",
            "üìã LAST 5 EXPERIMENTS:\n",
            "====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/xg/jz066c5d5jn87vqnb688_0d00000gn/T/ipykernel_73444/4244542121.py:124: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_summary = pd.concat([existing_summary, new_row], ignore_index=True)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>experiment_id</th>\n",
              "      <th>experiment_date</th>\n",
              "      <th>experiment_category</th>\n",
              "      <th>experiment_goal</th>\n",
              "      <th>system_prompt_id</th>\n",
              "      <th>user_prompt_id</th>\n",
              "      <th>model_name</th>\n",
              "      <th>temperature</th>\n",
              "      <th>max_tokens</th>\n",
              "      <th>criteria_file</th>\n",
              "      <th>examples_file</th>\n",
              "      <th>output_format</th>\n",
              "      <th>domain</th>\n",
              "      <th>topic</th>\n",
              "      <th>dataset_source</th>\n",
              "      <th>n_total_examples</th>\n",
              "      <th>n_successful</th>\n",
              "      <th>n_errors</th>\n",
              "      <th>stage2_accuracy</th>\n",
              "      <th>stage2_precision</th>\n",
              "      <th>stage2_recall</th>\n",
              "      <th>stage2_f1</th>\n",
              "      <th>stage2_tp</th>\n",
              "      <th>stage2_fp</th>\n",
              "      <th>stage2_tn</th>\n",
              "      <th>stage2_fn</th>\n",
              "      <th>stage3_accuracy</th>\n",
              "      <th>stage3_precision</th>\n",
              "      <th>stage3_recall</th>\n",
              "      <th>stage3_f1</th>\n",
              "      <th>stage3_tp</th>\n",
              "      <th>stage3_fp</th>\n",
              "      <th>stage3_tn</th>\n",
              "      <th>stage3_fn</th>\n",
              "      <th>llm_include_count</th>\n",
              "      <th>llm_maybe_count</th>\n",
              "      <th>llm_exclude_count</th>\n",
              "      <th>llm_error_count</th>\n",
              "      <th>likert_score_1</th>\n",
              "      <th>likert_score_2</th>\n",
              "      <th>likert_score_3</th>\n",
              "      <th>likert_score_4</th>\n",
              "      <th>likert_score_5</th>\n",
              "      <th>stage2_true_score_1</th>\n",
              "      <th>stage2_true_score_2</th>\n",
              "      <th>stage2_true_score_3</th>\n",
              "      <th>stage2_true_score_4</th>\n",
              "      <th>stage2_true_score_5</th>\n",
              "      <th>stage2_false_score_1</th>\n",
              "      <th>stage2_false_score_2</th>\n",
              "      <th>stage2_false_score_3</th>\n",
              "      <th>stage2_false_score_4</th>\n",
              "      <th>stage2_false_score_5</th>\n",
              "      <th>stage3_true_score_1</th>\n",
              "      <th>stage3_true_score_2</th>\n",
              "      <th>stage3_true_score_3</th>\n",
              "      <th>stage3_true_score_4</th>\n",
              "      <th>stage3_true_score_5</th>\n",
              "      <th>stage3_false_score_1</th>\n",
              "      <th>stage3_false_score_2</th>\n",
              "      <th>stage3_false_score_3</th>\n",
              "      <th>stage3_false_score_4</th>\n",
              "      <th>stage3_false_score_5</th>\n",
              "      <th>stage2_threshold</th>\n",
              "      <th>stage3_threshold</th>\n",
              "      <th>results_filename</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>001</td>\n",
              "      <td>2025-08-13</td>\n",
              "      <td>Testing</td>\n",
              "      <td>Test Set Up</td>\n",
              "      <td>SYS_001</td>\n",
              "      <td>USR_001</td>\n",
              "      <td>gpt-4o</td>\n",
              "      <td>0.000</td>\n",
              "      <td>4000</td>\n",
              "      <td>../prompts/Criteria_LB_01.csv</td>\n",
              "      <td>../prompts/exmpl_single_LB_01.csv</td>\n",
              "      <td>Binary</td>\n",
              "      <td>political_communication</td>\n",
              "      <td>media_dviersity</td>\n",
              "      <td>LB</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>0</td>\n",
              "      <td>0.840</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.556</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>37</td>\n",
              "      <td>3</td>\n",
              "      <td>0.860</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.800</td>\n",
              "      <td>0.533</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>001_LB_08141120.csv</td>\n",
              "      <td>2025-08-14T12:41:42.783545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  experiment_id experiment_date experiment_category experiment_goal system_prompt_id user_prompt_id model_name  temperature max_tokens                  criteria_file                      examples_file output_format                   domain            topic dataset_source n_total_examples n_successful n_errors  stage2_accuracy  stage2_precision  stage2_recall  stage2_f1 stage2_tp stage2_fp stage2_tn stage2_fn  stage3_accuracy  stage3_precision  stage3_recall  stage3_f1 stage3_tp stage3_fp stage3_tn stage3_fn llm_include_count llm_maybe_count llm_exclude_count llm_error_count likert_score_1 likert_score_2 likert_score_3 likert_score_4 likert_score_5 stage2_true_score_1 stage2_true_score_2 stage2_true_score_3 stage2_true_score_4 stage2_true_score_5 stage2_false_score_1 stage2_false_score_2 stage2_false_score_3 stage2_false_score_4 stage2_false_score_5 stage3_true_score_1 stage3_true_score_2 stage3_true_score_3 stage3_true_score_4 stage3_true_score_5 stage3_false_score_1  \\\n",
              "0           001      2025-08-13             Testing     Test Set Up          SYS_001        USR_001     gpt-4o        0.000       4000  ../prompts/Criteria_LB_01.csv  ../prompts/exmpl_single_LB_01.csv        Binary  political_communication  media_dviersity             LB               50           50        0            0.840             0.500          0.625      0.556         5         5        37         3            0.860             0.400          0.800      0.533         4         6        39         1               NaN             NaN               NaN             NaN            NaN            NaN            NaN            NaN            NaN                 NaN                 NaN                 NaN                 NaN                 NaN                  NaN                  NaN                  NaN                  NaN                  NaN                 NaN                 NaN                 NaN                 NaN                 NaN                  NaN   \n",
              "\n",
              "  stage3_false_score_2 stage3_false_score_3 stage3_false_score_4 stage3_false_score_5 stage2_threshold stage3_threshold     results_filename                   timestamp  \n",
              "0                  NaN                  NaN                  NaN                  NaN              NaN              NaN  001_LB_08141120.csv  2025-08-14T12:41:42.783545  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä SUMMARY STATS:\n",
            "   Total experiments: 1\n",
            "   Unique experiment IDs: 1\n",
            "   Datasets used: ['LB']\n"
          ]
        }
      ],
      "source": [
        "def add_experiment_to_summary_safe(results_dict, summary_file=\"../results/experiment_summary.csv\"):\n",
        "    \"\"\"Safely add experiment results to summary - never overwrites, only appends\"\"\"\n",
        "    \n",
        "    from datetime import datetime\n",
        "    \n",
        "    # Load existing summary (should exist now)\n",
        "    if not os.path.exists(summary_file):\n",
        "        print(f\"‚ùå Summary file not found: {summary_file}\")\n",
        "        print(\"Please run create_empty_experiment_summary() first!\")\n",
        "        return None\n",
        "    \n",
        "    existing_summary = pd.read_csv(summary_file)\n",
        "    print(f\"üìä Current summary has {len(existing_summary)} experiments\")\n",
        "    \n",
        "    # Create new row with only the data we have\n",
        "    new_row_data = {\n",
        "        # Basic experiment info (always available)\n",
        "        'experiment_id': EXPERIMENT_ID,\n",
        "        'experiment_date': EXPERIMENT_DATE,\n",
        "        'experiment_category': EXPERIMENT_CATEGORY,\n",
        "        'experiment_goal': EXPERIMENT_GOAL,\n",
        "        'system_prompt_id': SYSTEM_PROMPT_ID,\n",
        "        'user_prompt_id': USER_PROMPT_ID,\n",
        "        'model_name': MODEL_NAME,\n",
        "        'temperature': TEMPERATURE,\n",
        "        'max_tokens': MAX_TOKENS,\n",
        "        'criteria_file': CRITERIA_FILE,\n",
        "        'examples_file': EXAMPLES_FILE,\n",
        "        'output_format': OUTPUT_FORMAT,\n",
        "        'domain': DOMAIN,\n",
        "        'topic': TOPIC,\n",
        "        'dataset_source': DATASET_SOURCE,\n",
        "        'n_total_examples': results_dict['metrics']['total_examples'],\n",
        "        'n_successful': results_dict['metrics']['successful_classifications'],\n",
        "        'n_errors': results_dict['metrics']['errors'],\n",
        "        'results_filename': results_dict['filename'],\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    # Add Stage 2 metrics (always available)\n",
        "    stage2_metrics = results_dict['metrics']['stage_2_metrics']\n",
        "    new_row_data.update({\n",
        "        'stage2_accuracy': stage2_metrics['accuracy'],\n",
        "        'stage2_precision': stage2_metrics['precision'],\n",
        "        'stage2_recall': stage2_metrics['recall'],\n",
        "        'stage2_f1': stage2_metrics['f1_score'],\n",
        "        'stage2_tp': stage2_metrics['tp'],\n",
        "        'stage2_fp': stage2_metrics['fp'],\n",
        "        'stage2_tn': stage2_metrics['tn'],\n",
        "        'stage2_fn': stage2_metrics['fn']\n",
        "    })\n",
        "    \n",
        "    # Add Stage 3 metrics if available\n",
        "    if 'stage_3_metrics' in results_dict['metrics']:\n",
        "        stage3_metrics = results_dict['metrics']['stage_3_metrics']\n",
        "        new_row_data.update({\n",
        "            'stage3_accuracy': stage3_metrics['accuracy'],\n",
        "            'stage3_precision': stage3_metrics['precision'],\n",
        "            'stage3_recall': stage3_metrics['recall'],\n",
        "            'stage3_f1': stage3_metrics['f1_score'],\n",
        "            'stage3_tp': stage3_metrics['tp'],\n",
        "            'stage3_fp': stage3_metrics['fp'],\n",
        "            'stage3_tn': stage3_metrics['tn'],\n",
        "            'stage3_fn': stage3_metrics['fn']\n",
        "        })\n",
        "        \n",
        "        # Add thresholds if available\n",
        "        if 'threshold' in stage3_metrics:\n",
        "            new_row_data['stage3_threshold'] = stage3_metrics['threshold']\n",
        "    \n",
        "    # Add Stage 2 threshold if available\n",
        "    if 'threshold' in stage2_metrics:\n",
        "        new_row_data['stage2_threshold'] = stage2_metrics['threshold']\n",
        "    \n",
        "    # Add decision counts if available (for MAYBE experiments)\n",
        "    if 'decision_counts' in results_dict['metrics']:\n",
        "        decision_counts = results_dict['metrics']['decision_counts']\n",
        "        new_row_data.update({\n",
        "            'llm_include_count': decision_counts.get('INCLUDE', np.nan),\n",
        "            'llm_maybe_count': decision_counts.get('MAYBE', np.nan),\n",
        "            'llm_exclude_count': decision_counts.get('EXCLUDE', np.nan),\n",
        "            'llm_error_count': decision_counts.get('ERROR', np.nan)\n",
        "        })\n",
        "    \n",
        "    # Add Likert analysis if available\n",
        "    if 'likert_analysis' in results_dict['metrics']:\n",
        "        likert_analysis = results_dict['metrics']['likert_analysis']\n",
        "        \n",
        "        # Overall distribution\n",
        "        if 'overall_distribution' in likert_analysis:\n",
        "            overall_dist = likert_analysis['overall_distribution']\n",
        "            for i in range(1, 6):\n",
        "                new_row_data[f'likert_score_{i}'] = overall_dist.get(f'score_{i}', np.nan)\n",
        "        \n",
        "        # Stage 2 distributions\n",
        "        if 'stage2_true_distribution' in likert_analysis:\n",
        "            stage2_true_dist = likert_analysis['stage2_true_distribution']\n",
        "            for i in range(1, 6):\n",
        "                new_row_data[f'stage2_true_score_{i}'] = stage2_true_dist.get(f'score_{i}', np.nan)\n",
        "        \n",
        "        if 'stage2_false_distribution' in likert_analysis:\n",
        "            stage2_false_dist = likert_analysis['stage2_false_distribution']\n",
        "            for i in range(1, 6):\n",
        "                new_row_data[f'stage2_false_score_{i}'] = stage2_false_dist.get(f'score_{i}', np.nan)\n",
        "        \n",
        "        # Stage 3 distributions\n",
        "        if 'stage3_true_distribution' in likert_analysis:\n",
        "            stage3_true_dist = likert_analysis['stage3_true_distribution']\n",
        "            for i in range(1, 6):\n",
        "                new_row_data[f'stage3_true_score_{i}'] = stage3_true_dist.get(f'score_{i}', np.nan)\n",
        "        \n",
        "        if 'stage3_false_distribution' in likert_analysis:\n",
        "            stage3_false_dist = likert_analysis['stage3_false_distribution']\n",
        "            for i in range(1, 6):\n",
        "                new_row_data[f'stage3_false_score_{i}'] = stage3_false_dist.get(f'score_{i}', np.nan)\n",
        "    \n",
        "    # Create new row DataFrame with all columns from existing summary\n",
        "    new_row = pd.DataFrame([new_row_data])\n",
        "    \n",
        "    # Reindex to match existing summary columns (fills missing with NaN automatically)\n",
        "    new_row = new_row.reindex(columns=existing_summary.columns)\n",
        "    \n",
        "    # Append to existing summary (never overwrites)\n",
        "    updated_summary = pd.concat([existing_summary, new_row], ignore_index=True)\n",
        "    \n",
        "    # Save updated summary\n",
        "    updated_summary.to_csv(summary_file, index=False)\n",
        "    \n",
        "    print(f\"‚úÖ Added experiment {EXPERIMENT_ID} to existing summary\")\n",
        "    print(f\"üíæ Summary saved with {len(updated_summary)} total experiments\")\n",
        "    \n",
        "    # Show the last 5 rows\n",
        "    print(f\"\\nüìã LAST 5 EXPERIMENTS:\")\n",
        "    print(\"=\" * 100)\n",
        "    display(updated_summary.tail())\n",
        "    \n",
        "    print(f\"\\nüìä SUMMARY STATS:\")\n",
        "    print(f\"   Total experiments: {len(updated_summary)}\")\n",
        "    print(f\"   Unique experiment IDs: {updated_summary['experiment_id'].nunique()}\")\n",
        "    print(f\"   Datasets used: {updated_summary['dataset_source'].unique().tolist()}\")\n",
        "    \n",
        "    return updated_summary\n",
        "\n",
        "# Usage: \n",
        "summary_df = add_experiment_to_summary_safe(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Conclusions and Next Steps\n",
        "\n",
        "### Key Findings\n",
        "- \n",
        "\n",
        "### Next Steps\n",
        "- [Suggest follow-up experiments]\n",
        "- [List potential improvements]\n",
        "- [Identify areas for further investigation]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "SLRenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
